--- Execution profile ---
Total samples       : 2356812
not_walkable_not_Java: 2 (0.00%)
unknown_Java        : 24541 (1.04%)
not_walkable_Java   : 3683 (0.16%)
skipped             : 1 (0.00%)

--- 870003858 total (36.91%), 869091 samples
  [ 0] ReduceOps$5ReducingSink.get
  [ 1] ReduceOps$5ReducingSink.get
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 109495693 total (4.65%), 109794 samples
  [ 0] Cell$$Lambda$70.0x0000000801037850.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 99210580 total (4.21%), 98992 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 96788145 total (4.11%), 97145 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$70.0x0000000801037850.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 63856492 total (2.71%), 64113 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$60.0x0000000801036300.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 56544663 total (2.40%), 56560 samples
  [ 0] vtable stub
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 52984614 total (2.25%), 53039 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 49755177 total (2.11%), 49749 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 49519938 total (2.10%), 49722 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$70.0x0000000801037850.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 43300806 total (1.84%), 43339 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 41435268 total (1.76%), 41570 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$70.0x0000000801037850.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 41316081 total (1.75%), 41447 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$70.0x0000000801037850.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] Continuation.enterSpecial
  [15] Continuation.run
  [16] VirtualThread.runContinuation
  [17] VirtualThread$$Lambda$51.0x000000080103df08.run
  [18] ForkJoinTask$RunnableExecuteAction.exec
  [19] ForkJoinTask.doExec
  [20] ForkJoinPool$WorkQueue.topLevelExec
  [21] ForkJoinPool.scan
  [22] ForkJoinPool.runWorker
  [23] ForkJoinWorkerThread.run

--- 38364141 total (1.63%), 38555 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$60.0x0000000801036300.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 36831896 total (1.56%), 36790 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 33883281 total (1.44%), 33911 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 32501132 total (1.38%), 32420 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 24361394 total (1.03%), 24411 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 21979127 total (0.93%), 22011 samples
  [ 0] ArrayList$ArrayListSpliterator.<init>
  [ 1] ArrayList.spliterator
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 21074964 total (0.89%), 20984 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 18667765 total (0.79%), 18756 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] Cell.lambda$notifyLiveness$0
  [ 5] Cell$$Lambda$60.0x0000000801036300.accept
  [ 6] ArrayList.forEach
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 17298224 total (0.73%), 17316 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 13993415 total (0.59%), 14027 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 13340670 total (0.57%), 13379 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 10009228 total (0.42%), 9978 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 9626467 total (0.41%), 9598 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 9341469 total (0.40%), 9343 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 9034411 total (0.38%), 9034 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 8313064 total (0.35%), 8323 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 8305512 total (0.35%), 8313 samples
  [ 0] ReferencePipeline.map
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 8092611 total (0.34%), 8090 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 7434220 total (0.32%), 7437 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 7351203 total (0.31%), 7344 samples
  [ 0] ReduceOps$5ReducingSink.get
  [ 1] ReduceOps$5ReducingSink.get
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$AdaptedRunnableAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 7184453 total (0.30%), 7089 samples
  [ 0] vframeStream::vframeStream(JavaThread*, bool, bool, bool)
  [ 1] SharedRuntime::find_callee_method(JavaThread*)
  [ 2] SharedRuntime::reresolve_call_site(JavaThread*)
  [ 3] SharedRuntime::handle_wrong_method(JavaThread*)
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 6829541 total (0.29%), 6850 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 6663049 total (0.28%), 6644 samples
  [ 0] ArrayList$ArrayListSpliterator.<init>
  [ 1] ArrayList.spliterator
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 6536917 total (0.28%), 6551 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 6523335 total (0.28%), 6531 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] [unknown_Java]

--- 6520892 total (0.28%), 6540 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 6427816 total (0.27%), 6423 samples
  [ 0] StreamOpFlag.getMask
  [ 1] StreamOpFlag.combineOpFlags
  [ 2] AbstractPipeline.<init>
  [ 3] ReferencePipeline.<init>
  [ 4] ReferencePipeline$StatelessOp.<init>
  [ 5] ReferencePipeline$3.<init>
  [ 6] ReferencePipeline.map
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 6025418 total (0.26%), 6043 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] [unknown_Java]

--- 5712511 total (0.24%), 5735 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$70.0x0000000801037850.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 5658353 total (0.24%), 5682 samples
  [ 0] TickPerCell.lambda$tick$0
  [ 1] TickPerCell$$Lambda$58.0x0000000801035ec8.accept
  [ 2] ChannelsGrid.lambda$forEachChannel$0
  [ 3] ChannelsGrid$$Lambda$59.0x00000008010360e0.accept
  [ 4] Dimensions.forEachRowCol
  [ 5] ChannelsGrid.forEachChannel
  [ 6] TickPerCell.tick
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$53.0x0000000801035268.run
  [10] VirtualThread.run
  [11] VirtualThread$VThreadContinuation.lambda$new$0
  [12] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [13] Continuation.enter0
  [14] Continuation.enter
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 5566296 total (0.24%), 5566 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 5501668 total (0.23%), 5506 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] [unknown_Java]

--- 5472702 total (0.23%), 5494 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 5280536 total (0.22%), 5303 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$70.0x0000000801037850.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 5197334 total (0.22%), 5186 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 4849285 total (0.21%), 4839 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 4201123 total (0.18%), 4204 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 4199001 total (0.18%), 4212 samples
  [ 0] ReentrantLock$Sync.lock
  [ 1] ReentrantLock.lock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$70.0x0000000801037850.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 4155527 total (0.18%), 4155 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] [unknown_Java]

--- 4095820 total (0.17%), 4109 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$70.0x0000000801037850.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] Continuation.enterSpecial
  [15] Continuation.run
  [16] VirtualThread.runContinuation
  [17] VirtualThread$$Lambda$51.0x000000080103df08.run
  [18] ForkJoinTask$RunnableExecuteAction.exec
  [19] ForkJoinTask.doExec
  [20] ForkJoinPool$WorkQueue.topLevelExec
  [21] ForkJoinPool.scan
  [22] ForkJoinPool.runWorker
  [23] ForkJoinWorkerThread.run

--- 4033648 total (0.17%), 4016 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 3348159 total (0.14%), 3333 samples
  [ 0] Collection.stream
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 3341516 total (0.14%), 3332 samples
  [ 0] Object.<init>
  [ 1] PipelineHelper.<init>
  [ 2] AbstractPipeline.<init>
  [ 3] IntPipeline.<init>
  [ 4] IntPipeline$StatelessOp.<init>
  [ 5] ReferencePipeline$4.<init>
  [ 6] ReferencePipeline.mapToInt
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 3263363 total (0.14%), 3277 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$53.0x0000000801035268.run
  [13] VirtualThread.run
  [14] VirtualThread$VThreadContinuation.lambda$new$0
  [15] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [16] Continuation.enter0
  [17] Continuation.enter
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 3157291 total (0.13%), 3161 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 3138073 total (0.13%), 3140 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 3050231 total (0.13%), 3061 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$70.0x0000000801037850.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 3018364 total (0.13%), 3026 samples
  [ 0] Channel.take
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 3017290 total (0.13%), 3033 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$58.0x0000000801035ec8.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$59.0x00000008010360e0.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$53.0x0000000801035268.run
  [16] VirtualThread.run
  [17] VirtualThread$VThreadContinuation.lambda$new$0
  [18] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [19] Continuation.enter0
  [20] Continuation.enter
  [21] Continuation.enterSpecial
  [22] Continuation.run
  [23] VirtualThread.runContinuation
  [24] VirtualThread$$Lambda$51.0x000000080103df08.run
  [25] ForkJoinTask$RunnableExecuteAction.exec
  [26] ForkJoinTask.doExec
  [27] ForkJoinPool$WorkQueue.topLevelExec
  [28] ForkJoinPool.scan
  [29] ForkJoinPool.runWorker
  [30] ForkJoinWorkerThread.run

--- 2935726 total (0.12%), 2923 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 2929266 total (0.12%), 2945 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035268.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 2686347 total (0.11%), 2695 samples
  [ 0] ReentrantLock.lock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$70.0x0000000801037850.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 2660959 total (0.11%), 2663 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 2595864 total (0.11%), 2598 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 2381796 total (0.10%), 2388 samples
  [ 0] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 2291760 total (0.10%), 2303 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035268.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 2241723 total (0.10%), 2251 samples
  [ 0] ReentrantLock$Sync.isHeldExclusively
  [ 1] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$70.0x0000000801037850.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 2190767 total (0.09%), 2187 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 2077660 total (0.09%), 2072 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 2030763 total (0.09%), 2039 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$70.0x0000000801037850.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 1999536 total (0.08%), 2008 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$70.0x0000000801037850.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 1995784 total (0.08%), 1997 samples
  [ 0] StreamOpFlag.combineOpFlags
  [ 1] AbstractPipeline.<init>
  [ 2] ReferencePipeline.<init>
  [ 3] ReferencePipeline$StatelessOp.<init>
  [ 4] ReferencePipeline$3.<init>
  [ 5] ReferencePipeline.map
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 1995329 total (0.08%), 1991 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] G1RemSetScanState::G1ClearCardTableTask::do_work(unsigned int)
  [ 2] G1BatchedTask::work(unsigned int)
  [ 3] WorkerThread::run()
  [ 4] Thread::call_run()
  [ 5] thread_native_entry(Thread*)
  [ 6] start_thread

--- 1991806 total (0.08%), 1996 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 1969732 total (0.08%), 1978 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$70.0x0000000801037850.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 1968485 total (0.08%), 1977 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$70.0x0000000801037850.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 1948788 total (0.08%), 1957 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$70.0x0000000801037850.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 1940052 total (0.08%), 1947 samples
  [ 0] MemAllocator::allocate() const
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$53.0x0000000801035268.run
  [ 6] VirtualThread.run
  [ 7] VirtualThread$VThreadContinuation.lambda$new$0
  [ 8] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 9] Continuation.enter0
  [10] Continuation.enter
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 1878518 total (0.08%), 1875 samples
  [ 0] ReferencePipeline.mapToInt
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 1853149 total (0.08%), 1861 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$70.0x0000000801037850.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 1850633 total (0.08%), 1858 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] TickPerCell.lambda$tick$0
  [ 5] TickPerCell$$Lambda$58.0x0000000801035ec8.accept
  [ 6] ChannelsGrid.lambda$forEachChannel$0
  [ 7] ChannelsGrid$$Lambda$59.0x00000008010360e0.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] TickPerCell.tick
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035268.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 1820186 total (0.08%), 1791 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] ForkJoinPool.awaitWork
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 1766354 total (0.07%), 1769 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 1740870 total (0.07%), 1748 samples
  [ 0] AbstractQueuedSynchronizer.getState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$70.0x0000000801037850.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 1648870 total (0.07%), 1656 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 1627489 total (0.07%), 1618 samples
  [ 0] InstanceKlass::find_method_index(Array<Method*> const*, Symbol const*, Symbol const*, Klass::OverpassLookupMode, Klass::StaticLookupMode, Klass::PrivateLookupMode) [clone .constprop.0]
  [ 1] InstanceKlass::uncached_lookup_method(Symbol const*, Symbol const*, Klass::OverpassLookupMode, Klass::PrivateLookupMode) const
  [ 2] LinkResolver::lookup_method_in_klasses(LinkInfo const&, bool, bool)
  [ 3] LinkResolver::resolve_method(LinkInfo const&, Bytecodes::Code, JavaThread*)
  [ 4] LinkResolver::resolve_continuation_enter(CallInfo&, JavaThread*)
  [ 5] SharedRuntime::find_callee_info_helper(vframeStream&, Bytecodes::Code&, CallInfo&, JavaThread*)
  [ 6] SharedRuntime::find_callee_method(JavaThread*)
  [ 7] SharedRuntime::reresolve_call_site(JavaThread*)
  [ 8] SharedRuntime::handle_wrong_method(JavaThread*)
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 1573328 total (0.07%), 1581 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 1568459 total (0.07%), 1574 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] Continuation.enterSpecial
  [15] Continuation.run
  [16] VirtualThread.runContinuation
  [17] VirtualThread$$Lambda$51.0x000000080103df08.run
  [18] ForkJoinTask$RunnableExecuteAction.exec
  [19] ForkJoinTask.doExec
  [20] ForkJoinPool$WorkQueue.topLevelExec
  [21] ForkJoinPool.scan
  [22] ForkJoinPool.runWorker
  [23] ForkJoinWorkerThread.run

--- 1555939 total (0.07%), 1562 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 1553662 total (0.07%), 1561 samples
  [ 0] Boolean.booleanValue
  [ 1] Cell.lambda$calculateNextState$1
  [ 2] Cell$$Lambda$73.0x0000000801037ca0.applyAsInt
  [ 3] ReferencePipeline$4$1.accept
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 1521538 total (0.06%), 1526 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 1504083 total (0.06%), 1499 samples
  [ 0] G1CardSet::occupied() const
  [ 1] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 2] G1RemSetSamplingTask::execute()
  [ 3] G1ServiceThread::run_task(G1ServiceTask*)
  [ 4] G1ServiceThread::run_service()
  [ 5] ConcurrentGCThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 1455613 total (0.06%), 1433 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] ReduceOps$5ReducingSink.get
  [ 6] ReduceOps$5ReducingSink.get
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 1447193 total (0.06%), 1454 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$70.0x0000000801037850.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 1431332 total (0.06%), 1428 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 1423227 total (0.06%), 1424 samples
  [ 0] Object.<init>
  [ 1] PipelineHelper.<init>
  [ 2] AbstractPipeline.<init>
  [ 3] ReferencePipeline.<init>
  [ 4] ReferencePipeline$Head.<init>
  [ 5] StreamSupport.stream
  [ 6] Collection.stream
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 1284887 total (0.05%), 1290 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$53.0x0000000801035268.run
  [ 5] VirtualThread.run
  [ 6] VirtualThread$VThreadContinuation.lambda$new$0
  [ 7] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 8] Continuation.enter0
  [ 9] Continuation.enter
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 1268384 total (0.05%), 1276 samples
  [ 0] Channel.take
  [ 1] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 2] ChannelsGrid.lambda$forEachChannel$1
  [ 3] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 4] Dimensions.forEachRowCol
  [ 5] ChannelsGrid.forEachChannel
  [ 6] GameOfLife.calculateFrame
  [ 7] GameOfLife.lambda$calculateFrameBlocking$4
  [ 8] GameOfLife$$Lambda$53.0x0000000801035268.run
  [ 9] VirtualThread.run
  [10] VirtualThread$VThreadContinuation.lambda$new$0
  [11] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [12] Continuation.enter0
  [13] Continuation.enter
  [14] Continuation.enterSpecial
  [15] Continuation.run
  [16] VirtualThread.runContinuation
  [17] VirtualThread$$Lambda$51.0x000000080103df08.run
  [18] ForkJoinTask$RunnableExecuteAction.exec
  [19] ForkJoinTask.doExec
  [20] ForkJoinPool$WorkQueue.topLevelExec
  [21] ForkJoinPool.scan
  [22] ForkJoinPool.runWorker
  [23] ForkJoinWorkerThread.run

--- 1246831 total (0.05%), 1252 samples
  [ 0] Cell$$Lambda$60.0x0000000801036300.accept
  [ 1] ArrayList.forEach
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 1212479 total (0.05%), 1199 samples
  [ 0] update_cfs_group_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] ForkJoinPool.awaitWork
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 1192585 total (0.05%), 1188 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 1162591 total (0.05%), 1146 samples
  [ 0] update_curr_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] ForkJoinPool.awaitWork
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 1137532 total (0.05%), 1143 samples
  [ 0] AbstractOwnableSynchronizer.setExclusiveOwnerThread
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035268.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 1137048 total (0.05%), 1143 samples
  [ 0] ReentrantLock$Sync.isHeldExclusively
  [ 1] AbstractQueuedSynchronizer$ConditionObject.enableWait
  [ 2] AbstractQueuedSynchronizer$ConditionObject.await
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$53.0x0000000801035268.run
  [13] VirtualThread.run
  [14] VirtualThread$VThreadContinuation.lambda$new$0
  [15] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [16] Continuation.enter0
  [17] Continuation.enter
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 1078498 total (0.05%), 1082 samples
  [ 0] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 2] GameOfLife.lambda$calculateFrameBlocking$4
  [ 3] GameOfLife$$Lambda$53.0x0000000801035268.run
  [ 4] VirtualThread.run
  [ 5] VirtualThread$VThreadContinuation.lambda$new$0
  [ 6] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 7] Continuation.enter0
  [ 8] Continuation.enter
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 1072720 total (0.05%), 1058 samples
  [ 0] __schedule_[k]
  [ 1] schedule_[k]
  [ 2] futex_wait_queue_[k]
  [ 3] futex_wait_[k]
  [ 4] do_futex_[k]
  [ 5] __x64_sys_futex_[k]
  [ 6] do_syscall_64_[k]
  [ 7] entry_SYSCALL_64_after_hwframe_[k]
  [ 8] __futex_abstimed_wait_common
  [ 9] Unsafe.park
  [10] LockSupport.park
  [11] ForkJoinPool.awaitWork
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 1067413 total (0.05%), 1071 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 1024427 total (0.04%), 1025 samples
  [ 0] G1CollectedHeap::requires_barriers(stackChunkOopDesc*) const
  [ 1] Cont thaw
  [ 2] [not_walkable_Java]

--- 1018513 total (0.04%), 1021 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 974612 total (0.04%), 976 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$53.0x0000000801035268.run
  [ 6] VirtualThread.run
  [ 7] VirtualThread$VThreadContinuation.lambda$new$0
  [ 8] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 9] Continuation.enter0
  [10] Continuation.enter
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 974024 total (0.04%), 974 samples
  [ 0] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 1] [unknown_Java]

--- 970963 total (0.04%), 975 samples
  [ 0] AbstractQueuedSynchronizer.enqueue
  [ 1] AbstractQueuedSynchronizer$ConditionObject.doSignal
  [ 2] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 3] LockedSingleValue.put
  [ 4] Channel.put
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 962976 total (0.04%), 939 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] __futex_abstimed_wait_common
  [ 4] Unsafe.park
  [ 5] LockSupport.park
  [ 6] ForkJoinPool.awaitWork
  [ 7] ForkJoinPool.runWorker
  [ 8] ForkJoinWorkerThread.run

--- 945410 total (0.04%), 947 samples
  [ 0] Cell$$Lambda$70.0x0000000801037850.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$AdaptedRunnableAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 942149 total (0.04%), 947 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$60.0x0000000801036300.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 928151 total (0.04%), 932 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 912458 total (0.04%), 915 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 896094 total (0.04%), 901 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 877982 total (0.04%), 882 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$70.0x0000000801037850.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$AdaptedRunnableAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 864156 total (0.04%), 866 samples
  [ 0] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$53.0x0000000801035268.run
  [ 5] VirtualThread.run
  [ 6] VirtualThread$VThreadContinuation.lambda$new$0
  [ 7] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 8] Continuation.enter0
  [ 9] Continuation.enter
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 857814 total (0.04%), 862 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.put
  [ 2] Channel.put
  [ 3] Cell.lambda$notifyLiveness$0
  [ 4] Cell$$Lambda$60.0x0000000801036300.accept
  [ 5] ArrayList.forEach
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 849071 total (0.04%), 837 samples
  [ 0] update_load_avg_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] ForkJoinPool.awaitWork
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 840941 total (0.04%), 844 samples
  [ 0] VirtualThread.unpark
  [ 1] System$2.unparkVirtualThread
  [ 2] VirtualThreads.unpark
  [ 3] LockSupport.unpark
  [ 4] AbstractQueuedSynchronizer.signalNext
  [ 5] AbstractQueuedSynchronizer.release
  [ 6] ReentrantLock.unlock
  [ 7] LockedSingleValue.put
  [ 8] Channel.put
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 834616 total (0.04%), 830 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] ForkJoinPool.awaitWork
  [ 4] ForkJoinPool.runWorker
  [ 5] ForkJoinWorkerThread.run

--- 834565 total (0.04%), 837 samples
  [ 0] ObjArrayAllocator::initialize(HeapWordImpl**) const
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$53.0x0000000801035268.run
  [ 7] VirtualThread.run
  [ 8] VirtualThread$VThreadContinuation.lambda$new$0
  [ 9] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [10] Continuation.enter0
  [11] Continuation.enter
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 829459 total (0.04%), 832 samples
  [ 0] __memmove_sse2_unaligned_erms
  [ 1] long* thaw<Config<(oop_kind)0, G1BarrierSet> >(JavaThread*, int)
  [ 2] Cont thaw
  [ 3] [not_walkable_Java]

--- 811407 total (0.03%), 801 samples
  [ 0] dequeue_task_fair_[k]
  [ 1] __schedule_[k]
  [ 2] schedule_[k]
  [ 3] futex_wait_queue_[k]
  [ 4] futex_wait_[k]
  [ 5] do_futex_[k]
  [ 6] __x64_sys_futex_[k]
  [ 7] do_syscall_64_[k]
  [ 8] entry_SYSCALL_64_after_hwframe_[k]
  [ 9] __futex_abstimed_wait_common
  [10] Unsafe.park
  [11] LockSupport.park
  [12] ForkJoinPool.awaitWork
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 806325 total (0.03%), 810 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.await
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 4] ChannelsGrid.lambda$forEachChannel$1
  [ 5] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] GameOfLife.calculateFrame
  [ 9] GameOfLife.lambda$calculateFrameBlocking$4
  [10] GameOfLife$$Lambda$53.0x0000000801035268.run
  [11] VirtualThread.run
  [12] VirtualThread$VThreadContinuation.lambda$new$0
  [13] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [14] Continuation.enter0
  [15] Continuation.enter
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 806069 total (0.03%), 811 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$60.0x0000000801036300.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 802488 total (0.03%), 801 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$AdaptedRunnableAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 802126 total (0.03%), 803 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 801860 total (0.03%), 806 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.tryAcquire
  [ 2] AbstractQueuedSynchronizer.acquire
  [ 3] AbstractQueuedSynchronizer$ConditionObject.await
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035268.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 796226 total (0.03%), 788 samples
  [ 0] Parker::park(bool, long)
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] ForkJoinPool.awaitWork
  [ 5] ForkJoinPool.runWorker
  [ 6] ForkJoinWorkerThread.run

--- 783663 total (0.03%), 788 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 759046 total (0.03%), 763 samples
  [ 0] ForkJoinPool.scan
  [ 1] ForkJoinPool.runWorker
  [ 2] ForkJoinWorkerThread.run

--- 758089 total (0.03%), 745 samples
  [ 0] update_blocked_averages_[k]
  [ 1] newidle_balance_[k]
  [ 2] pick_next_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] ForkJoinPool.awaitWork
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 731757 total (0.03%), 734 samples
  [ 0] Unsafe.getAndBitwiseAndInt
  [ 1] AbstractQueuedSynchronizer$Node.getAndUnsetStatus
  [ 2] AbstractQueuedSynchronizer$ConditionObject.doSignal
  [ 3] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 726896 total (0.03%), 730 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$60.0x0000000801036300.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 719423 total (0.03%), 721 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 286822ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
  [ 1] int freeze<Config<(oop_kind)0, G1BarrierSet> >(JavaThread*, long*)
  [ 2] Continuation.enterSpecial
  [ 3] Continuation.run
  [ 4] VirtualThread.runContinuation
  [ 5] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 6] ForkJoinTask$RunnableExecuteAction.exec
  [ 7] ForkJoinTask.doExec
  [ 8] ForkJoinPool$WorkQueue.topLevelExec
  [ 9] ForkJoinPool.scan
  [10] ForkJoinPool.runWorker
  [11] ForkJoinWorkerThread.run

--- 710315 total (0.03%), 712 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$70.0x0000000801037850.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 697626 total (0.03%), 688 samples
  [ 0] __update_load_avg_cfs_rq_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] ForkJoinPool.awaitWork
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 688577 total (0.03%), 693 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$60.0x0000000801036300.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 686846 total (0.03%), 690 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$60.0x0000000801036300.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 680179 total (0.03%), 681 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 674046 total (0.03%), 677 samples
  [ 0] Channel.put
  [ 1] Cell.lambda$notifyLiveness$0
  [ 2] Cell$$Lambda$60.0x0000000801036300.accept
  [ 3] ArrayList.forEach
  [ 4] Cell.notifyLiveness
  [ 5] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 6] Iterable.forEach
  [ 7] CellsGroup.run
  [ 8] Continuation.enterSpecial
  [ 9] Continuation.run
  [10] VirtualThread.runContinuation
  [11] VirtualThread$$Lambda$51.0x000000080103df08.run
  [12] ForkJoinTask$RunnableExecuteAction.exec
  [13] ForkJoinTask.doExec
  [14] ForkJoinPool$WorkQueue.topLevelExec
  [15] ForkJoinPool.scan
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 671122 total (0.03%), 675 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$60.0x0000000801036300.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 671022 total (0.03%), 670 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 652422 total (0.03%), 657 samples
  [ 0] ForkJoinPool.scan
  [ 1] ForkJoinPool.runWorker
  [ 2] ForkJoinWorkerThread.run

--- 651672 total (0.03%), 656 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.enableWait
  [ 1] AbstractQueuedSynchronizer$ConditionObject.await
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035268.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 643887 total (0.03%), 617 samples
  [ 0] syscall_return_via_sysret_[k]
  [ 1] __futex_abstimed_wait_common
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] ForkJoinPool.awaitWork
  [ 5] ForkJoinPool.runWorker
  [ 6] ForkJoinWorkerThread.run

--- 642267 total (0.03%), 645 samples
  [ 0] ReentrantLock$Sync.isHeldExclusively
  [ 1] AbstractQueuedSynchronizer$ConditionObject.enableWait
  [ 2] AbstractQueuedSynchronizer$ConditionObject.await
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 641184 total (0.03%), 639 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$53.0x0000000801035268.run
  [ 6] VirtualThread.run
  [ 7] VirtualThread$VThreadContinuation.lambda$new$0
  [ 8] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 9] Continuation.enter0
  [10] Continuation.enter
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 640129 total (0.03%), 643 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] AbstractQueuedSynchronizer$ConditionObject.enableWait
  [ 3] AbstractQueuedSynchronizer$ConditionObject.await
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035268.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 626536 total (0.03%), 626 samples
  [ 0] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 625600 total (0.03%), 617 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_change_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] ForkJoinPool.signalWork
  [15] ForkJoinPool.scan
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 609206 total (0.03%), 606 samples
  [ 0] ___pthread_cond_wait
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] ForkJoinPool.awaitWork
  [ 4] ForkJoinPool.runWorker
  [ 5] ForkJoinWorkerThread.run

--- 608765 total (0.03%), 613 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$53.0x0000000801035268.run
  [10] VirtualThread.run
  [11] VirtualThread$VThreadContinuation.lambda$new$0
  [12] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [13] Continuation.enter0
  [14] Continuation.enter
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 601755 total (0.03%), 603 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$70.0x0000000801037850.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 596434 total (0.03%), 599 samples
  [ 0] ForkJoinPool$WorkQueue.push
  [ 1] ForkJoinPool.poolSubmit
  [ 2] ForkJoinPool.execute
  [ 3] VirtualThread.submitRunContinuation
  [ 4] VirtualThread.submitRunContinuation
  [ 5] VirtualThread.unpark
  [ 6] System$2.unparkVirtualThread
  [ 7] VirtualThreads.unpark
  [ 8] LockSupport.unpark
  [ 9] AbstractQueuedSynchronizer.signalNext
  [10] AbstractQueuedSynchronizer.release
  [11] ReentrantLock.unlock
  [12] LockedSingleValue.put
  [13] Channel.put
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 586205 total (0.02%), 588 samples
  [ 0] DirectMethodHandle.allocateInstance
  [ 1] DirectMethodHandle$Holder.newInvokeSpecial
  [ 2] Invokers$Holder.linkToTargetMethod
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 585164 total (0.02%), 588 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$60.0x0000000801036300.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 581330 total (0.02%), 582 samples
  [ 0] FreeListAllocator::reset()
  [ 1] HeapRegionRemSet::clear_locked(bool)
  [ 2] HeapRegion::hr_clear(bool)
  [ 3] G1CollectedHeap::free_region(HeapRegion*, FreeRegionList*)
  [ 4] FreeCSetClosure::do_heap_region(HeapRegion*)
  [ 5] G1CollectedHeap::par_iterate_regions_array(HeapRegionClosure*, HeapRegionClaimer*, unsigned int const*, unsigned long, unsigned int) const
  [ 6] G1PostEvacuateCollectionSetCleanupTask2::FreeCollectionSetTask::do_work(unsigned int)
  [ 7] G1BatchedTask::work(unsigned int)
  [ 8] WorkerThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 579986 total (0.02%), 572 samples
  [ 0] __update_load_avg_se_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] ForkJoinPool.awaitWork
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 577867 total (0.02%), 576 samples
  [ 0] freeze_epilog(JavaThread*, ContinuationWrapper&, freeze_result) [clone .part.0] [clone .isra.0]
  [ 1] Continuation.enterSpecial
  [ 2] Continuation.run
  [ 3] VirtualThread.runContinuation
  [ 4] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 5] ForkJoinTask$RunnableExecuteAction.exec
  [ 6] ForkJoinTask.doExec
  [ 7] ForkJoinPool$WorkQueue.topLevelExec
  [ 8] ForkJoinPool.scan
  [ 9] ForkJoinPool.runWorker
  [10] ForkJoinWorkerThread.run

--- 575173 total (0.02%), 570 samples
  [ 0] reweight_entity_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] ForkJoinPool.awaitWork
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 570322 total (0.02%), 566 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] Continuation.enterSpecial
  [ 3] Continuation.run
  [ 4] VirtualThread.runContinuation
  [ 5] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 6] ForkJoinTask$RunnableExecuteAction.exec
  [ 7] ForkJoinTask.doExec
  [ 8] ForkJoinPool$WorkQueue.topLevelExec
  [ 9] ForkJoinPool.scan
  [10] ForkJoinPool.runWorker
  [11] ForkJoinWorkerThread.run

--- 559169 total (0.02%), 550 samples
  [ 0] enqueue_entity_[k]
  [ 1] enqueue_task_fair_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] ForkJoinPool.signalWork
  [15] ForkJoinPool.scan
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 552197 total (0.02%), 554 samples
  [ 0] ForkJoinPool.scan
  [ 1] ForkJoinPool.runWorker
  [ 2] ForkJoinWorkerThread.run

--- 552030 total (0.02%), 555 samples
  [ 0] VirtualThread.start
  [ 1] VirtualThread.start
  [ 2] Thread.startVirtualThread
  [ 3] GameOfLife$$Lambda$42.0x0000000801034a20.accept
  [ 4] GameOfLife.calculateFrameBlocking
  [ 5] GameOfLifeBenchmark.benchmark
  [ 6] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
  [ 7] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_Throughput
  [ 8] DirectMethodHandle$Holder.invokeSpecial
  [ 9] LambdaForm$MH.0x000000080102e000.invoke
  [10] LambdaForm$MH.0x000000080102e400.invokeExact_MT
  [11] DirectMethodHandleAccessor.invokeImpl
  [12] DirectMethodHandleAccessor.invoke
  [13] Method.invoke
  [14] BenchmarkHandler$BenchmarkTask.call
  [15] BenchmarkHandler$BenchmarkTask.call
  [16] FutureTask.run
  [17] Executors$RunnableAdapter.call
  [18] FutureTask.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 548614 total (0.02%), 547 samples
  [ 0] Unsafe.park
  [ 1] LockSupport.park
  [ 2] ForkJoinPool.awaitWork
  [ 3] ForkJoinPool.runWorker
  [ 4] ForkJoinWorkerThread.run

--- 545639 total (0.02%), 546 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] Continuation.enterSpecial
  [ 3] Continuation.run
  [ 4] VirtualThread.runContinuation
  [ 5] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 6] ForkJoinTask$RunnableExecuteAction.exec
  [ 7] ForkJoinTask.doExec
  [ 8] ForkJoinPool$WorkQueue.topLevelExec
  [ 9] ForkJoinPool.scan
  [10] ForkJoinPool.runWorker
  [11] ForkJoinWorkerThread.run

--- 543772 total (0.02%), 547 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 540945 total (0.02%), 543 samples
  [ 0] G1Analytics::predict_scan_card_num(unsigned long, bool) const
  [ 1] G1Policy::predict_region_non_copy_time_ms(HeapRegion*, bool) const
  [ 2] G1CollectionSet::update_young_region_prediction(HeapRegion*, unsigned long)
  [ 3] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 4] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 5] G1RemSetSamplingTask::execute()
  [ 6] G1ServiceThread::run_task(G1ServiceTask*)
  [ 7] G1ServiceThread::run_service()
  [ 8] ConcurrentGCThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 524577 total (0.02%), 520 samples
  [ 0] dequeue_entity_[k]
  [ 1] dequeue_task_fair_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] ForkJoinPool.awaitWork
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 522751 total (0.02%), 522 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] Continuation.enterSpecial
  [ 3] Continuation.run
  [ 4] VirtualThread.runContinuation
  [ 5] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 6] ForkJoinTask$RunnableExecuteAction.exec
  [ 7] ForkJoinTask.doExec
  [ 8] ForkJoinPool$WorkQueue.topLevelExec
  [ 9] ForkJoinPool.scan
  [10] ForkJoinPool.runWorker
  [11] ForkJoinWorkerThread.run

--- 517442 total (0.02%), 518 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] Continuation.enterSpecial
  [ 3] Continuation.run
  [ 4] VirtualThread.runContinuation
  [ 5] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 6] ForkJoinTask$RunnableExecuteAction.exec
  [ 7] ForkJoinTask.doExec
  [ 8] ForkJoinPool$WorkQueue.topLevelExec
  [ 9] ForkJoinPool.scan
  [10] ForkJoinPool.runWorker
  [11] ForkJoinWorkerThread.run

--- 508231 total (0.02%), 509 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 503540 total (0.02%), 502 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] Continuation.enterSpecial
  [ 3] Continuation.run
  [ 4] VirtualThread.runContinuation
  [ 5] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 6] ForkJoinTask$RunnableExecuteAction.exec
  [ 7] ForkJoinTask.doExec
  [ 8] ForkJoinPool$WorkQueue.topLevelExec
  [ 9] ForkJoinPool.scan
  [10] ForkJoinPool.runWorker
  [11] ForkJoinWorkerThread.run

--- 502369 total (0.02%), 488 samples
  [ 0] __condvar_dec_grefs
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] ForkJoinPool.awaitWork
  [ 4] ForkJoinPool.runWorker
  [ 5] ForkJoinWorkerThread.run

--- 501343 total (0.02%), 502 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 497624 total (0.02%), 500 samples
  [ 0] AbstractQueuedSynchronizer.acquire
  [ 1] AbstractQueuedSynchronizer$ConditionObject.await
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$63.0x0000000801036960.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035268.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 496818 total (0.02%), 492 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] __futex_abstimed_wait_common
  [ 4] Unsafe.park
  [ 5] LockSupport.park
  [ 6] AbstractQueuedSynchronizer$ConditionNode.block
  [ 7] ForkJoinPool.unmanagedBlock
  [ 8] ForkJoinPool.managedBlock
  [ 9] AbstractQueuedSynchronizer$ConditionObject.await
  [10] LockedSingleValue.take
  [11] Channel.take
  [12] GameOfLife.calculateFrameBlocking
  [13] GameOfLifeBenchmark.benchmark
  [14] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
  [15] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_Throughput
  [16] DirectMethodHandle$Holder.invokeSpecial
  [17] LambdaForm$MH.0x000000080102e000.invoke
  [18] LambdaForm$MH.0x000000080102e400.invokeExact_MT
  [19] DirectMethodHandleAccessor.invokeImpl
  [20] DirectMethodHandleAccessor.invoke
  [21] Method.invoke
  [22] BenchmarkHandler$BenchmarkTask.call
  [23] BenchmarkHandler$BenchmarkTask.call
  [24] FutureTask.run
  [25] Executors$RunnableAdapter.call
  [26] FutureTask.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 496793 total (0.02%), 498 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$55.0x0000000801035a98.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 487568 total (0.02%), 488 samples
  [ 0] ArrayList$SubList$1.checkForComodification
  [ 1] ArrayList$SubList$1.next
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 485399 total (0.02%), 486 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 483647 total (0.02%), 479 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 286822ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
  [ 1] int freeze<Config<(oop_kind)0, G1BarrierSet> >(JavaThread*, long*)
  [ 2] Continuation.enterSpecial
  [ 3] Continuation.run
  [ 4] VirtualThread.runContinuation
  [ 5] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 6] ForkJoinTask$RunnableExecuteAction.exec
  [ 7] ForkJoinTask.doExec
  [ 8] ForkJoinPool$WorkQueue.topLevelExec
  [ 9] ForkJoinPool.scan
  [10] ForkJoinPool.runWorker
  [11] ForkJoinWorkerThread.run

--- 481164 total (0.02%), 481 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.doSignal
  [ 1] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] Cell.calculateNextState
  [ 5] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [ 6] Iterable.forEach
  [ 7] CellsGroup.run
  [ 8] Continuation.enterSpecial
  [ 9] Continuation.run
  [10] VirtualThread.runContinuation
  [11] VirtualThread$$Lambda$51.0x000000080103df08.run
  [12] ForkJoinTask$RunnableExecuteAction.exec
  [13] ForkJoinTask.doExec
  [14] ForkJoinPool$WorkQueue.topLevelExec
  [15] ForkJoinPool.scan
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 480713 total (0.02%), 484 samples
  [ 0] System$2.setExtentLocalCache
  [ 1] Continuation.run
  [ 2] VirtualThread.runContinuation
  [ 3] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 4] ForkJoinTask$RunnableExecuteAction.exec
  [ 5] ForkJoinTask.doExec
  [ 6] ForkJoinPool$WorkQueue.topLevelExec
  [ 7] ForkJoinPool.scan
  [ 8] ForkJoinPool.runWorker
  [ 9] ForkJoinWorkerThread.run

--- 480483 total (0.02%), 483 samples
  [ 0] ForkJoinPool$WorkQueue.push
  [ 1] ForkJoinPool.poolSubmit
  [ 2] ForkJoinPool.execute
  [ 3] VirtualThread.submitRunContinuation
  [ 4] VirtualThread.submitRunContinuation
  [ 5] VirtualThread.unpark
  [ 6] System$2.unparkVirtualThread
  [ 7] VirtualThreads.unpark
  [ 8] LockSupport.unpark
  [ 9] AbstractQueuedSynchronizer.signalNext
  [10] AbstractQueuedSynchronizer.release
  [11] ReentrantLock.unlock
  [12] LockedSingleValue.put
  [13] Channel.put
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 467870 total (0.02%), 465 samples
  [ 0] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 1] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 2] G1RemSetSamplingTask::execute()
  [ 3] G1ServiceThread::run_task(G1ServiceTask*)
  [ 4] G1ServiceThread::run_service()
  [ 5] ConcurrentGCThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 467515 total (0.02%), 465 samples
  [ 0] int freeze<Config<(oop_kind)0, G1BarrierSet> >(JavaThread*, long*)
  [ 1] Continuation.enterSpecial
  [ 2] Continuation.run
  [ 3] VirtualThread.runContinuation
  [ 4] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 5] ForkJoinTask$RunnableExecuteAction.exec
  [ 6] ForkJoinTask.doExec
  [ 7] ForkJoinPool$WorkQueue.topLevelExec
  [ 8] ForkJoinPool.scan
  [ 9] ForkJoinPool.runWorker
  [10] ForkJoinWorkerThread.run

--- 465088 total (0.02%), 466 samples
  [ 0] ForkJoinPool$WorkQueue.casSlotToNull
  [ 1] ForkJoinPool.scan
  [ 2] ForkJoinPool.runWorker
  [ 3] ForkJoinWorkerThread.run

--- 460858 total (0.02%), 457 samples
  [ 0] update_sd_lb_stats.constprop.0_[k]
  [ 1] find_busiest_group_[k]
  [ 2] load_balance_[k]
  [ 3] newidle_balance_[k]
  [ 4] pick_next_task_fair_[k]
  [ 5] __schedule_[k]
  [ 6] schedule_[k]
  [ 7] futex_wait_queue_[k]
  [ 8] futex_wait_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] __futex_abstimed_wait_common
  [14] Unsafe.park
  [15] LockSupport.park
  [16] ForkJoinPool.awaitWork
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 457512 total (0.02%), 451 samples
  [ 0] cpuacct_charge_[k]
  [ 1] update_curr_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] ForkJoinPool.awaitWork
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 450347 total (0.02%), 453 samples
  [ 0] update_register_map1(ImmutableOopMap const*, frame const*, RegisterMap*)
  [ 1] vframeStream::vframeStream(JavaThread*, bool, bool, bool)
  [ 2] SharedRuntime::find_callee_method(JavaThread*)
  [ 3] SharedRuntime::reresolve_call_site(JavaThread*)
  [ 4] SharedRuntime::handle_wrong_method(JavaThread*)
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 448394 total (0.02%), 446 samples
  [ 0] LinkResolver::resolve_continuation_enter(CallInfo&, JavaThread*)
  [ 1] SharedRuntime::find_callee_info_helper(vframeStream&, Bytecodes::Code&, CallInfo&, JavaThread*)
  [ 2] SharedRuntime::find_callee_method(JavaThread*)
  [ 3] SharedRuntime::reresolve_call_site(JavaThread*)
  [ 4] SharedRuntime::handle_wrong_method(JavaThread*)
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 439059 total (0.02%), 422 samples
  [ 0] fpregs_restore_userregs_[k]
  [ 1] exit_to_user_mode_prepare_[k]
  [ 2] syscall_exit_to_user_mode_[k]
  [ 3] do_syscall_64_[k]
  [ 4] entry_SYSCALL_64_after_hwframe_[k]
  [ 5] __futex_abstimed_wait_common
  [ 6] Unsafe.park
  [ 7] LockSupport.park
  [ 8] ForkJoinPool.awaitWork
  [ 9] ForkJoinPool.runWorker
  [10] ForkJoinWorkerThread.run

--- 438085 total (0.02%), 440 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$70.0x0000000801037850.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$64.0x0000000801036b98.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$AdaptedRunnableAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

       total  percent  samples  top
  ----------  -------  -------  ---
   877367125   37.23%   876447  ReduceOps$5ReducingSink.get
   151138190    6.41%   150844  AbstractPipeline.<init>
   149054773    6.32%   149667  AbstractQueuedSynchronizer.compareAndSetState
   120299980    5.10%   120430  Sink$ChainedReference.<init>
   110441103    4.69%   110741  Cell$$Lambda$70.0x0000000801037850.apply
    79706987    3.38%    80038  AbstractQueuedSynchronizer.release
    73991797    3.14%    73850  StreamOpFlag.fromCharacteristics
    67958460    2.88%    67956  ReferencePipeline$4.opWrapSink
    64002943    2.72%    64260  Cell.lambda$notifyLiveness$0
    60854830    2.58%    60907  ReferencePipeline$3.opWrapSink
    57012599    2.42%    57028  vtable stub
    51997527    2.21%    52177  LockedSingleValue.take
    50638733    2.15%    50801  Channel.take
    43724126    1.86%    43834  ReferencePipeline$3$1.accept
    37425449    1.59%    37463  Sink$ChainedReference.begin
    28887935    1.23%    28901  ArrayList$ArrayListSpliterator.<init>
    21228714    0.90%    21137  StreamSupport.stream
     8765373    0.37%     8767  AbstractPipeline.wrapSink
     8371443    0.36%     8379  ReferencePipeline.map
     7648592    0.32%     7676  ReentrantLock$NonfairSync.initialTryLock
     7268342    0.31%     7172  vframeStream::vframeStream(JavaThread*, bool, bool, bool)
     6821757    0.29%     6848  AbstractQueuedSynchronizer.signalNext
     6701651    0.28%     6732  AbstractOwnableSynchronizer.getExclusiveOwnerThread
     6468765    0.27%     6464  StreamOpFlag.getMask
     6259120    0.27%     6285  AbstractQueuedSynchronizer$ConditionObject.signal
     5683278    0.24%     5707  TickPerCell.lambda$tick$0
     5182596    0.22%     5203  ReferencePipeline$4$1.accept
     5167556    0.22%     5151  Cell.calculateNextState
     5128685    0.22%     5153  ReentrantLock$Sync.isHeldExclusively
     5003199    0.21%     4963  __memset_avx2_unaligned_erms
     4985435    0.21%     4977  Object.<init>
     4945761    0.21%     4961  ReentrantLock$Sync.lock
     4480007    0.19%     4499  ReentrantLock$Sync.tryRelease
     4032568    0.17%     4053  LockedSingleValue.put
     3942770    0.17%     3897  psi_group_change_[k]
     3455391    0.15%     3478  ForkJoinPool.scan
     3381323    0.14%     3366  Collection.stream
     3221054    0.14%     3236  AbstractQueuedSynchronizer.setState
     3050228    0.13%     3040  Iterable.forEach
     2942690    0.12%     2953  ReentrantLock.lock
     2856480    0.12%     2870  ChannelsGrid.getChannel
     2718135    0.12%     2675  update_blocked_averages_[k]
     2663143    0.11%     2627  __update_load_avg_cfs_rq_[k]
     2626528    0.11%     2626  ArrayList$SubList$1.next
     2558992    0.11%     2517  syscall_exit_to_user_mode_[k]
     2552541    0.11%     2559  CellsGroup$$Lambda$55.0x0000000801035a98.accept
     2379482    0.10%     2385  ArrayList.forEach
     2312739    0.10%     2320  MemAllocator::allocate() const
     2217345    0.09%     2219  Dimensions.forEachRowCol
     2167090    0.09%     2181  AbstractQueuedSynchronizer.acquire
     2135683    0.09%     2144  AbstractQueuedSynchronizer$ConditionObject.enableWait
     2130153    0.09%     2104  update_load_avg_[k]
     2102785    0.09%     2081  update_cfs_group_[k]
     2090509    0.09%     2092  StreamOpFlag.combineOpFlags
     2083530    0.09%     2095  ForkJoinPool$WorkQueue.push
     2054345    0.09%     2060  Cell.notifyLiveness
     1901602    0.08%     1898  ReferencePipeline.mapToInt
     1891260    0.08%     1899  AbstractQueuedSynchronizer.getState
     1861490    0.08%     1837  update_curr_[k]
     1779285    0.08%     1782  ArrayList$ArrayListSpliterator.forEachRemaining
     1777750    0.08%     1780  AbstractOwnableSynchronizer.setExclusiveOwnerThread
     1768617    0.08%     1757  InstanceKlass::find_method_index(Array<Method*> const*, Symbol const*, Symbol const*, Klass::OverpassLookupMode, Klass::StaticLookupMode, Klass::PrivateLookupMode) [clone .constprop.0]
     1749660    0.07%     1742  G1CardSet::occupied() const
     1743661    0.07%     1739  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 286822ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
     1639724    0.07%     1646  Cell$$Lambda$60.0x0000000801036300.accept
     1631757    0.07%     1636  AbstractQueuedSynchronizer.enqueue
     1610285    0.07%     1616  ReduceOps$5ReducingSink.accept
     1606570    0.07%     1606  CellsGroup$$Lambda$64.0x0000000801036b98.accept
     1588539    0.07%     1596  Boolean.booleanValue
     1555908    0.07%     1538  __update_load_avg_se_[k]
     1466237    0.06%     1472  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
     1453776    0.06%     1456  G1CollectedHeap::requires_barriers(stackChunkOopDesc*) const
     1448872    0.06%     1453  VirtualThread.unpark
     1443598    0.06%     1442  __tls_get_addr
     1436911    0.06%     1443  Unsafe.getAndBitwiseAndInt
     1432542    0.06%     1415  enqueue_entity_[k]
     1345978    0.06%     1348  __memmove_sse2_unaligned_erms
     1306086    0.06%     1284  __entry_text_start_[k]
     1246432    0.05%     1236  reweight_entity_[k]
     1243210    0.05%     1250  AbstractQueuedSynchronizer$ConditionObject.await
     1237213    0.05%     1221  __schedule_[k]
     1180442    0.05%     1184  ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
     1167721    0.05%     1171  ObjArrayAllocator::initialize(HeapWordImpl**) const
     1140396    0.05%     1138  int freeze<Config<(oop_kind)0, G1BarrierSet> >(JavaThread*, long*)
     1131969    0.05%     1127  Unsafe_Park
     1097387    0.05%     1085  enqueue_task_fair_[k]
     1068958    0.05%     1066  ___pthread_cond_wait
     1053427    0.04%     1056  TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
     1012650    0.04%     1004  _raw_spin_lock_[k]
      987768    0.04%      988  AbstractQueuedSynchronizer$ConditionObject.doSignal
      986421    0.04%      991  ForkJoinPool.awaitWork
      979706    0.04%      972  Parker::park(bool, long)
      911997    0.04%      901  dequeue_task_fair_[k]
      893189    0.04%      891  freeze_epilog(JavaThread*, ContinuationWrapper&, freeze_result) [clone .part.0] [clone .isra.0]
      885793    0.04%      891  VirtualThread.runContinuation
      874214    0.04%      879  VirtualThread.compareAndSetState
      848747    0.04%      815  syscall_return_via_sysret_[k]
      841223    0.04%      846  VirtualThread.unmount
      822188    0.03%      809  check_preemption_disabled_[k]
      805514    0.03%      807  G1ParScanThreadState::trim_queue_to_threshold(unsigned int)
      798359    0.03%      799  Unsafe.park
      787523    0.03%      775  futex_wake_[k]
      776545    0.03%      780  ForkJoinPool.signalWork
      766297    0.03%      768  Channel.put
      750454    0.03%      761  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<548964ul, G1BarrierSet>, (AccessInternal::BarrierType)2, 548964ul>::oop_access_barrier(void*)
      739374    0.03%      732  select_task_rq_fair_[k]
      737214    0.03%      728  update_rq_clock_[k]
      734449    0.03%      733  ArrayList$SubList$1.checkForComodification
      727966    0.03%      727  G1Analytics::predict_scan_card_num(unsigned long, bool) const
      716870    0.03%      726  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)3, 286822ul>::oop_access_barrier(oopDesc*, long)
      707501    0.03%      708  ChannelsGrid$$Lambda$66.0x0000000801036fc8.accept
      698481    0.03%      702  Unsafe.getAndBitwiseOrInt
      686968    0.03%      672  __condvar_dec_grefs
      671509    0.03%      673  SafepointMechanism::update_poll_values(JavaThread*)
      652461    0.03%      655  AbsSeq::dsd() const
      642249    0.03%      644  MemAllocator::Allocation::notify_allocation_jvmti_sampler()
      635234    0.03%      628  native_sched_clock_[k]
      634704    0.03%      636  Sink$ChainedReference.end
      632054    0.03%      597  fpregs_restore_userregs_[k]
      620174    0.03%      622  AbstractQueuedSynchronizer.casTail
      613293    0.03%      615  DirectMethodHandle.allocateInstance
      611195    0.03%      610  LinkResolver::resolve_continuation_enter(CallInfo&, JavaThread*)
      603535    0.03%      579  restore_fpregs_from_fpstate_[k]
      599805    0.03%      604  pthread_mutex_trylock@@GLIBC_2.34
      591817    0.03%      587  dequeue_entity_[k]
      586996    0.02%      587  __GI___pthread_mutex_lock
      581330    0.02%      582  FreeListAllocator::reset()
      570306    0.02%      564  futex_q_lock_[k]
      557016    0.02%      550  cpuacct_charge_[k]
      555562    0.02%      558  Continuation.yield0
      552030    0.02%      555  VirtualThread.start
      542566    0.02%      502  __get_user_8_[k]
      540897    0.02%      536  __calc_delta_[k]
      526879    0.02%      527  __pthread_mutex_unlock_usercnt
      521293    0.02%      525  System$2.setExtentLocalCache
      521275    0.02%      524  update_register_map1(ImmutableOopMap const*, frame const*, RegisterMap*)
      513513    0.02%      513  Continuation::prepare_thaw(JavaThread*, bool)
      508600    0.02%      503  psi_task_change_[k]
      507333    0.02%      507  OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
      483677    0.02%      477  futex_wake_mark_[k]
      479046    0.02%      480  ForkJoinPool$WorkQueue.casSlotToNull
      478932    0.02%      479  oopDesc::address_field(int) const
      478809    0.02%      475  update_sd_lb_stats.constprop.0_[k]
      470005    0.02%      466  G1Policy::preventive_collection_required(unsigned int)
      468860    0.02%      466  G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
      463729    0.02%      456  try_to_wake_up_[k]
      463639    0.02%      464  FreezeBase::freeze_fast_copy(stackChunkOopDesc*, int)
      454059    0.02%      408  exit_to_user_mode_prepare_[k]
      452331    0.02%      453  LinkResolver::check_method_loader_constraints(LinkInfo const&, methodHandle const&, char const*, JavaThread*)
      450716    0.02%      447  __futex_abstimed_wait_common
      450119    0.02%      448  System$2.unparkVirtualThread
      442425    0.02%      436  rcu_sched_clock_irq_[k]
      434579    0.02%      425  _raw_spin_lock_irqsave_[k]
      430332    0.02%      426  update_irq_load_avg_[k]
      422711    0.02%      423  PipelineHelper.<init>
      418613    0.02%      412  rb_next_[k]
      415738    0.02%      414  IntPipeline.reduce
      412231    0.02%      414  G1Analytics::predict_card_merge_time_ms(unsigned long, bool) const
      410826    0.02%      410  G1FromCardCache::clear(unsigned int)
      406649    0.02%      401  iterate_groups_[k]
      405298    0.02%      403  HeapRegionManager::allocate_free_region(HeapRegionType, unsigned int)
      405259    0.02%      406  Klass::check_array_allocation_length(int, int, JavaThread*)
      397247    0.02%      399  Thaw<Config<(oop_kind)0, G1BarrierSet> >::thaw_fast(stackChunkOopDesc*)
      396475    0.02%      392  newidle_balance_[k]
      395216    0.02%      397  long* thaw<Config<(oop_kind)0, G1BarrierSet> >(JavaThread*, int)
      386353    0.02%      382  available_idle_cpu_[k]
      383914    0.02%      385  ThreadsListHandle::cv_internal_thread_to_JavaThread(_jobject*, JavaThread**, oopDesc**)
      380447    0.02%      375  __cgroup_account_cputime_[k]
      370961    0.02%      374  ___pthread_cond_signal
      365293    0.02%      366  MemAllocator::Allocation::check_out_of_memory()
      365215    0.02%      359  __hrtimer_run_queues_[k]
      355088    0.02%      356  Freeze<Config<(oop_kind)0, G1BarrierSet> >::allocate_chunk(unsigned long)
      349827    0.01%      352  VirtualThread.afterYield
      344585    0.01%      340  resched_curr_[k]
      341411    0.01%      343  ThreadPoolExecutor$Worker.tryRelease
      337440    0.01%      336  Unsafe_Unpark
      335315    0.01%      332  preempt_count_add_[k]
      330587    0.01%      332  LinkResolver::lookup_method_in_klasses(LinkInfo const&, bool, bool)
      328801    0.01%      330  ChannelsGrid$$Lambda$59.0x00000008010360e0.accept
      318334    0.01%      309  futex_wait_[k]
      316636    0.01%      314  update_min_vruntime_[k]
      316222    0.01%      312  AbsSeq::davg() const
      315896    0.01%      287  __rseq_handle_notify_resume_[k]
      314769    0.01%      321  java_lang_Thread::set_thread_status(oopDesc*, JavaThreadStatus)
      311132    0.01%      305  preempt_count_sub_[k]
      294757    0.01%      297  OopMapStream::find_next() [clone .part.0]
      294080    0.01%      290  timerqueue_add_[k]
      291566    0.01%      292  MemAllocator::Allocation::notify_allocation_jfr_sampler()
      291462    0.01%      289  RegisterMap::RegisterMap(JavaThread*, bool, bool, bool)
      290742    0.01%      287  __perf_event_task_sched_out_[k]
      290566    0.01%      288  G1Allocator::unsafe_max_tlab_alloc()
      290286    0.01%      291  void OopOopIterateBackwardsDispatch<G1ScanEvacuatedObjClosure>::Table::oop_oop_iterate_backwards<InstanceKlass, narrowOop>(G1ScanEvacuatedObjClosure*, oopDesc*, Klass*)
      280844    0.01%      279  JavaThread::threadObj() const
      279510    0.01%      281  ForkJoinPool$WorkQueue.getAndSetAccess
      279376    0.01%      281  VirtualThread.park
      278580    0.01%      275  psi_task_switch_[k]
      271342    0.01%      270  G1CardSet::clear()
      270849    0.01%      266  asm_sysvec_apic_timer_interrupt_[k]
      269831    0.01%      265  error_entry_[k]
      268680    0.01%      272  native_write_msr_[k]
