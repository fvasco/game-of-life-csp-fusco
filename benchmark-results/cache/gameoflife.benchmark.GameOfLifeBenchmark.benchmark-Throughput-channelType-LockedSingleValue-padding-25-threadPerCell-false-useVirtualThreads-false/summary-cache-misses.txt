--- Execution profile ---
Total samples       : 9592582
unknown_Java        : 31742 (0.33%)
not_walkable_Java   : 186 (0.00%)
deoptimization      : 26 (0.00%)
skipped             : 1 (0.00%)

--- 1515549802 total (15.80%), 1515878 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$53.0x0000000801036510.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 1241205779 total (12.94%), 1238937 samples
  [ 0] ReduceOps$5ReducingSink.get
  [ 1] ReduceOps$5ReducingSink.get
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 1039781964 total (10.84%), 1041578 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$53.0x0000000801036510.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 934964845 total (9.75%), 937487 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 406372752 total (4.24%), 406610 samples
  [ 0] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 404157787 total (4.21%), 404685 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] Cell.lambda$notifyLiveness$0
  [ 5] Cell$$Lambda$53.0x0000000801036510.accept
  [ 6] ArrayList.forEach
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 289825631 total (3.02%), 290314 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 226777982 total (2.36%), 227060 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 166485473 total (1.74%), 166639 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 158182018 total (1.65%), 158285 samples
  [ 0] TickPerCell.lambda$tick$0
  [ 1] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 2] ChannelsGrid.lambda$forEachChannel$0
  [ 3] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 4] Dimensions.forEachRowCol
  [ 5] ChannelsGrid.forEachChannel
  [ 6] TickPerCell.tick
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 140845902 total (1.47%), 141177 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 95689688 total (1.00%), 95562 samples
  [ 0] Channel.take
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 95293090 total (0.99%), 95258 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 91772590 total (0.96%), 91956 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 90106253 total (0.94%), 90070 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 86695488 total (0.90%), 86691 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] TickPerCell.lambda$tick$0
  [ 5] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 6] ChannelsGrid.lambda$forEachChannel$0
  [ 7] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] TickPerCell.tick
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 69922707 total (0.73%), 69838 samples
  [ 0] ReferencePipeline$StatelessOp.<init>
  [ 1] ReferencePipeline$3.<init>
  [ 2] ReferencePipeline.map
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 64858588 total (0.68%), 64776 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 62669802 total (0.65%), 62718 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 58202269 total (0.61%), 58171 samples
  [ 0] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 55964214 total (0.58%), 56048 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 54754688 total (0.57%), 54897 samples
  [ 0] ReentrantLock$Sync.lock
  [ 1] ReentrantLock.lock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 53860037 total (0.56%), 53711 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 53099441 total (0.55%), 52953 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 52941417 total (0.55%), 52926 samples
  [ 0] vtable stub
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 48328374 total (0.50%), 48238 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 44583461 total (0.46%), 44393 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 43337745 total (0.45%), 43308 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 37363902 total (0.39%), 37337 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 31941289 total (0.33%), 31948 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 31875730 total (0.33%), 31896 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 31568002 total (0.33%), 31589 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 30254062 total (0.32%), 30299 samples
  [ 0] Channel.take
  [ 1] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 2] ChannelsGrid.lambda$forEachChannel$1
  [ 3] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 4] Dimensions.forEachRowCol
  [ 5] ChannelsGrid.forEachChannel
  [ 6] GameOfLife.calculateFrame
  [ 7] GameOfLife.lambda$calculateFrameBlocking$4
  [ 8] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 9] ThreadPoolExecutor.runWorker
  [10] ThreadPoolExecutor$Worker.run
  [11] Thread.run

--- 30059024 total (0.31%), 30119 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$53.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 28840105 total (0.30%), 28805 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 27355224 total (0.29%), 27372 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 26603181 total (0.28%), 26496 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 26458251 total (0.28%), 26494 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 24569509 total (0.26%), 24643 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 23116744 total (0.24%), 23039 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 23113980 total (0.24%), 23128 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 21238063 total (0.22%), 21258 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.put
  [ 4] Channel.put
  [ 5] Cell.lambda$notifyLiveness$0
  [ 6] Cell$$Lambda$53.0x0000000801036510.accept
  [ 7] ArrayList.forEach
  [ 8] Cell.notifyLiveness
  [ 9] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 20909369 total (0.22%), 20943 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$53.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 20620164 total (0.21%), 20667 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$53.0x0000000801036510.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 19840679 total (0.21%), 19866 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 19660663 total (0.20%), 19699 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 19503886 total (0.20%), 19480 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 19181351 total (0.20%), 19207 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 18883295 total (0.20%), 18903 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 18869677 total (0.20%), 18901 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 18733509 total (0.20%), 18702 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 17184782 total (0.18%), 17178 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 16574211 total (0.17%), 16562 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 15632037 total (0.16%), 15652 samples
  [ 0] Channel.put
  [ 1] Cell.lambda$notifyLiveness$0
  [ 2] Cell$$Lambda$53.0x0000000801036510.accept
  [ 3] ArrayList.forEach
  [ 4] Cell.notifyLiveness
  [ 5] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 6] Iterable.forEach
  [ 7] CellsGroup.run
  [ 8] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 9] ThreadPoolExecutor.runWorker
  [10] ThreadPoolExecutor$Worker.run
  [11] Thread.run

--- 15556208 total (0.16%), 15588 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.put
  [ 2] Channel.put
  [ 3] Cell.lambda$notifyLiveness$0
  [ 4] Cell$$Lambda$53.0x0000000801036510.accept
  [ 5] ArrayList.forEach
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 15399537 total (0.16%), 15447 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 15315624 total (0.16%), 15309 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 15079663 total (0.16%), 15124 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 15065158 total (0.16%), 15109 samples
  [ 0] ReentrantLock$Sync.isHeldExclusively
  [ 1] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 14974881 total (0.16%), 15017 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 14561471 total (0.15%), 14602 samples
  [ 0] ReentrantLock.lock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 14112868 total (0.15%), 14154 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 13979447 total (0.15%), 13938 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 13850140 total (0.14%), 13893 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 13654663 total (0.14%), 13700 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 13554652 total (0.14%), 13572 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 12953104 total (0.14%), 12954 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 12755410 total (0.13%), 12786 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$53.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 12676686 total (0.13%), 12715 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 12601814 total (0.13%), 12612 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.put
  [ 2] Channel.put
  [ 3] Cell.lambda$notifyLiveness$0
  [ 4] Cell$$Lambda$53.0x0000000801036510.accept
  [ 5] ArrayList.forEach
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 12587932 total (0.13%), 12599 samples
  [ 0] Cell$$Lambda$53.0x0000000801036510.accept
  [ 1] ArrayList.forEach
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 12266686 total (0.13%), 12298 samples
  [ 0] AbstractQueuedSynchronizer.getState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$53.0x0000000801036510.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 12241541 total (0.13%), 12226 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 11165386 total (0.12%), 10846 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] AbstractQueuedSynchronizer$ConditionNode.block
  [14] ForkJoinPool.unmanagedBlock
  [15] ForkJoinPool.managedBlock
  [16] AbstractQueuedSynchronizer$ConditionObject.await
  [17] LockedSingleValue.take
  [18] Channel.take
  [19] TickPerCell.waitTick
  [20] Cell.notifyLiveness
  [21] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [22] Iterable.forEach
  [23] CellsGroup.run
  [24] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [25] ThreadPoolExecutor.runWorker
  [26] ThreadPoolExecutor$Worker.run
  [27] Thread.run

--- 11114199 total (0.12%), 11091 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 11048610 total (0.12%), 11037 samples
  [ 0] ArrayList$ArrayListSpliterator.characteristics
  [ 1] StreamOpFlag.fromCharacteristics
  [ 2] StreamSupport.stream
  [ 3] Collection.stream
  [ 4] Cell.calculateNextState
  [ 5] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 6] Iterable.forEach
  [ 7] CellsGroup.run
  [ 8] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 9] ThreadPoolExecutor.runWorker
  [10] ThreadPoolExecutor$Worker.run
  [11] Thread.run

--- 10824662 total (0.11%), 10859 samples
  [ 0] ReentrantLock.unlock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 10501767 total (0.11%), 10529 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$53.0x0000000801036510.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 10371190 total (0.11%), 10363 samples
  [ 0] ReferencePipeline.<init>
  [ 1] ReferencePipeline$Head.<init>
  [ 2] StreamSupport.stream
  [ 3] Collection.stream
  [ 4] Cell.calculateNextState
  [ 5] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 6] Iterable.forEach
  [ 7] CellsGroup.run
  [ 8] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 9] ThreadPoolExecutor.runWorker
  [10] ThreadPoolExecutor$Worker.run
  [11] Thread.run

--- 10179777 total (0.11%), 10178 samples
  [ 0] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 1] Dimensions.forEachRowCol
  [ 2] ChannelsGrid.forEachChannel
  [ 3] TickPerCell.tick
  [ 4] GameOfLife.calculateFrame
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 10130043 total (0.11%), 10106 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 9820750 total (0.10%), 9844 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 9499034 total (0.10%), 9512 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 9267813 total (0.10%), 9284 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 9209513 total (0.10%), 9224 samples
  [ 0] Cell$$Lambda$59.0x00000008010371e8.applyAsInt
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 8987009 total (0.09%), 9002 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 8544849 total (0.09%), 8563 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$53.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 8495647 total (0.09%), 8511 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 8380153 total (0.09%), 8384 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 8338690 total (0.09%), 8346 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 8275366 total (0.09%), 8288 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 8193910 total (0.09%), 8209 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 8175299 total (0.09%), 8185 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 8060993 total (0.08%), 8059 samples
  [ 0] ReentrantLock$Sync.lock
  [ 1] ReentrantLock.lock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 7978366 total (0.08%), 7976 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 7787735 total (0.08%), 7721 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 7540144 total (0.08%), 7537 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 7498377 total (0.08%), 7496 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 7246966 total (0.08%), 7262 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$53.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 7208910 total (0.08%), 7208 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 7119725 total (0.07%), 7127 samples
  [ 0] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 1] Dimensions.forEachRowCol
  [ 2] ChannelsGrid.forEachChannel
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 7109689 total (0.07%), 7121 samples
  [ 0] ReentrantLock$Sync.lock
  [ 1] ReentrantLock.lock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 7071763 total (0.07%), 7082 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 7070748 total (0.07%), 7079 samples
  [ 0] Channel.put
  [ 1] Cell.lambda$notifyLiveness$0
  [ 2] Cell$$Lambda$53.0x0000000801036510.accept
  [ 3] ArrayList.forEach
  [ 4] Cell.notifyLiveness
  [ 5] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 6] Iterable.forEach
  [ 7] CellsGroup.run
  [ 8] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 9] ThreadPoolExecutor.runWorker
  [10] ThreadPoolExecutor$Worker.run
  [11] Thread.run

--- 6750777 total (0.07%), 6759 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 6273095 total (0.07%), 6168 samples
  [ 0] ThreadPoolExecutor.runWorker
  [ 1] ThreadPoolExecutor$Worker.run
  [ 2] Thread.run

--- 6104790 total (0.06%), 6109 samples
  [ 0] Cell$$Lambda$53.0x0000000801036510.accept
  [ 1] ArrayList.forEach
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 6096912 total (0.06%), 6098 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 5970725 total (0.06%), 5984 samples
  [ 0] ReentrantLock.lock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 5858740 total (0.06%), 5861 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 5754178 total (0.06%), 5758 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 5750867 total (0.06%), 5756 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 5673715 total (0.06%), 5694 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] ChannelsGrid.lambda$forEachChannel$1
  [ 2] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 3] Dimensions.forEachRowCol
  [ 4] ChannelsGrid.forEachChannel
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 5629710 total (0.06%), 5625 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 5477318 total (0.06%), 5461 samples
  [ 0] DirectMethodHandle.allocateInstance
  [ 1] DirectMethodHandle$Holder.newInvokeSpecial
  [ 2] Invokers$Holder.linkToTargetMethod
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 5376389 total (0.06%), 5386 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 5338137 total (0.06%), 5344 samples
  [ 0] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 5016144 total (0.05%), 4977 samples
  [ 0] ThreadPoolExecutor.runWorker
  [ 1] ThreadPoolExecutor$Worker.run
  [ 2] Thread.run

--- 5003552 total (0.05%), 4981 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 4724953 total (0.05%), 4714 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 4531166 total (0.05%), 4544 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 4437124 total (0.05%), 4431 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 4385440 total (0.05%), 4394 samples
  [ 0] ReentrantLock.lock
  [ 1] LockedSingleValue.put
  [ 2] Channel.put
  [ 3] Cell.lambda$notifyLiveness$0
  [ 4] Cell$$Lambda$53.0x0000000801036510.accept
  [ 5] ArrayList.forEach
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 4359704 total (0.05%), 4351 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 4254158 total (0.04%), 4258 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] [unknown_Java]

--- 4162607 total (0.04%), 4154 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 4117294 total (0.04%), 4122 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 4068426 total (0.04%), 4080 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 4012426 total (0.04%), 4004 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 4009423 total (0.04%), 4014 samples
  [ 0] AbstractOwnableSynchronizer.setExclusiveOwnerThread
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 3962389 total (0.04%), 3964 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 3884586 total (0.04%), 3881 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 3835681 total (0.04%), 3844 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 3789786 total (0.04%), 3790 samples
  [ 0] ArrayList.forEach
  [ 1] [unknown_Java]

--- 3729045 total (0.04%), 3737 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 3683345 total (0.04%), 3687 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 3661532 total (0.04%), 3670 samples
  [ 0] ReentrantLock$Sync.isHeldExclusively
  [ 1] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3615970 total (0.04%), 3614 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] [unknown_Java]

--- 3609143 total (0.04%), 3617 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$53.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 3598553 total (0.04%), 3594 samples
  [ 0] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 1] [unknown_Java]

--- 3591144 total (0.04%), 3598 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$53.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 3465446 total (0.04%), 3467 samples
  [ 0] AbstractQueuedSynchronizer.getState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 3448974 total (0.04%), 3451 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.isHeldExclusively
  [ 2] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 3] LockedSingleValue.put
  [ 4] Channel.put
  [ 5] TickPerCell.lambda$tick$0
  [ 6] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 7] ChannelsGrid.lambda$forEachChannel$0
  [ 8] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] TickPerCell.tick
  [12] GameOfLife.calculateFrame
  [13] GameOfLife.lambda$calculateFrameBlocking$4
  [14] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 3419911 total (0.04%), 3429 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.put
  [ 4] Channel.put
  [ 5] TickPerCell.lambda$tick$0
  [ 6] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 7] ChannelsGrid.lambda$forEachChannel$0
  [ 8] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] TickPerCell.tick
  [12] GameOfLife.calculateFrame
  [13] GameOfLife.lambda$calculateFrameBlocking$4
  [14] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 3413361 total (0.04%), 3412 samples
  [ 0] ArrayList.elementAt
  [ 1] ArrayList.forEach
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 3411171 total (0.04%), 3411 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] [unknown_Java]

--- 3400636 total (0.04%), 3407 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] AbstractQueuedSynchronizer$ConditionNode.block
  [ 4] ForkJoinPool.unmanagedBlock
  [ 5] ForkJoinPool.managedBlock
  [ 6] AbstractQueuedSynchronizer$ConditionObject.await
  [ 7] LockedSingleValue.take
  [ 8] Channel.take
  [ 9] GameOfLife$$Lambda$55.0x0000000801036950.test
  [10] ChannelsGrid.lambda$forEachChannel$1
  [11] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [12] Dimensions.forEachRowCol
  [13] ChannelsGrid.forEachChannel
  [14] GameOfLife.calculateFrame
  [15] GameOfLife.lambda$calculateFrameBlocking$4
  [16] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 3360158 total (0.04%), 3368 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 3262854 total (0.03%), 3259 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 3259319 total (0.03%), 3246 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_change_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] AbstractQueuedSynchronizer.signalNext
  [15] AbstractQueuedSynchronizer.release
  [16] ReentrantLock.unlock
  [17] LockedSingleValue.put
  [18] Channel.put
  [19] TickPerCell.lambda$tick$0
  [20] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [21] ChannelsGrid.lambda$forEachChannel$0
  [22] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [23] Dimensions.forEachRowCol
  [24] ChannelsGrid.forEachChannel
  [25] TickPerCell.tick
  [26] GameOfLife.calculateFrame
  [27] GameOfLife.lambda$calculateFrameBlocking$4
  [28] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [29] ThreadPoolExecutor.runWorker
  [30] ThreadPoolExecutor$Worker.run
  [31] Thread.run

--- 3217538 total (0.03%), 3225 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 3207792 total (0.03%), 3207 samples
  [ 0] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 1] [unknown_Java]

--- 3069877 total (0.03%), 3007 samples
  [ 0] __schedule_[k]
  [ 1] schedule_[k]
  [ 2] futex_wait_queue_[k]
  [ 3] futex_wait_[k]
  [ 4] do_futex_[k]
  [ 5] __x64_sys_futex_[k]
  [ 6] do_syscall_64_[k]
  [ 7] entry_SYSCALL_64_after_hwframe_[k]
  [ 8] __futex_abstimed_wait_common
  [ 9] Unsafe.park
  [10] LockSupport.park
  [11] AbstractQueuedSynchronizer$ConditionNode.block
  [12] ForkJoinPool.unmanagedBlock
  [13] ForkJoinPool.managedBlock
  [14] AbstractQueuedSynchronizer$ConditionObject.await
  [15] LockedSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 3041355 total (0.03%), 2968 samples
  [ 0] update_load_avg_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] AbstractQueuedSynchronizer$ConditionNode.block
  [15] ForkJoinPool.unmanagedBlock
  [16] ForkJoinPool.managedBlock
  [17] AbstractQueuedSynchronizer$ConditionObject.await
  [18] LockedSingleValue.take
  [19] Channel.take
  [20] TickPerCell.waitTick
  [21] Cell.notifyLiveness
  [22] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [23] Iterable.forEach
  [24] CellsGroup.run
  [25] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 3024453 total (0.03%), 3032 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 3022373 total (0.03%), 3019 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 3006722 total (0.03%), 2998 samples
  [ 0] Invokers$Holder.linkToTargetMethod
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 3005788 total (0.03%), 3010 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2986850 total (0.03%), 2921 samples
  [ 0] update_curr_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] AbstractQueuedSynchronizer$ConditionNode.block
  [15] ForkJoinPool.unmanagedBlock
  [16] ForkJoinPool.managedBlock
  [17] AbstractQueuedSynchronizer$ConditionObject.await
  [18] LockedSingleValue.take
  [19] Channel.take
  [20] TickPerCell.waitTick
  [21] Cell.notifyLiveness
  [22] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [23] Iterable.forEach
  [24] CellsGroup.run
  [25] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 2982734 total (0.03%), 2969 samples
  [ 0] ArrayList.spliterator
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 2958340 total (0.03%), 2965 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 4] ChannelsGrid.lambda$forEachChannel$1
  [ 5] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] GameOfLife.calculateFrame
  [ 9] GameOfLife.lambda$calculateFrameBlocking$4
  [10] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 2942312 total (0.03%), 2940 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 2934979 total (0.03%), 2942 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 4] ChannelsGrid.lambda$forEachChannel$1
  [ 5] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] GameOfLife.calculateFrame
  [ 9] GameOfLife.lambda$calculateFrameBlocking$4
  [10] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 2917374 total (0.03%), 2920 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] [unknown_Java]

--- 2905129 total (0.03%), 2837 samples
  [ 0] __update_load_avg_cfs_rq_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] AbstractQueuedSynchronizer$ConditionNode.block
  [16] ForkJoinPool.unmanagedBlock
  [17] ForkJoinPool.managedBlock
  [18] AbstractQueuedSynchronizer$ConditionObject.await
  [19] LockedSingleValue.take
  [20] Channel.take
  [21] TickPerCell.waitTick
  [22] Cell.notifyLiveness
  [23] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [24] Iterable.forEach
  [25] CellsGroup.run
  [26] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 2869381 total (0.03%), 2872 samples
  [ 0] ReentrantLock.lock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 4] ChannelsGrid.lambda$forEachChannel$1
  [ 5] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] GameOfLife.calculateFrame
  [ 9] GameOfLife.lambda$calculateFrameBlocking$4
  [10] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 2791739 total (0.03%), 2791 samples
  [ 0] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 1] Dimensions.forEachRowCol
  [ 2] ChannelsGrid.forEachChannel
  [ 3] TickPerCell.tick
  [ 4] GameOfLife.calculateFrame
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 2782045 total (0.03%), 2755 samples
  [ 0] Parker::park(bool, long)
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] AbstractQueuedSynchronizer$ConditionNode.block
  [ 5] ForkJoinPool.unmanagedBlock
  [ 6] ForkJoinPool.managedBlock
  [ 7] AbstractQueuedSynchronizer$ConditionObject.await
  [ 8] LockedSingleValue.take
  [ 9] Channel.take
  [10] GameOfLife.calculateFrameBlocking
  [11] GameOfLifeBenchmark.benchmark
  [12] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
  [13] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_Throughput
  [14] DirectMethodHandle$Holder.invokeSpecial
  [15] LambdaForm$MH.0x000000080102e000.invoke
  [16] LambdaForm$MH.0x000000080102e400.invokeExact_MT
  [17] DirectMethodHandleAccessor.invokeImpl
  [18] DirectMethodHandleAccessor.invoke
  [19] Method.invoke
  [20] BenchmarkHandler$BenchmarkTask.call
  [21] BenchmarkHandler$BenchmarkTask.call
  [22] FutureTask.run
  [23] Executors$RunnableAdapter.call
  [24] FutureTask.run
  [25] ThreadPoolExecutor.runWorker
  [26] ThreadPoolExecutor$Worker.run
  [27] Thread.run

--- 2776060 total (0.03%), 2782 samples
  [ 0] Sink$ChainedReference.end
  [ 1] Sink$ChainedReference.end
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 2775483 total (0.03%), 2766 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 2746836 total (0.03%), 2746 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2746718 total (0.03%), 2683 samples
  [ 0] update_cfs_group_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] AbstractQueuedSynchronizer$ConditionNode.block
  [15] ForkJoinPool.unmanagedBlock
  [16] ForkJoinPool.managedBlock
  [17] AbstractQueuedSynchronizer$ConditionObject.await
  [18] LockedSingleValue.take
  [19] Channel.take
  [20] TickPerCell.waitTick
  [21] Cell.notifyLiveness
  [22] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [23] Iterable.forEach
  [24] CellsGroup.run
  [25] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 2741561 total (0.03%), 2742 samples
  [ 0] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 1] ChannelsGrid.lambda$forEachChannel$1
  [ 2] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 3] Dimensions.forEachRowCol
  [ 4] ChannelsGrid.forEachChannel
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2732161 total (0.03%), 2727 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] AbstractQueuedSynchronizer$ConditionObject.enableWait
  [ 2] AbstractQueuedSynchronizer$ConditionObject.await
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$55.0x0000000801036950.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 2707882 total (0.03%), 2708 samples
  [ 0] G1ParScanThreadState::trim_queue_to_threshold(unsigned int)
  [ 1] G1ParScanThreadState::steal_and_trim_queue(GenericTaskQueueSet<OverflowTaskQueue<ScannerTask, (MEMFLAGS)5, 131072u>, (MEMFLAGS)5>*)
  [ 2] G1ParEvacuateFollowersClosure::do_void()
  [ 3] G1EvacuateRegionsTask::evacuate_live_objects(G1ParScanThreadState*, unsigned int)
  [ 4] G1EvacuateRegionsBaseTask::work(unsigned int)
  [ 5] WorkerThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 2647018 total (0.03%), 2649 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] TickPerCell.lambda$tick$0
  [ 3] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 4] ChannelsGrid.lambda$forEachChannel$0
  [ 5] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] TickPerCell.tick
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2624170 total (0.03%), 2628 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] TickPerCell.lambda$tick$0
  [ 3] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 4] ChannelsGrid.lambda$forEachChannel$0
  [ 5] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] TickPerCell.tick
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2618828 total (0.03%), 2616 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 2598561 total (0.03%), 2599 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2577233 total (0.03%), 2584 samples
  [ 0] AbstractOwnableSynchronizer.setExclusiveOwnerThread
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 2516522 total (0.03%), 2518 samples
  [ 0] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 1] Dimensions.forEachRowCol
  [ 2] ChannelsGrid.forEachChannel
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 2474138 total (0.03%), 2475 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 2473026 total (0.03%), 2477 samples
  [ 0] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
  [ 1] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_Throughput
  [ 2] DirectMethodHandle$Holder.invokeSpecial
  [ 3] LambdaForm$MH.0x000000080102e000.invoke
  [ 4] LambdaForm$MH.0x000000080102e400.invokeExact_MT
  [ 5] DirectMethodHandleAccessor.invokeImpl
  [ 6] DirectMethodHandleAccessor.invoke
  [ 7] Method.invoke
  [ 8] BenchmarkHandler$BenchmarkTask.call
  [ 9] BenchmarkHandler$BenchmarkTask.call
  [10] FutureTask.run
  [11] Executors$RunnableAdapter.call
  [12] FutureTask.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 2452457 total (0.03%), 2436 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] AbstractQueuedSynchronizer$ConditionNode.block
  [ 4] ForkJoinPool.unmanagedBlock
  [ 5] ForkJoinPool.managedBlock
  [ 6] AbstractQueuedSynchronizer$ConditionObject.await
  [ 7] LockedSingleValue.take
  [ 8] Channel.take
  [ 9] GameOfLife.calculateFrameBlocking
  [10] GameOfLifeBenchmark.benchmark
  [11] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
  [12] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_Throughput
  [13] DirectMethodHandle$Holder.invokeSpecial
  [14] LambdaForm$MH.0x000000080102e000.invoke
  [15] LambdaForm$MH.0x000000080102e400.invokeExact_MT
  [16] DirectMethodHandleAccessor.invokeImpl
  [17] DirectMethodHandleAccessor.invoke
  [18] Method.invoke
  [19] BenchmarkHandler$BenchmarkTask.call
  [20] BenchmarkHandler$BenchmarkTask.call
  [21] FutureTask.run
  [22] Executors$RunnableAdapter.call
  [23] FutureTask.run
  [24] ThreadPoolExecutor.runWorker
  [25] ThreadPoolExecutor$Worker.run
  [26] Thread.run

--- 2440371 total (0.03%), 2442 samples
  [ 0] Channel.take
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 2433903 total (0.03%), 2437 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 2432923 total (0.03%), 2440 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 2416570 total (0.03%), 2410 samples
  [ 0] PipelineHelper.<init>
  [ 1] AbstractPipeline.<init>
  [ 2] ReferencePipeline.<init>
  [ 3] ReferencePipeline$StatelessOp.<init>
  [ 4] ReferencePipeline$3.<init>
  [ 5] ReferencePipeline.map
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 2416159 total (0.03%), 2406 samples
  [ 0] enqueue_entity_[k]
  [ 1] enqueue_task_fair_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] AbstractQueuedSynchronizer.signalNext
  [15] AbstractQueuedSynchronizer.release
  [16] ReentrantLock.unlock
  [17] LockedSingleValue.put
  [18] Channel.put
  [19] TickPerCell.lambda$tick$0
  [20] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [21] ChannelsGrid.lambda$forEachChannel$0
  [22] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [23] Dimensions.forEachRowCol
  [24] ChannelsGrid.forEachChannel
  [25] TickPerCell.tick
  [26] GameOfLife.calculateFrame
  [27] GameOfLife.lambda$calculateFrameBlocking$4
  [28] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [29] ThreadPoolExecutor.runWorker
  [30] ThreadPoolExecutor$Worker.run
  [31] Thread.run

--- 2415961 total (0.03%), 2415 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 2396546 total (0.02%), 2339 samples
  [ 0] __update_load_avg_se_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] AbstractQueuedSynchronizer$ConditionNode.block
  [16] ForkJoinPool.unmanagedBlock
  [17] ForkJoinPool.managedBlock
  [18] AbstractQueuedSynchronizer$ConditionObject.await
  [19] LockedSingleValue.take
  [20] Channel.take
  [21] TickPerCell.waitTick
  [22] Cell.notifyLiveness
  [23] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [24] Iterable.forEach
  [25] CellsGroup.run
  [26] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 2372638 total (0.02%), 2375 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2371545 total (0.02%), 2374 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2366268 total (0.02%), 2369 samples
  [ 0] AbstractPipeline.copyInto
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 2365876 total (0.02%), 2354 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] AbstractQueuedSynchronizer$ConditionNode.block
  [ 4] ForkJoinPool.unmanagedBlock
  [ 5] ForkJoinPool.managedBlock
  [ 6] AbstractQueuedSynchronizer$ConditionObject.await
  [ 7] LockedSingleValue.take
  [ 8] Channel.take
  [ 9] TickPerCell.waitTick
  [10] Cell.notifyLiveness
  [11] CellsGroup$$Lambda$48.0x0000000801035690.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 2358295 total (0.02%), 2350 samples
  [ 0] update_load_avg_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] AbstractQueuedSynchronizer.signalNext
  [16] AbstractQueuedSynchronizer.release
  [17] ReentrantLock.unlock
  [18] LockedSingleValue.put
  [19] Channel.put
  [20] TickPerCell.lambda$tick$0
  [21] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [22] ChannelsGrid.lambda$forEachChannel$0
  [23] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [24] Dimensions.forEachRowCol
  [25] ChannelsGrid.forEachChannel
  [26] TickPerCell.tick
  [27] GameOfLife.calculateFrame
  [28] GameOfLife.lambda$calculateFrameBlocking$4
  [29] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [30] ThreadPoolExecutor.runWorker
  [31] ThreadPoolExecutor$Worker.run
  [32] Thread.run

--- 2354209 total (0.02%), 2362 samples
  [ 0] AbstractQueuedSynchronizer.getState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$58.0x0000000801036fc0.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$57.0x0000000801036da8.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 2338672 total (0.02%), 2341 samples
  [ 0] Integer.intValue
  [ 1] ChannelsGrid.lambda$forEachChannel$1
  [ 2] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 3] Dimensions.forEachRowCol
  [ 4] ChannelsGrid.forEachChannel
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2298941 total (0.02%), 2300 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] TickPerCell.lambda$tick$0
  [ 3] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 4] ChannelsGrid.lambda$forEachChannel$0
  [ 5] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] TickPerCell.tick
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2279886 total (0.02%), 2280 samples
  [ 0] ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
  [ 1] [unknown_Java]

       total  percent  samples  top
  ----------  -------  -------  ---
  2299778727   23.97%  2304577  AbstractQueuedSynchronizer.compareAndSetState
  1515549802   15.80%  1515878  Cell.lambda$notifyLiveness$0
  1241230889   12.94%  1238962  ReduceOps$5ReducingSink.get
   912036409    9.51%   913008  AbstractQueuedSynchronizer.release
   414788445    4.32%   415328  LockedSingleValue.take
   406488795    4.24%   406726  Cell$$Lambda$58.0x0000000801036fc0.apply
   300258832    3.13%   300342  Channel.take
   235758931    2.46%   235176  AbstractPipeline.<init>
   158182018    1.65%   158285  TickPerCell.lambda$tick$0
    99319434    1.04%    99057  StreamSupport.stream
    95333002    0.99%    95298  Cell.calculateNextState
    94966633    0.99%    95147  LockedSingleValue.put
    90165227    0.94%    90229  Sink$ChainedReference.<init>
    80853775    0.84%    80918  ArrayList$ArrayListSpliterator.forEachRemaining
    79667440    0.83%    79822  AbstractQueuedSynchronizer$ConditionObject.signal
    72143357    0.75%    72055  ReferencePipeline$StatelessOp.<init>
    69953572    0.73%    70105  ReentrantLock$Sync.lock
    64761835    0.68%    64738  CellsGroup$$Lambda$48.0x0000000801035690.accept
    64560141    0.67%    64637  ChannelsGrid.getChannel
    56340868    0.59%    56392  ReentrantLock$NonfairSync.initialTryLock
    52942394    0.55%    52927  vtable stub
    52648338    0.55%    52618  Dimensions.forEachRowCol
    50266163    0.52%    50237  ReferencePipeline$4.opWrapSink
    46258045    0.48%    46357  AbstractQueuedSynchronizer.setState
    43645576    0.45%    43693  ReferencePipeline$3$1.accept
    43583742    0.45%    43595  ReferencePipeline$3.opWrapSink
    36738493    0.38%    36711  AbstractQueuedSynchronizer.signalNext
    36520252    0.38%    36573  AbstractOwnableSynchronizer.getExclusiveOwnerThread
    36497120    0.38%    36476  Cell.notifyLiveness
    34104655    0.36%    34197  ReentrantLock$Sync.tryRelease
    30417146    0.32%    30484  ReentrantLock.lock
    30025434    0.31%    30061  Sink$ChainedReference.begin
    26379598    0.27%    26298  StreamOpFlag.fromCharacteristics
    24457019    0.25%    24485  Channel.put
    22505921    0.23%    22495  ArrayList$SubList$1.next
    22444824    0.23%    22412  ArrayList.forEach
    21140316    0.22%    21184  AbstractQueuedSynchronizer.getState
    20920248    0.22%    20974  ReentrantLock$Sync.isHeldExclusively
    19522800    0.20%    19113  psi_group_change_[k]
    19029928    0.20%    19059  ReferencePipeline$4$1.accept
    18866983    0.20%    18687  ThreadPoolExecutor.runWorker
    18692722    0.19%    18708  Cell$$Lambda$53.0x0000000801036510.accept
    18118362    0.19%    18099  Iterable.forEach
    17499866    0.18%    17532  ReduceOps$5ReducingSink.accept
    16490576    0.17%    16504  ChannelsGrid$$Lambda$56.0x0000000801036b88.accept
    16221202    0.17%    16218  ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
    13682455    0.14%    13718  ReentrantLock.unlock
    12626981    0.13%    12377  __update_load_avg_cfs_rq_[k]
    11882601    0.12%    11658  update_load_avg_[k]
    11048610    0.12%    11037  ArrayList$ArrayListSpliterator.characteristics
    10371190    0.11%    10363  ReferencePipeline.<init>
     9209513    0.10%     9224  Cell$$Lambda$59.0x00000008010371e8.applyAsInt
     8973412    0.09%     8801  __update_load_avg_se_[k]
     8932987    0.09%     8911  Unsafe_Park
     8497721    0.09%     8346  update_cfs_group_[k]
     8388893    0.09%     8223  update_curr_[k]
     8104842    0.08%     8101  AbstractPipeline.wrapSink
     7624174    0.08%     7634  AbstractOwnableSynchronizer.setExclusiveOwnerThread
     7176855    0.07%     7176  ArrayList$SubList$1.checkForComodification
     5844734    0.06%     5696  update_blocked_averages_[k]
     5777168    0.06%     5771  CellsGroup$$Lambda$57.0x0000000801036da8.accept
     5680490    0.06%     5695  Sink$ChainedReference.end
     5627502    0.06%     5609  DirectMethodHandle.allocateInstance
     5565054    0.06%     5463  __entry_text_start_[k]
     5521063    0.06%     5483  Parker::park(bool, long)
     5402637    0.06%     5320  reweight_entity_[k]
     4654008    0.05%     4649  Object.<init>
     4400326    0.05%     4322  syscall_exit_to_user_mode_[k]
     4380394    0.05%     4299  __schedule_[k]
     4074855    0.04%     4067  PipelineHelper.<init>
     3902129    0.04%     3856  I2C/C2I adapters
     3892551    0.04%     3868  __memset_avx2_unaligned_erms
     3888990    0.04%     3894  ___pthread_cond_wait
     3710580    0.04%     3705  AbstractQueuedSynchronizer$ConditionObject.doSignal
     3641212    0.04%     3632  Thread.interrupted
     3601539    0.04%     3576  enqueue_entity_[k]
     3591857    0.04%     3593  G1ParScanThreadState::trim_queue_to_threshold(unsigned int)
     3517673    0.04%     3518  Unsafe.park
     3474374    0.04%     3466  Invokers$Holder.linkToTargetMethod
     3413361    0.04%     3412  ArrayList.elementAt
     3357026    0.03%     3365  GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
     3294469    0.03%     3234  dequeue_task_fair_[k]
     3288718    0.03%     3291  Integer.intValue
     3217837    0.03%     3205  AbstractQueuedSynchronizer$ConditionObject.enableWait
     3094170    0.03%     3030  __calc_delta_[k]
     2982734    0.03%     2969  ArrayList.spliterator
     2964849    0.03%     2970  AbstractQueuedSynchronizer.acquire
     2741561    0.03%     2742  GameOfLife$$Lambda$55.0x0000000801036950.test
     2657948    0.03%     2602  check_preemption_disabled_[k]
     2623846    0.03%     2606  enqueue_task_fair_[k]
     2608691    0.03%     2615  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
     2526806    0.03%     2500  CellsGroup.run
     2499619    0.03%     2452  futex_q_lock_[k]
     2401314    0.03%     2344  update_min_vruntime_[k]
     2367655    0.02%     2372  AbstractQueuedSynchronizer.enqueue
     2366268    0.02%     2369  AbstractPipeline.copyInto
     2358569    0.02%     2317  _raw_spin_lock_[k]
     2325351    0.02%     2282  dequeue_entity_[k]
     2308975    0.02%     2311  ReferencePipeline.map
     2248184    0.02%     2250  ChannelsGrid.lambda$forEachChannel$1
     2111161    0.02%     2064  iterate_groups_[k]
     2072262    0.02%     2080  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<548964ul, G1BarrierSet>, (AccessInternal::BarrierType)2, 548964ul>::oop_access_barrier(void*)
     2068365    0.02%     2032  update_irq_load_avg_[k]
     2060755    0.02%     2068  Unsafe.getAndBitwiseAndInt
     2034585    0.02%     1969  rb_next_[k]
     2031592    0.02%     2027  __GI___pthread_mutex_lock
     1999049    0.02%     1964  update_rq_clock_[k]
     1962219    0.02%     1924  cpuacct_charge_[k]
     1947740    0.02%     1941  ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
     1945976    0.02%     1881  __get_user_8_[k]
     1874502    0.02%     1865  G1CardSet::occupied() const
     1848961    0.02%     1796  rcu_sched_clock_irq_[k]
     1731633    0.02%     1735  pthread_mutex_trylock@@GLIBC_2.34
     1722173    0.02%     1660  restore_fpregs_from_fpstate_[k]
     1721581    0.02%     1703  select_task_rq_fair_[k]
     1719308    0.02%     1719  StreamOpFlag.combineOpFlags
     1717354    0.02%     1693  futex_wake_mark_[k]
     1680540    0.02%     1648  futex_wake_[k]
     1638776    0.02%     1603  task_tick_fair_[k]
     1616286    0.02%     1621  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)3, 286822ul>::oop_access_barrier(oopDesc*, long)
     1607277    0.02%     1568  timerqueue_add_[k]
     1604199    0.02%     1555  __hrtimer_run_queues_[k]
     1583467    0.02%     1581  AbstractQueuedSynchronizer$ConditionObject.await
     1566688    0.02%     1566  MemAllocator::allocate() const
     1537516    0.02%     1483  syscall_return_via_sysret_[k]
     1524970    0.02%     1488  psi_task_switch_[k]
     1517182    0.02%     1506  psi_task_change_[k]
     1501700    0.02%     1503  TickPerCell.waitTick
     1496223    0.02%     1474  native_sched_clock_[k]
     1452262    0.02%     1456  TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
     1425939    0.01%     1415  __clock_gettime
     1412107    0.01%     1383  newidle_balance_[k]
     1369784    0.01%     1375  AbstractQueuedSynchronizer$ConditionNode.isReleasable
     1360003    0.01%     1315  fpregs_restore_userregs_[k]
     1356782    0.01%     1362  Unsafe_Unpark
     1329572    0.01%     1333  AbstractQueuedSynchronizer$ConditionObject.canReacquire
     1280047    0.01%     1272  __condvar_dec_grefs
     1275774    0.01%     1267  ObjArrayKlass::allocate(int, JavaThread*)
     1272633    0.01%     1272  JavaThread::threadObj() const
     1262501    0.01%     1263  ArrayList$SubList$1.hasNext
     1225104    0.01%     1227  __GI___pthread_getspecific
     1134540    0.01%     1119  try_to_wake_up_[k]
     1130614    0.01%     1135  void OopOopIterateBackwardsDispatch<G1ScanEvacuatedObjClosure>::Table::oop_oop_iterate_backwards<InstanceKlass, narrowOop>(G1ScanEvacuatedObjClosure*, oopDesc*, Klass*)
     1124199    0.01%     1097  preempt_count_sub_[k]
     1115758    0.01%     1092  update_sd_lb_stats.constprop.0_[k]
     1108230    0.01%     1112  ___pthread_cond_signal
     1097685    0.01%     1100  __tls_get_addr
     1094541    0.01%     1094  IntPipeline.<init>
     1090107    0.01%     1087  ThreadsListHandle::cv_internal_thread_to_JavaThread(_jobject*, JavaThread**, oopDesc**)
     1074888    0.01%     1078  native_write_msr_[k]
     1061813    0.01%     1037  asm_sysvec_apic_timer_interrupt_[k]
     1042619    0.01%     1040  java_lang_Thread::get_thread_status(oopDesc*)
     1038674    0.01%     1037  JavaFrameAnchor::make_walkable()
     1030618    0.01%     1031  __pthread_mutex_unlock_usercnt
     1029595    0.01%     1032  ThreadsListHandle::ThreadsListHandle(Thread*)
     1029294    0.01%     1012  preempt_count_add_[k]
     1029252    0.01%     1038  _pthread_cleanup_pop@@GLIBC_2.34
     1025456    0.01%     1005  G1Allocator::unsafe_max_tlab_alloc()
     1016639    0.01%      997  __perf_event_task_sched_out_[k]
     1010676    0.01%     1008  OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
     1009001    0.01%      984  native_read_msr_[k]
      964028    0.01%      972  java_lang_Thread::set_thread_status(oopDesc*, JavaThreadStatus)
      950666    0.01%      925  update_process_times_[k]
      938859    0.01%      940  IntPipeline.sum
      938184    0.01%      912  hrtimer_interrupt_[k]
      929233    0.01%      923  __futex_abstimed_wait_common
      889234    0.01%      878  __vdso_clock_gettime
      886023    0.01%      874  G1Policy::preventive_collection_required(unsigned int)
      881084    0.01%      860  __accumulate_pelt_segments_[k]
      870968    0.01%      864  G1Analytics::predict_scan_card_num(unsigned long, bool) const
      866115    0.01%      843  timekeeping_advance_[k]
      846593    0.01%      825  _raw_spin_lock_irqsave_[k]
      843528    0.01%      827  __cgroup_account_cputime_[k]
      831012    0.01%      826  ThreadPoolExecutor.getTask
      828495    0.01%      809  futex_wait_[k]
      823235    0.01%      824  ObjArrayAllocator::initialize(HeapWordImpl**) const
      822171    0.01%      821  ReferencePipeline$3.<init>
      815784    0.01%      814  MemAllocator::Allocation::notify_allocation_jvmti_sampler()
      813384    0.01%      814  Klass::check_array_allocation_length(int, int, JavaThread*)
      811554    0.01%      772  exit_to_user_mode_prepare_[k]
      805110    0.01%      787  hrtimer_active_[k]
      800699    0.01%      794  ClassLoaderData::holder() const
      794814    0.01%      797  LockSupport.unpark
      786541    0.01%      763  __cgroup_account_cputime_field_[k]
      766313    0.01%      756  G1CollectedHeap::allocate_new_tlab(unsigned long, unsigned long, unsigned long*)
      762700    0.01%      756  HeapRegionManager::allocate_free_region(HeapRegionType, unsigned int)
      762313    0.01%      752  __GI___pthread_disable_asynccancel
      753912    0.01%      746  MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
      732709    0.01%      730  ThreadPoolExecutor.runStateAtLeast
      707900    0.01%      694  rebalance_domains_[k]
      660052    0.01%      651  psi_flags_change_[k]
      659795    0.01%      646  __softirqentry_text_start_[k]
      653529    0.01%      643  G1CollectedHeap::fill_with_dummy_object(HeapWordImpl**, HeapWordImpl**, bool)
      629742    0.01%      619  __get_user_nocheck_4_[k]
      624092    0.01%      621  G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
      616885    0.01%      606  put_prev_task_fair_[k]
      616027    0.01%      617  G1FromCardCache::clear(unsigned int)
      612381    0.01%      611  __pthread_mutex_cond_lock
      606840    0.01%      591  scheduler_tick_[k]
      605444    0.01%      591  read_tsc_[k]
