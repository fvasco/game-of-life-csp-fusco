--- Execution profile ---
Total samples       : 3516567
unknown_Java        : 28186 (0.80%)
not_walkable_Java   : 181 (0.01%)
skipped             : 5 (0.00%)

--- 1480227459 total (42.09%), 1479239 samples
  [ 0] ReduceOps$5ReducingSink.get
  [ 1] ReduceOps$5ReducingSink.get
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 225094927 total (6.40%), 225942 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$65.0x0000000801033228.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 105765475 total (3.01%), 105948 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 100032198 total (2.84%), 99820 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 90494483 total (2.57%), 90679 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 87247161 total (2.48%), 87270 samples
  [ 0] vtable stub
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 83019205 total (2.36%), 83273 samples
  [ 0] Cell$$Lambda$65.0x0000000801033228.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 79848247 total (2.27%), 79862 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 73927270 total (2.10%), 74021 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 62400689 total (1.77%), 62575 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 59319066 total (1.69%), 59412 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 58877664 total (1.67%), 58801 samples
  [ 0] Objects.requireNonNull
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 55646053 total (1.58%), 55658 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 53762523 total (1.53%), 53528 samples
  [ 0] ArrayList$ArrayListSpliterator.<init>
  [ 1] ArrayList.spliterator
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 53107153 total (1.51%), 52938 samples
  [ 0] Objects.requireNonNull
  [ 1] ReferencePipeline.map
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 39364994 total (1.12%), 39499 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$65.0x0000000801033228.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 37359839 total (1.06%), 37275 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 37098906 total (1.05%), 37193 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$52.0x0000000801036518.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 28350990 total (0.81%), 28323 samples
  [ 0] Collection.stream
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 26857181 total (0.76%), 27000 samples
  [ 0] OneToOneParkingSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$52.0x0000000801036518.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 24618821 total (0.70%), 24675 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 23610072 total (0.67%), 23654 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 18445164 total (0.52%), 18394 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 16875872 total (0.48%), 16928 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 15412915 total (0.44%), 15417 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 14202590 total (0.40%), 14215 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 13535222 total (0.38%), 13544 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 12720668 total (0.36%), 12727 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 11697187 total (0.33%), 11692 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 10953030 total (0.31%), 10938 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 10483963 total (0.30%), 10497 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 10287243 total (0.29%), 10323 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$65.0x0000000801033228.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 10167128 total (0.29%), 10164 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 9551818 total (0.27%), 9571 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 9285328 total (0.26%), 9318 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$65.0x0000000801033228.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 8684631 total (0.25%), 8713 samples
  [ 0] Cell$$Lambda$65.0x0000000801033228.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 8339558 total (0.24%), 8305 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 8324504 total (0.24%), 8319 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 7880556 total (0.22%), 7898 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 7653971 total (0.22%), 7669 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 7603067 total (0.22%), 7606 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] [unknown_Java]

--- 7197476 total (0.20%), 7184 samples
  [ 0] IntPipeline$StatelessOp.<init>
  [ 1] ReferencePipeline$4.<init>
  [ 2] ReferencePipeline.mapToInt
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 6980511 total (0.20%), 6987 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] [unknown_Java]

--- 6963008 total (0.20%), 6975 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 6560134 total (0.19%), 6583 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 6556762 total (0.19%), 6583 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 6436668 total (0.18%), 6464 samples
  [ 0] Cell$$Lambda$69.0x0000000801032800.applyAsInt
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 6417827 total (0.18%), 6445 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 6227274 total (0.18%), 6253 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 6178830 total (0.18%), 6189 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] [unknown_Java]

--- 6173367 total (0.18%), 6156 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 6135477 total (0.17%), 6160 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 6100088 total (0.17%), 6083 samples
  [ 0] PipelineHelper.<init>
  [ 1] AbstractPipeline.<init>
  [ 2] ReferencePipeline.<init>
  [ 3] ReferencePipeline$Head.<init>
  [ 4] StreamSupport.stream
  [ 5] Collection.stream
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 6053449 total (0.17%), 6079 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 5722085 total (0.16%), 5745 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 5691305 total (0.16%), 5688 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 5466113 total (0.16%), 5451 samples
  [ 0] ReferencePipeline.map
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 5327574 total (0.15%), 5351 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] GameOfLife.calculateFrame
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 5162457 total (0.15%), 5179 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$65.0x0000000801033228.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 5104179 total (0.15%), 5105 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 4731609 total (0.13%), 4738 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 4568064 total (0.13%), 4548 samples
  [ 0] ReferencePipeline.map
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 4479963 total (0.13%), 4470 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 4368354 total (0.12%), 4381 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$65.0x0000000801033228.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 4206343 total (0.12%), 4215 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 4159345 total (0.12%), 4164 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 4139330 total (0.12%), 4148 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] [unknown_Java]

--- 4118412 total (0.12%), 4115 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 4065533 total (0.12%), 4088 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 4019919 total (0.11%), 4039 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 4013749 total (0.11%), 4048 samples
  [ 0] OneToOneParkingSingleValue.put
  [ 1] Channel.put
  [ 2] TickPerCell.lambda$tick$0
  [ 3] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [ 4] ChannelsGrid.lambda$forEachChannel$0
  [ 5] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] TickPerCell.tick
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3889660 total (0.11%), 3898 samples
  [ 0] AbstractPipeline.copyInto
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 3789178 total (0.11%), 3786 samples
  [ 0] ReferencePipeline.mapToInt
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 3764190 total (0.11%), 3772 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 3715367 total (0.11%), 3721 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3708881 total (0.11%), 3717 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3573092 total (0.10%), 3602 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$57.0x0000000801036d98.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$62.0x0000000801037830.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 3373555 total (0.10%), 3367 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] G1RemSetScanState::G1ClearCardTableTask::do_work(unsigned int)
  [ 2] G1BatchedTask::work(unsigned int)
  [ 3] WorkerThread::run()
  [ 4] Thread::call_run()
  [ 5] thread_native_entry(Thread*)
  [ 6] start_thread

--- 3317340 total (0.09%), 3325 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3296307 total (0.09%), 3301 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3276787 total (0.09%), 3284 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3249345 total (0.09%), 3257 samples
  [ 0] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 3234140 total (0.09%), 3237 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 3067148 total (0.09%), 3069 samples
  [ 0] ReferencePipeline$StatelessOp.<init>
  [ 1] ReferencePipeline$3.<init>
  [ 2] ReferencePipeline.map
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2794688 total (0.08%), 2790 samples
  [ 0] MemAllocator::allocate() const
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] GameOfLife.calculateFrame
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 2746396 total (0.08%), 2750 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2723367 total (0.08%), 2726 samples
  [ 0] Objects.requireNonNull
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2635981 total (0.07%), 2634 samples
  [ 0] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 2] GameOfLife.calculateFrame
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 2389094 total (0.07%), 2395 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2329958 total (0.07%), 2295 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] ReduceOps$5ReducingSink.get
  [ 6] ReduceOps$5ReducingSink.get
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 2248590 total (0.06%), 2249 samples
  [ 0] Sink$ChainedReference.end
  [ 1] Sink$ChainedReference.end
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 2192453 total (0.06%), 2197 samples
  [ 0] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 1] [unknown_Java]

--- 1918988 total (0.05%), 1923 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 1879314 total (0.05%), 1879 samples
  [ 0] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 2] GameOfLife.calculateFrame
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 1762396 total (0.05%), 1760 samples
  [ 0] Objects.requireNonNull
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 1699735 total (0.05%), 1683 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] OneToOneParkingSingleValue.take
  [14] Channel.take
  [15] TickPerCell.waitTick
  [16] Cell.notifyLiveness
  [17] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [18] Iterable.forEach
  [19] CellsGroup.run
  [20] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [21] ThreadPoolExecutor.runWorker
  [22] ThreadPoolExecutor$Worker.run
  [23] Thread.run

--- 1665640 total (0.05%), 1668 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 1645200 total (0.05%), 1650 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 1595288 total (0.05%), 1598 samples
  [ 0] ReferencePipeline.mapToInt
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 1585799 total (0.05%), 1590 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 1568120 total (0.04%), 1573 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 1555882 total (0.04%), 1561 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 1554276 total (0.04%), 1557 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 1548460 total (0.04%), 1551 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 1541284 total (0.04%), 1546 samples
  [ 0] ArrayList$SubList$1.checkForComodification
  [ 1] ArrayList$SubList$1.next
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 1531901 total (0.04%), 1536 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 1512944 total (0.04%), 1498 samples
  [ 0] update_curr_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 1480655 total (0.04%), 1476 samples
  [ 0] G1CardSet::occupied() const
  [ 1] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 2] G1RemSetSamplingTask::execute()
  [ 3] G1ServiceThread::run_task(G1ServiceTask*)
  [ 4] G1ServiceThread::run_service()
  [ 5] ConcurrentGCThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 1436931 total (0.04%), 1439 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 1403221 total (0.04%), 1400 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_change_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] OneToOneParkingSingleValue.put
  [15] Channel.put
  [16] TickPerCell.lambda$tick$0
  [17] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [18] ChannelsGrid.lambda$forEachChannel$0
  [19] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [20] Dimensions.forEachRowCol
  [21] ChannelsGrid.forEachChannel
  [22] TickPerCell.tick
  [23] GameOfLife.calculateFrame
  [24] GameOfLife.lambda$calculateFrameBlocking$4
  [25] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 1325537 total (0.04%), 1318 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] OneToOneParkingSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 1309221 total (0.04%), 1303 samples
  [ 0] enqueue_entity_[k]
  [ 1] enqueue_task_fair_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] OneToOneParkingSingleValue.put
  [15] Channel.put
  [16] TickPerCell.lambda$tick$0
  [17] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [18] ChannelsGrid.lambda$forEachChannel$0
  [19] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [20] Dimensions.forEachRowCol
  [21] ChannelsGrid.forEachChannel
  [22] TickPerCell.tick
  [23] GameOfLife.calculateFrame
  [24] GameOfLife.lambda$calculateFrameBlocking$4
  [25] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 1278497 total (0.04%), 1264 samples
  [ 0] __schedule_[k]
  [ 1] schedule_[k]
  [ 2] futex_wait_queue_[k]
  [ 3] futex_wait_[k]
  [ 4] do_futex_[k]
  [ 5] __x64_sys_futex_[k]
  [ 6] do_syscall_64_[k]
  [ 7] entry_SYSCALL_64_after_hwframe_[k]
  [ 8] __futex_abstimed_wait_common
  [ 9] Unsafe.park
  [10] LockSupport.park
  [11] OneToOneParkingSingleValue.take
  [12] Channel.take
  [13] TickPerCell.waitTick
  [14] Cell.notifyLiveness
  [15] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 1252380 total (0.04%), 1241 samples
  [ 0] update_cfs_group_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 1223574 total (0.03%), 1221 samples
  [ 0] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 1148059 total (0.03%), 1150 samples
  [ 0] Object.<init>
  [ 1] PipelineHelper.<init>
  [ 2] AbstractPipeline.<init>
  [ 3] ReferencePipeline.<init>
  [ 4] ReferencePipeline$Head.<init>
  [ 5] StreamSupport.stream
  [ 6] Collection.stream
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 1123912 total (0.03%), 1122 samples
  [ 0] ObjArrayAllocator::initialize(HeapWordImpl**) const
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 1120127 total (0.03%), 1111 samples
  [ 0] update_load_avg_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 1117408 total (0.03%), 1117 samples
  [ 0] MemAllocator::Allocation::notify_allocation_jvmti_sampler()
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 1109126 total (0.03%), 1109 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 1069607 total (0.03%), 1060 samples
  [ 0] dequeue_task_fair_[k]
  [ 1] __schedule_[k]
  [ 2] schedule_[k]
  [ 3] futex_wait_queue_[k]
  [ 4] futex_wait_[k]
  [ 5] do_futex_[k]
  [ 6] __x64_sys_futex_[k]
  [ 7] do_syscall_64_[k]
  [ 8] entry_SYSCALL_64_after_hwframe_[k]
  [ 9] __futex_abstimed_wait_common
  [10] Unsafe.park
  [11] LockSupport.park
  [12] OneToOneParkingSingleValue.take
  [13] Channel.take
  [14] TickPerCell.waitTick
  [15] Cell.notifyLiveness
  [16] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 1067138 total (0.03%), 1061 samples
  [ 0] enqueue_task_fair_[k]
  [ 1] enqueue_task_[k]
  [ 2] ttwu_do_activate_[k]
  [ 3] try_to_wake_up_[k]
  [ 4] wake_up_q_[k]
  [ 5] futex_wake_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] ___pthread_cond_signal
  [11] Unsafe.unpark
  [12] LockSupport.unpark
  [13] OneToOneParkingSingleValue.put
  [14] Channel.put
  [15] TickPerCell.lambda$tick$0
  [16] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [17] ChannelsGrid.lambda$forEachChannel$0
  [18] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [19] Dimensions.forEachRowCol
  [20] ChannelsGrid.forEachChannel
  [21] TickPerCell.tick
  [22] GameOfLife.calculateFrame
  [23] GameOfLife.lambda$calculateFrameBlocking$4
  [24] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [25] ThreadPoolExecutor.runWorker
  [26] ThreadPoolExecutor$Worker.run
  [27] Thread.run

--- 1005000 total (0.03%), 1006 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 960606 total (0.03%), 940 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] __futex_abstimed_wait_common
  [ 4] Unsafe.park
  [ 5] LockSupport.park
  [ 6] OneToOneParkingSingleValue.take
  [ 7] Channel.take
  [ 8] TickPerCell.waitTick
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 945408 total (0.03%), 947 samples
  [ 0] FreeListAllocator::reset()
  [ 1] HeapRegionRemSet::clear_locked(bool)
  [ 2] HeapRegion::hr_clear(bool)
  [ 3] G1CollectedHeap::free_region(HeapRegion*, FreeRegionList*)
  [ 4] FreeCSetClosure::do_heap_region(HeapRegion*)
  [ 5] G1CollectedHeap::par_iterate_regions_array(HeapRegionClosure*, HeapRegionClaimer*, unsigned int const*, unsigned long, unsigned int) const
  [ 6] G1PostEvacuateCollectionSetCleanupTask2::FreeCollectionSetTask::do_work(unsigned int)
  [ 7] G1BatchedTask::work(unsigned int)
  [ 8] WorkerThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 913733 total (0.03%), 906 samples
  [ 0] Parker::park(bool, long)
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] OneToOneParkingSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 891149 total (0.03%), 894 samples
  [ 0] OneToOneParkingSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$52.0x0000000801036518.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 834499 total (0.02%), 836 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 824469 total (0.02%), 811 samples
  [ 0] update_blocked_averages_[k]
  [ 1] newidle_balance_[k]
  [ 2] pick_next_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 817950 total (0.02%), 785 samples
  [ 0] syscall_return_via_sysret_[k]
  [ 1] __futex_abstimed_wait_common
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] OneToOneParkingSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 812958 total (0.02%), 813 samples
  [ 0] Klass::check_array_allocation_length(int, int, JavaThread*)
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] GameOfLife.calculateFrame
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 808989 total (0.02%), 809 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 776572 total (0.02%), 778 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$57.0x0000000801036d98.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$62.0x0000000801037830.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 766479 total (0.02%), 759 samples
  [ 0] Unsafe.park
  [ 1] LockSupport.park
  [ 2] OneToOneParkingSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 741992 total (0.02%), 738 samples
  [ 0] __update_load_avg_cfs_rq_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] OneToOneParkingSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 724572 total (0.02%), 722 samples
  [ 0] update_load_avg_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] OneToOneParkingSingleValue.put
  [16] Channel.put
  [17] TickPerCell.lambda$tick$0
  [18] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [19] ChannelsGrid.lambda$forEachChannel$0
  [20] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [21] Dimensions.forEachRowCol
  [22] ChannelsGrid.forEachChannel
  [23] TickPerCell.tick
  [24] GameOfLife.calculateFrame
  [25] GameOfLife.lambda$calculateFrameBlocking$4
  [26] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 661816 total (0.02%), 652 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] ReferencePipeline$3.opWrapSink
  [ 6] AbstractPipeline.wrapSink
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 635896 total (0.02%), 630 samples
  [ 0] __update_load_avg_se_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] OneToOneParkingSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 632709 total (0.02%), 627 samples
  [ 0] dequeue_entity_[k]
  [ 1] dequeue_task_fair_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] OneToOneParkingSingleValue.take
  [14] Channel.take
  [15] TickPerCell.waitTick
  [16] Cell.notifyLiveness
  [17] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [18] Iterable.forEach
  [19] CellsGroup.run
  [20] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [21] ThreadPoolExecutor.runWorker
  [22] ThreadPoolExecutor$Worker.run
  [23] Thread.run

--- 624989 total (0.02%), 623 samples
  [ 0] MemAllocator::Allocation::check_out_of_memory()
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 624533 total (0.02%), 627 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)3, 286822ul>::oop_access_barrier(oopDesc*, long)
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] OneToOneParkingSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 613122 total (0.02%), 613 samples
  [ 0] reweight_entity_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] OneToOneParkingSingleValue.put
  [16] Channel.put
  [17] TickPerCell.lambda$tick$0
  [18] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [19] ChannelsGrid.lambda$forEachChannel$0
  [20] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [21] Dimensions.forEachRowCol
  [22] ChannelsGrid.forEachChannel
  [23] TickPerCell.tick
  [24] GameOfLife.calculateFrame
  [25] GameOfLife.lambda$calculateFrameBlocking$4
  [26] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 611800 total (0.02%), 607 samples
  [ 0] update_sd_lb_stats.constprop.0_[k]
  [ 1] find_busiest_group_[k]
  [ 2] load_balance_[k]
  [ 3] newidle_balance_[k]
  [ 4] pick_next_task_fair_[k]
  [ 5] __schedule_[k]
  [ 6] schedule_[k]
  [ 7] futex_wait_queue_[k]
  [ 8] futex_wait_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] __futex_abstimed_wait_common
  [14] Unsafe.park
  [15] LockSupport.park
  [16] OneToOneParkingSingleValue.take
  [17] Channel.take
  [18] TickPerCell.waitTick
  [19] Cell.notifyLiveness
  [20] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [21] Iterable.forEach
  [22] CellsGroup.run
  [23] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [24] ThreadPoolExecutor.runWorker
  [25] ThreadPoolExecutor$Worker.run
  [26] Thread.run

--- 602650 total (0.02%), 588 samples
  [ 0] __condvar_dec_grefs
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] OneToOneParkingSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 590943 total (0.02%), 586 samples
  [ 0] reweight_entity_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 589400 total (0.02%), 585 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] OneToOneParkingSingleValue.take
  [14] Channel.take
  [15] GameOfLife$$Lambda$57.0x0000000801036d98.test
  [16] ChannelsGrid.lambda$forEachChannel$1
  [17] ChannelsGrid$$Lambda$62.0x0000000801037830.accept
  [18] Dimensions.forEachRowCol
  [19] ChannelsGrid.forEachChannel
  [20] GameOfLife.calculateFrame
  [21] GameOfLife.lambda$calculateFrameBlocking$4
  [22] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 586302 total (0.02%), 590 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$52.0x0000000801036518.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 564452 total (0.02%), 558 samples
  [ 0] ___pthread_cond_wait
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] OneToOneParkingSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 563143 total (0.02%), 559 samples
  [ 0] resched_curr_[k]
  [ 1] check_preempt_curr_[k]
  [ 2] ttwu_do_wakeup_[k]
  [ 3] try_to_wake_up_[k]
  [ 4] wake_up_q_[k]
  [ 5] futex_wake_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] ___pthread_cond_signal
  [11] Unsafe.unpark
  [12] LockSupport.unpark
  [13] OneToOneParkingSingleValue.put
  [14] Channel.put
  [15] TickPerCell.lambda$tick$0
  [16] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [17] ChannelsGrid.lambda$forEachChannel$0
  [18] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [19] Dimensions.forEachRowCol
  [20] ChannelsGrid.forEachChannel
  [21] TickPerCell.tick
  [22] GameOfLife.calculateFrame
  [23] GameOfLife.lambda$calculateFrameBlocking$4
  [24] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [25] ThreadPoolExecutor.runWorker
  [26] ThreadPoolExecutor$Worker.run
  [27] Thread.run

--- 562281 total (0.02%), 557 samples
  [ 0] cpuacct_charge_[k]
  [ 1] update_curr_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] OneToOneParkingSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 542234 total (0.02%), 536 samples
  [ 0] futex_q_lock_[k]
  [ 1] futex_wait_setup_[k]
  [ 2] futex_wait_[k]
  [ 3] do_futex_[k]
  [ 4] __x64_sys_futex_[k]
  [ 5] do_syscall_64_[k]
  [ 6] entry_SYSCALL_64_after_hwframe_[k]
  [ 7] __futex_abstimed_wait_common
  [ 8] Unsafe.park
  [ 9] LockSupport.park
  [10] OneToOneParkingSingleValue.take
  [11] Channel.take
  [12] TickPerCell.waitTick
  [13] Cell.notifyLiveness
  [14] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 527514 total (0.02%), 529 samples
  [ 0] Sink$ChainedReference.end
  [ 1] Sink$ChainedReference.end
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 508873 total (0.01%), 507 samples
  [ 0] ThreadPoolExecutor$Worker.tryAcquire
  [ 1] AbstractQueuedSynchronizer.acquire
  [ 2] ThreadPoolExecutor$Worker.lock
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 484762 total (0.01%), 485 samples
  [ 0] __update_load_avg_se_[k]
  [ 1] update_load_avg_[k]
  [ 2] enqueue_entity_[k]
  [ 3] enqueue_task_fair_[k]
  [ 4] enqueue_task_[k]
  [ 5] ttwu_do_activate_[k]
  [ 6] try_to_wake_up_[k]
  [ 7] wake_up_q_[k]
  [ 8] futex_wake_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] ___pthread_cond_signal
  [14] Unsafe.unpark
  [15] LockSupport.unpark
  [16] OneToOneParkingSingleValue.put
  [17] Channel.put
  [18] TickPerCell.lambda$tick$0
  [19] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [20] ChannelsGrid.lambda$forEachChannel$0
  [21] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [22] Dimensions.forEachRowCol
  [23] ChannelsGrid.forEachChannel
  [24] TickPerCell.tick
  [25] GameOfLife.calculateFrame
  [26] GameOfLife.lambda$calculateFrameBlocking$4
  [27] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [28] ThreadPoolExecutor.runWorker
  [29] ThreadPoolExecutor$Worker.run
  [30] Thread.run

--- 484609 total (0.01%), 481 samples
  [ 0] G1CardSet::clear()
  [ 1] HeapRegionRemSet::clear_locked(bool)
  [ 2] HeapRegion::hr_clear(bool)
  [ 3] G1CollectedHeap::free_region(HeapRegion*, FreeRegionList*)
  [ 4] FreeCSetClosure::do_heap_region(HeapRegion*)
  [ 5] G1CollectedHeap::par_iterate_regions_array(HeapRegionClosure*, HeapRegionClaimer*, unsigned int const*, unsigned long, unsigned int) const
  [ 6] G1PostEvacuateCollectionSetCleanupTask2::FreeCollectionSetTask::do_work(unsigned int)
  [ 7] G1BatchedTask::work(unsigned int)
  [ 8] WorkerThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 482765 total (0.01%), 479 samples
  [ 0] __calc_delta_[k]
  [ 1] update_curr_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] OneToOneParkingSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 474753 total (0.01%), 476 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$65.0x0000000801033228.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 462343 total (0.01%), 462 samples
  [ 0] __tls_get_addr
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] GameOfLife.calculateFrame
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 461241 total (0.01%), 463 samples
  [ 0] Sink$ChainedReference.end
  [ 1] [unknown_Java]

--- 458817 total (0.01%), 456 samples
  [ 0] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 1] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 2] G1RemSetSamplingTask::execute()
  [ 3] G1ServiceThread::run_task(G1ServiceTask*)
  [ 4] G1ServiceThread::run_service()
  [ 5] ConcurrentGCThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 457963 total (0.01%), 460 samples
  [ 0] G1Analytics::predict_scan_card_num(unsigned long, bool) const
  [ 1] G1Policy::predict_region_non_copy_time_ms(HeapRegion*, bool) const
  [ 2] G1CollectionSet::update_young_region_prediction(HeapRegion*, unsigned long)
  [ 3] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 4] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 5] G1RemSetSamplingTask::execute()
  [ 6] G1ServiceThread::run_task(G1ServiceTask*)
  [ 7] G1ServiceThread::run_service()
  [ 8] ConcurrentGCThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 454908 total (0.01%), 455 samples
  [ 0] update_cfs_group_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] OneToOneParkingSingleValue.put
  [16] Channel.put
  [17] TickPerCell.lambda$tick$0
  [18] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [19] ChannelsGrid.lambda$forEachChannel$0
  [20] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [21] Dimensions.forEachRowCol
  [22] ChannelsGrid.forEachChannel
  [23] TickPerCell.tick
  [24] GameOfLife.calculateFrame
  [25] GameOfLife.lambda$calculateFrameBlocking$4
  [26] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 453222 total (0.01%), 454 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 451903 total (0.01%), 454 samples
  [ 0] Sink$ChainedReference.end
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 435866 total (0.01%), 432 samples
  [ 0] G1Policy::preventive_collection_required(unsigned int)
  [ 1] G1CollectedHeap::attempt_allocation_slow(unsigned long)
  [ 2] G1CollectedHeap::allocate_new_tlab(unsigned long, unsigned long, unsigned long*)
  [ 3] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 4] MemAllocator::allocate() const
  [ 5] InstanceKlass::allocate_instance(JavaThread*)
  [ 6] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 7] ReduceOps$5ReducingSink.get
  [ 8] ReduceOps$5ReducingSink.get
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 431608 total (0.01%), 428 samples
  [ 0] newidle_balance_[k]
  [ 1] pick_next_task_fair_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] OneToOneParkingSingleValue.take
  [14] Channel.take
  [15] TickPerCell.waitTick
  [16] Cell.notifyLiveness
  [17] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [18] Iterable.forEach
  [19] CellsGroup.run
  [20] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [21] ThreadPoolExecutor.runWorker
  [22] ThreadPoolExecutor$Worker.run
  [23] Thread.run

--- 431070 total (0.01%), 433 samples
  [ 0] ___pthread_cond_wait
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] OneToOneParkingSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 429129 total (0.01%), 428 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] ___pthread_cond_signal
  [ 4] Unsafe.unpark
  [ 5] LockSupport.unpark
  [ 6] OneToOneParkingSingleValue.put
  [ 7] Channel.put
  [ 8] TickPerCell.lambda$tick$0
  [ 9] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [10] ChannelsGrid.lambda$forEachChannel$0
  [11] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [12] Dimensions.forEachRowCol
  [13] ChannelsGrid.forEachChannel
  [14] TickPerCell.tick
  [15] GameOfLife.calculateFrame
  [16] GameOfLife.lambda$calculateFrameBlocking$4
  [17] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 426315 total (0.01%), 428 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 425240 total (0.01%), 413 samples
  [ 0] __futex_abstimed_wait_common
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] OneToOneParkingSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 423551 total (0.01%), 423 samples
  [ 0] __update_load_avg_cfs_rq_[k]
  [ 1] update_load_avg_[k]
  [ 2] enqueue_entity_[k]
  [ 3] enqueue_task_fair_[k]
  [ 4] enqueue_task_[k]
  [ 5] ttwu_do_activate_[k]
  [ 6] try_to_wake_up_[k]
  [ 7] wake_up_q_[k]
  [ 8] futex_wake_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] ___pthread_cond_signal
  [14] Unsafe.unpark
  [15] LockSupport.unpark
  [16] OneToOneParkingSingleValue.put
  [17] Channel.put
  [18] TickPerCell.lambda$tick$0
  [19] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [20] ChannelsGrid.lambda$forEachChannel$0
  [21] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [22] Dimensions.forEachRowCol
  [23] ChannelsGrid.forEachChannel
  [24] TickPerCell.tick
  [25] GameOfLife.calculateFrame
  [26] GameOfLife.lambda$calculateFrameBlocking$4
  [27] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [28] ThreadPoolExecutor.runWorker
  [29] ThreadPoolExecutor$Worker.run
  [30] Thread.run

--- 420427 total (0.01%), 419 samples
  [ 0] GameOfLife.endOfFrame
  [ 1] GameOfLife.calculateFrame
  [ 2] GameOfLife.lambda$calculateFrameBlocking$4
  [ 3] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 418110 total (0.01%), 419 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 415391 total (0.01%), 409 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] AbstractPipeline.<init>
  [ 6] ReferencePipeline.<init>
  [ 7] ReferencePipeline$Head.<init>
  [ 8] StreamSupport.stream
  [ 9] Collection.stream
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 403259 total (0.01%), 403 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 397479 total (0.01%), 381 samples
  [ 0] fpregs_restore_userregs_[k]
  [ 1] exit_to_user_mode_prepare_[k]
  [ 2] syscall_exit_to_user_mode_[k]
  [ 3] do_syscall_64_[k]
  [ 4] entry_SYSCALL_64_after_hwframe_[k]
  [ 5] __futex_abstimed_wait_common
  [ 6] Unsafe.park
  [ 7] LockSupport.park
  [ 8] OneToOneParkingSingleValue.take
  [ 9] Channel.take
  [10] TickPerCell.waitTick
  [11] Cell.notifyLiveness
  [12] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 395976 total (0.01%), 385 samples
  [ 0] futex_wait_[k]
  [ 1] do_futex_[k]
  [ 2] __x64_sys_futex_[k]
  [ 3] do_syscall_64_[k]
  [ 4] entry_SYSCALL_64_after_hwframe_[k]
  [ 5] __futex_abstimed_wait_common
  [ 6] Unsafe.park
  [ 7] LockSupport.park
  [ 8] OneToOneParkingSingleValue.take
  [ 9] Channel.take
  [10] TickPerCell.waitTick
  [11] Cell.notifyLiveness
  [12] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 393741 total (0.01%), 371 samples
  [ 0] __get_user_8_[k]
  [ 1] __rseq_handle_notify_resume_[k]
  [ 2] exit_to_user_mode_prepare_[k]
  [ 3] syscall_exit_to_user_mode_[k]
  [ 4] do_syscall_64_[k]
  [ 5] entry_SYSCALL_64_after_hwframe_[k]
  [ 6] __futex_abstimed_wait_common
  [ 7] Unsafe.park
  [ 8] LockSupport.park
  [ 9] OneToOneParkingSingleValue.take
  [10] Channel.take
  [11] TickPerCell.waitTick
  [12] Cell.notifyLiveness
  [13] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 393153 total (0.01%), 388 samples
  [ 0] G1CollectedHeap::attempt_allocation_slow(unsigned long)
  [ 1] G1CollectedHeap::allocate_new_tlab(unsigned long, unsigned long, unsigned long*)
  [ 2] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 3] MemAllocator::allocate() const
  [ 4] InstanceKlass::allocate_instance(JavaThread*)
  [ 5] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 6] ReduceOps$5ReducingSink.get
  [ 7] ReduceOps$5ReducingSink.get
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 388621 total (0.01%), 388 samples
  [ 0] __GI___pthread_mutex_lock
  [ 1] Unsafe_Unpark
  [ 2] Unsafe.unpark
  [ 3] LockSupport.unpark
  [ 4] OneToOneParkingSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 382917 total (0.01%), 385 samples
  [ 0] G1RemSetScanState::G1ClearCardTableTask::do_work(unsigned int)
  [ 1] G1BatchedTask::work(unsigned int)
  [ 2] WorkerThread::run()
  [ 3] Thread::call_run()
  [ 4] thread_native_entry(Thread*)
  [ 5] start_thread

--- 375554 total (0.01%), 370 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] ReferencePipeline$4.opWrapSink
  [ 6] AbstractPipeline.wrapSink
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 372036 total (0.01%), 370 samples
  [ 0] G1SegmentedArray::num_segments() const
  [ 1] HeapRegionRemSet::card_set_memory_stats() const
  [ 2] G1PrepareEvacuationTask::G1PrepareRegionsClosure::do_heap_region(HeapRegion*)
  [ 3] HeapRegionManager::par_iterate(HeapRegionClosure*, HeapRegionClaimer*, unsigned int) const
  [ 4] G1PrepareEvacuationTask::work(unsigned int)
  [ 5] WorkerThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 370670 total (0.01%), 371 samples
  [ 0] Object.<init>
  [ 1] PipelineHelper.<init>
  [ 2] AbstractPipeline.<init>
  [ 3] ReferencePipeline.<init>
  [ 4] ReferencePipeline$StatelessOp.<init>
  [ 5] ReferencePipeline$3.<init>
  [ 6] ReferencePipeline.map
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 370309 total (0.01%), 356 samples
  [ 0] restore_fpregs_from_fpstate_[k]
  [ 1] fpregs_restore_userregs_[k]
  [ 2] exit_to_user_mode_prepare_[k]
  [ 3] syscall_exit_to_user_mode_[k]
  [ 4] do_syscall_64_[k]
  [ 5] entry_SYSCALL_64_after_hwframe_[k]
  [ 6] __futex_abstimed_wait_common
  [ 7] Unsafe.park
  [ 8] LockSupport.park
  [ 9] OneToOneParkingSingleValue.take
  [10] Channel.take
  [11] TickPerCell.waitTick
  [12] Cell.notifyLiveness
  [13] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 365912 total (0.01%), 368 samples
  [ 0] AbsSeq::dsd() const
  [ 1] G1Policy::predict_region_non_copy_time_ms(HeapRegion*, bool) const
  [ 2] G1CollectionSet::update_young_region_prediction(HeapRegion*, unsigned long)
  [ 3] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 4] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 5] G1RemSetSamplingTask::execute()
  [ 6] G1ServiceThread::run_task(G1ServiceTask*)
  [ 7] G1ServiceThread::run_service()
  [ 8] ConcurrentGCThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 362614 total (0.01%), 364 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 359677 total (0.01%), 353 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] __lll_lock_wake
  [ 4] Unsafe_Park
  [ 5] Unsafe.park
  [ 6] LockSupport.park
  [ 7] OneToOneParkingSingleValue.take
  [ 8] Channel.take
  [ 9] TickPerCell.waitTick
  [10] Cell.notifyLiveness
  [11] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 359217 total (0.01%), 355 samples
  [ 0] update_rq_clock_[k]
  [ 1] __schedule_[k]
  [ 2] schedule_[k]
  [ 3] futex_wait_queue_[k]
  [ 4] futex_wait_[k]
  [ 5] do_futex_[k]
  [ 6] __x64_sys_futex_[k]
  [ 7] do_syscall_64_[k]
  [ 8] entry_SYSCALL_64_after_hwframe_[k]
  [ 9] __futex_abstimed_wait_common
  [10] Unsafe.park
  [11] LockSupport.park
  [12] OneToOneParkingSingleValue.take
  [13] Channel.take
  [14] TickPerCell.waitTick
  [15] Cell.notifyLiveness
  [16] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 347984 total (0.01%), 348 samples
  [ 0] psi_task_change_[k]
  [ 1] enqueue_task_[k]
  [ 2] ttwu_do_activate_[k]
  [ 3] try_to_wake_up_[k]
  [ 4] wake_up_q_[k]
  [ 5] futex_wake_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] ___pthread_cond_signal
  [11] Unsafe.unpark
  [12] LockSupport.unpark
  [13] OneToOneParkingSingleValue.put
  [14] Channel.put
  [15] TickPerCell.lambda$tick$0
  [16] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [17] ChannelsGrid.lambda$forEachChannel$0
  [18] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [19] Dimensions.forEachRowCol
  [20] ChannelsGrid.forEachChannel
  [21] TickPerCell.tick
  [22] GameOfLife.calculateFrame
  [23] GameOfLife.lambda$calculateFrameBlocking$4
  [24] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [25] ThreadPoolExecutor.runWorker
  [26] ThreadPoolExecutor$Worker.run
  [27] Thread.run

--- 346472 total (0.01%), 351 samples
  [ 0] LinkedBlockingQueue.offer
  [ 1] ThreadPoolExecutor.execute
  [ 2] GameOfLife$$Lambda$43.0x0000000801034c28.accept
  [ 3] GameOfLife.calculateFrameBlocking
  [ 4] GameOfLifeBenchmark.benchmark
  [ 5] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
  [ 6] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_Throughput
  [ 7] DirectMethodHandle$Holder.invokeSpecial
  [ 8] LambdaForm$MH.0x000000080102e000.invoke
  [ 9] LambdaForm$MH.0x000000080102e400.invokeExact_MT
  [10] DirectMethodHandleAccessor.invokeImpl
  [11] DirectMethodHandleAccessor.invoke
  [12] Method.invoke
  [13] BenchmarkHandler$BenchmarkTask.call
  [14] BenchmarkHandler$BenchmarkTask.call
  [15] FutureTask.run
  [16] Executors$RunnableAdapter.call
  [17] FutureTask.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 335751 total (0.01%), 332 samples
  [ 0] __cgroup_account_cputime_[k]
  [ 1] update_curr_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] OneToOneParkingSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 334384 total (0.01%), 324 samples
  [ 0] __entry_text_start_[k]
  [ 1] __futex_abstimed_wait_common
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] OneToOneParkingSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 333815 total (0.01%), 334 samples
  [ 0] MemAllocator::Allocation::notify_allocation_jfr_sampler()
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] GameOfLife.calculateFrame
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 332862 total (0.01%), 328 samples
  [ 0] __entry_text_start_[k]
  [ 1] __lll_lock_wake
  [ 2] Unsafe_Park
  [ 3] Unsafe.park
  [ 4] LockSupport.park
  [ 5] OneToOneParkingSingleValue.take
  [ 6] Channel.take
  [ 7] TickPerCell.waitTick
  [ 8] Cell.notifyLiveness
  [ 9] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 326619 total (0.01%), 328 samples
  [ 0] __GI___pthread_enable_asynccancel
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] OneToOneParkingSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 326499 total (0.01%), 329 samples
  [ 0] G1Analytics::predict_card_merge_time_ms(unsigned long, bool) const
  [ 1] G1Policy::predict_region_non_copy_time_ms(HeapRegion*, bool) const
  [ 2] G1CollectionSet::update_young_region_prediction(HeapRegion*, unsigned long)
  [ 3] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 4] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 5] G1RemSetSamplingTask::execute()
  [ 6] G1ServiceThread::run_task(G1ServiceTask*)
  [ 7] G1ServiceThread::run_service()
  [ 8] ConcurrentGCThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 326422 total (0.01%), 323 samples
  [ 0] HeapRegionManager::allocate_free_region(HeapRegionType, unsigned int)
  [ 1] G1CollectedHeap::new_region(unsigned long, HeapRegionType, bool, unsigned int)
  [ 2] G1CollectedHeap::new_mutator_alloc_region(unsigned long, bool, unsigned int)
  [ 3] G1AllocRegion::new_alloc_region_and_allocate(unsigned long, bool)
  [ 4] G1CollectedHeap::attempt_allocation_slow(unsigned long)
  [ 5] G1CollectedHeap::allocate_new_tlab(unsigned long, unsigned long, unsigned long*)
  [ 6] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 7] MemAllocator::allocate() const
  [ 8] InstanceKlass::allocate_instance(JavaThread*)
  [ 9] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [10] ReduceOps$5ReducingSink.get
  [11] ReduceOps$5ReducingSink.get
  [12] ReduceOps$ReduceOp.evaluateSequential
  [13] AbstractPipeline.evaluate
  [14] IntPipeline.reduce
  [15] IntPipeline.sum
  [16] Cell.calculateNextState
  [17] CellsGroup$$Lambda$59.0x0000000801036fd0.accept
  [18] Iterable.forEach
  [19] CellsGroup.run
  [20] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [21] ThreadPoolExecutor.runWorker
  [22] ThreadPoolExecutor$Worker.run
  [23] Thread.run

--- 326205 total (0.01%), 326 samples
  [ 0] try_to_wake_up_[k]
  [ 1] wake_up_q_[k]
  [ 2] futex_wake_[k]
  [ 3] do_futex_[k]
  [ 4] __x64_sys_futex_[k]
  [ 5] do_syscall_64_[k]
  [ 6] entry_SYSCALL_64_after_hwframe_[k]
  [ 7] ___pthread_cond_signal
  [ 8] Unsafe.unpark
  [ 9] LockSupport.unpark
  [10] OneToOneParkingSingleValue.put
  [11] Channel.put
  [12] TickPerCell.lambda$tick$0
  [13] TickPerCell$$Lambda$50.0x0000000801035ec0.accept
  [14] ChannelsGrid.lambda$forEachChannel$0
  [15] ChannelsGrid$$Lambda$51.0x00000008010360d8.accept
  [16] Dimensions.forEachRowCol
  [17] ChannelsGrid.forEachChannel
  [18] TickPerCell.tick
  [19] GameOfLife.calculateFrame
  [20] GameOfLife.lambda$calculateFrameBlocking$4
  [21] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 321562 total (0.01%), 320 samples
  [ 0] ObjArrayKlass::allocate(int, JavaThread*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$49.0x0000000801035ac0.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

       total  percent  samples  top
  ----------  -------  -------  ---
  1480240469   42.09%  1479252  ReduceOps$5ReducingSink.get
   299650792    8.52%   300772  OneToOneParkingSingleValue.take
   266056900    7.57%   265992  StreamSupport.stream
   242898145    6.91%   243352  Sink$ChainedReference.<init>
   116471578    3.31%   116226  Objects.requireNonNull
   111558333    3.17%   111454  AbstractPipeline.<init>
   105145881    2.99%   105176  ReferencePipeline$4.opWrapSink
    95110133    2.70%    95229  ReferencePipeline$3.opWrapSink
    91703836    2.61%    91986  Cell$$Lambda$65.0x0000000801033228.apply
    87248155    2.48%    87271  vtable stub
    70650184    2.01%    70814  ArrayList$ArrayListSpliterator.forEachRemaining
    53762523    1.53%    53528  ArrayList$ArrayListSpliterator.<init>
    47955493    1.36%    48044  Sink$ChainedReference.begin
    37685208    1.07%    37783  Cell.lambda$notifyLiveness$0
    32186846    0.92%    32366  OneToOneParkingSingleValue.put
    30020023    0.85%    30107  ReferencePipeline$3$1.accept
    28350990    0.81%    28323  Collection.stream
    19748696    0.56%    19825  ReferencePipeline$4$1.accept
    17564130    0.50%    17527  Cell.calculateNextState
    14898737    0.42%    14902  AbstractPipeline.wrapSink
    12753457    0.36%    12805  ReduceOps$5ReducingSink.accept
    10034177    0.29%     9999  ReferencePipeline.map
     9488030    0.27%     9533  Dimensions.forEachRowCol
     8535683    0.24%     8545  Iterable.forEach
     8147906    0.23%     8080  __memset_avx2_unaligned_erms
     7197476    0.20%     7184  IntPipeline$StatelessOp.<init>
     6436668    0.18%     6464  Cell$$Lambda$69.0x0000000801032800.applyAsInt
     6109984    0.17%     6120  ArrayList$SubList$1.next
     6100088    0.17%     6083  PipelineHelper.<init>
     5441798    0.15%     5454  CellsGroup$$Lambda$59.0x0000000801036fd0.accept
     5384466    0.15%     5384  ReferencePipeline.mapToInt
     5085126    0.14%     5055  psi_group_change_[k]
     5070845    0.14%     5086  Channel.take
     4786595    0.14%     4776  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
     3889660    0.11%     3898  AbstractPipeline.copyInto
     3705211    0.11%     3711  Sink$ChainedReference.end
     3229404    0.09%     3225  MemAllocator::allocate() const
     3102888    0.09%     3100  TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
     3067148    0.09%     3069  ReferencePipeline$StatelessOp.<init>
     3040315    0.09%     3022  update_load_avg_[k]
     3018251    0.09%     3023  StreamOpFlag.fromCharacteristics
     2939762    0.08%     2916  __update_load_avg_cfs_rq_[k]
     2820991    0.08%     2780  update_blocked_averages_[k]
     2817739    0.08%     2781  syscall_exit_to_user_mode_[k]
     2692585    0.08%     2691  ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
     2653641    0.08%     2635  update_curr_[k]
     2555248    0.07%     2539  update_cfs_group_[k]
     2353772    0.07%     2320  __entry_text_start_[k]
     2295065    0.07%     2287  Unsafe_Park
     2011252    0.06%     2010  ArrayList$SubList$1.checkForComodification
     1943501    0.06%     1931  __update_load_avg_se_[k]
     1908292    0.05%     1898  enqueue_entity_[k]
     1844271    0.05%     1826  __schedule_[k]
     1781767    0.05%     1775  reweight_entity_[k]
     1754887    0.05%     1748  G1CardSet::occupied() const
     1719648    0.05%     1723  Object.<init>
     1563755    0.04%     1557  ___pthread_cond_wait
     1554900    0.04%     1547  enqueue_task_fair_[k]
     1522290    0.04%     1521  ObjArrayAllocator::initialize(HeapWordImpl**) const
     1497853    0.04%     1485  Parker::park(bool, long)
     1485756    0.04%     1435  syscall_return_via_sysret_[k]
     1471230    0.04%     1460  dequeue_task_fair_[k]
     1446932    0.04%     1445  MemAllocator::Allocation::notify_allocation_jvmti_sampler()
     1393757    0.04%     1388  Unsafe.park
     1297373    0.04%     1296  __tls_get_addr
     1135792    0.03%     1144  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)3, 286822ul>::oop_access_barrier(oopDesc*, long)
     1074076    0.03%     1074  Klass::check_array_allocation_length(int, int, JavaThread*)
     1066294    0.03%     1059  _raw_spin_lock_[k]
     1028313    0.03%     1020  update_rq_clock_[k]
     1023449    0.03%      996  __condvar_dec_grefs
     1009680    0.03%      998  check_preemption_disabled_[k]
      979754    0.03%      979  __GI___pthread_mutex_lock
      956493    0.03%      950  dequeue_entity_[k]
      945408    0.03%      947  FreeListAllocator::reset()
      928048    0.03%      930  pthread_mutex_trylock@@GLIBC_2.34
      884811    0.03%      876  futex_q_lock_[k]
      883493    0.03%      876  __calc_delta_[k]
      871539    0.02%      873  select_task_rq_fair_[k]
      841890    0.02%      840  MemAllocator::Allocation::check_out_of_memory()
      839994    0.02%      834  cpuacct_charge_[k]
      808638    0.02%      803  __pthread_mutex_unlock_usercnt
      805906    0.02%      748  __get_user_8_[k]
      777268    0.02%      748  restore_fpregs_from_fpstate_[k]
      763218    0.02%      758  native_sched_clock_[k]
      759795    0.02%      744  futex_wake_[k]
      751449    0.02%      748  try_to_wake_up_[k]
      747916    0.02%      698  fpregs_restore_userregs_[k]
      741905    0.02%      736  update_sd_lb_stats.constprop.0_[k]
      732132    0.02%      732  Unsafe_Unpark
      708209    0.02%      705  psi_task_change_[k]
      703880    0.02%      698  resched_curr_[k]
      691617    0.02%      694  CellsGroup$$Lambda$46.0x0000000801035478.accept
      686158    0.02%      685  G1Analytics::predict_scan_card_num(unsigned long, bool) const
      684073    0.02%      661  __futex_abstimed_wait_common
      677446    0.02%      685  ThreadsListHandle::ThreadsListHandle(Thread*)
      662873    0.02%      657  newidle_balance_[k]
      648940    0.02%      651  AbsSeq::dsd() const
      628799    0.02%      570  exit_to_user_mode_prepare_[k]
      619971    0.02%      622  ___pthread_cond_signal
      619410    0.02%      637  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<548964ul, G1BarrierSet>, (AccessInternal::BarrierType)2, 548964ul>::oop_access_barrier(void*)
      611177    0.02%      611  futex_wake_mark_[k]
      609779    0.02%      612  Cell.notifyLiveness
      600343    0.02%      585  futex_wait_[k]
      596142    0.02%      591  G1Policy::preventive_collection_required(unsigned int)
      587611    0.02%      544  __rseq_handle_notify_resume_[k]
      561950    0.02%      555  G1CollectedHeap::attempt_allocation_slow(unsigned long)
      539143    0.02%      539  G1FromCardCache::clear(unsigned int)
      538376    0.02%      536  update_irq_load_avg_[k]
      523076    0.01%      528  ThreadsListHandle::cv_internal_thread_to_JavaThread(_jobject*, JavaThread**, oopDesc**)
      512467    0.01%      527  java_lang_Thread::set_thread_status(oopDesc*, JavaThreadStatus)
      508873    0.01%      507  ThreadPoolExecutor$Worker.tryAcquire
      499385    0.01%      493  rcu_sched_clock_irq_[k]
      489652    0.01%      490  G1Analytics::predict_card_merge_time_ms(unsigned long, bool) const
      487526    0.01%      483  HeapRegionManager::allocate_free_region(HeapRegionType, unsigned int)
      484609    0.01%      481  G1CardSet::clear()
      481466    0.01%      477  __cgroup_account_cputime_[k]
      471030    0.01%      462  _raw_spin_lock_irqsave_[k]
      460791    0.01%      458  G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
      458798    0.01%      462  LinkedBlockingQueue.offer
      455759    0.01%      456  MemAllocator::Allocation::notify_allocation_jfr_sampler()
      447101    0.01%      444  __GI___pthread_getspecific
      445868    0.01%      443  update_min_vruntime_[k]
      443513    0.01%      452  native_write_msr_[k]
      440470    0.01%      437  iterate_groups_[k]
      440384    0.01%      436  __perf_event_task_sched_out_[k]
      434429    0.01%      432  ObjAllocator::initialize(HeapWordImpl**) const
      434048    0.01%      429  AbsSeq::davg() const
      428274    0.01%      430  java_lang_Thread::get_thread_status(oopDesc*)
      425980    0.01%      422  psi_task_switch_[k]
      424224    0.01%      423  GameOfLife.endOfFrame
      413620    0.01%      411  G1CollectedHeap::allocate_new_tlab(unsigned long, unsigned long, unsigned long*)
      413244    0.01%      410  preempt_count_add_[k]
      411678    0.01%      414  Channel.put
      408849    0.01%      414  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<544868ul, G1BarrierSet>, (AccessInternal::BarrierType)2, 544868ul>::oop_access_barrier(void*)
      394428    0.01%      396  __vdso_clock_gettime
      391215    0.01%      393  __GI___pthread_enable_asynccancel
      387392    0.01%      383  G1Allocator::unsafe_max_tlab_alloc()
      382917    0.01%      385  G1RemSetScanState::G1ClearCardTableTask::do_work(unsigned int)
      372036    0.01%      370  G1SegmentedArray::num_segments() const
      369954    0.01%      367  JavaThread::threadObj() const
      364927    0.01%      361  rcu_note_context_switch_[k]
      358573    0.01%      355  load_balance_[k]
      354645    0.01%      353  ObjArrayKlass::allocate(int, JavaThread*)
      354370    0.01%      355  available_idle_cpu_[k]
      351350    0.01%      350  check_preempt_curr_[k]
      350907    0.01%      346  native_read_msr_[k]
      349945    0.01%      337  __pthread_mutex_cond_lock
      336836    0.01%      333  preempt_count_sub_[k]
      326801    0.01%      322  __hrtimer_run_queues_[k]
      325084    0.01%      316  ThreadLocalStorage::is_initialized()
      324117    0.01%      321  HSpaceCounters::update_used(unsigned long)
      322905    0.01%      318  asm_sysvec_apic_timer_interrupt_[k]
      321842    0.01%      325  __clock_gettime
      318300    0.01%      320  G1Analytics::predict_card_scan_time_ms(unsigned long, bool) const
      317248    0.01%      316  psi_flags_change_[k]
      316520    0.01%      315  G1RebuildFreeListTask::work(unsigned int)
      312678    0.01%      317  LinkedBlockingQueue.enqueue
      309482    0.01%      309  _find_next_bit_[k]
      308027    0.01%      308  AbstractQueuedSynchronizer.acquire
      304835    0.01%      304  OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
      295024    0.01%      296  AbstractOwnableSynchronizer.setExclusiveOwnerThread
      291302    0.01%      289  JavaFrameAnchor::make_walkable()
      288536    0.01%      286  G1CollectionSet::add_eden_region(HeapRegion*)
      287422    0.01%      288  ObjectSampler::is_created()
      284506    0.01%      278  futex_hash_[k]
      282616    0.01%      283  SharedRuntime::on_slowpath_allocation_exit(JavaThread*)
      280490    0.01%      281  ClassLoaderData::holder() const
      277660    0.01%      264  __x64_sys_futex_[k]
      274945    0.01%      274  G1CollectedHeap::fill_with_dummy_object(HeapWordImpl**, HeapWordImpl**, bool)
      267106    0.01%      268  ThreadPoolExecutor.execute
      266798    0.01%      265  __get_user_nocheck_4_[k]
      263966    0.01%      260  rb_erase_[k]
      263183    0.01%      261  __rcu_read_lock_[k]
      262172    0.01%      262  OptoRuntime::new_instance_C(Klass*, JavaThread*)
      262077    0.01%      262  G1CardSetMemoryManager::memory_stats() const
      261526    0.01%      259  check_spread.isra.0_[k]
      261253    0.01%      260  MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
      254860    0.01%      254  Unsafe.unpark
      254696    0.01%      244  get_futex_key_[k]
      253761    0.01%      250  rb_next_[k]
      252988    0.01%      254  ReferencePipeline.<init>
      250479    0.01%      241  do_futex_[k]
      247968    0.01%      248  G1Policy::predict_region_non_copy_time_ms(HeapRegion*, bool) const
      247745    0.01%      246  put_prev_task_fair_[k]
      244979    0.01%      245  JfrAllocationTracer::JfrAllocationTracer(Klass const*, HeapWordImpl**, unsigned long, bool, JavaThread*)
      243304    0.01%      241  timerqueue_add_[k]
      243138    0.01%      240  schedule_[k]
      241071    0.01%      238  timekeeping_advance_[k]
      239148    0.01%      241  Thread.interrupted
      236519    0.01%      229  GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
      235020    0.01%      234  HeapRegion::set_eden()
      230327    0.01%      226  can_migrate_task_[k]
      227574    0.01%      226  G1MonitoringSupport::update_eden_size()
      226857    0.01%      221  __GI___pthread_mutex_unlock
      226376    0.01%      226  AllocTracer::send_allocation_in_new_tlab(Klass*, HeapWordImpl**, unsigned long, unsigned long, JavaThread*)
      224926    0.01%      225  JfrObjectAllocationSample::send_event(Klass const*, unsigned long, bool, Thread*)
      222963    0.01%      221  __update_idle_core_[k]
      220429    0.01%      187  __irqentry_text_end_[k]
      218007    0.01%      219  G1PrepareEvacuationTask::G1PrepareRegionsClosure::do_heap_region(HeapRegion*)
      216242    0.01%      214  ttwu_do_activate_[k]
