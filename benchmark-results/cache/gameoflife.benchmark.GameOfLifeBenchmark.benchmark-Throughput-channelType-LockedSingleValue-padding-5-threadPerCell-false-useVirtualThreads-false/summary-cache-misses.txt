--- Execution profile ---
Total samples       : 2595450
unknown_Java        : 24038 (0.93%)
not_walkable_Java   : 197 (0.01%)

--- 953515271 total (36.74%), 952547 samples
  [ 0] ReduceOps$5ReducingSink.get
  [ 1] ReduceOps$5ReducingSink.get
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 131964680 total (5.08%), 132333 samples
  [ 0] Cell$$Lambda$68.0x0000000801033678.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 115515247 total (4.45%), 115228 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 107678025 total (4.15%), 108102 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$68.0x0000000801033678.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 83768373 total (3.23%), 83783 samples
  [ 0] vtable stub
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 78286223 total (3.02%), 78546 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$54.0x0000000801036510.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 70157288 total (2.70%), 70156 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 57037759 total (2.20%), 57123 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 54748728 total (2.11%), 54978 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$68.0x0000000801033678.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 46524027 total (1.79%), 46757 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$54.0x0000000801036510.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 46518995 total (1.79%), 46559 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 46454412 total (1.79%), 46599 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$68.0x0000000801033678.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 44874395 total (1.73%), 45030 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$68.0x0000000801033678.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 40640577 total (1.57%), 40596 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 38015916 total (1.46%), 37918 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 29852038 total (1.15%), 29923 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 24799868 total (0.96%), 24697 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 23522347 total (0.91%), 23560 samples
  [ 0] ArrayList$ArrayListSpliterator.<init>
  [ 1] ArrayList.spliterator
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 23492964 total (0.91%), 23589 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] Cell.lambda$notifyLiveness$0
  [ 5] Cell$$Lambda$54.0x0000000801036510.accept
  [ 6] ArrayList.forEach
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 23476546 total (0.90%), 23499 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 19795980 total (0.76%), 19816 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 14095077 total (0.54%), 14135 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 12974200 total (0.50%), 13003 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 12119520 total (0.47%), 12083 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 10386821 total (0.40%), 10422 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 10310551 total (0.40%), 10315 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 10301763 total (0.40%), 10272 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 10209997 total (0.39%), 10203 samples
  [ 0] StreamOpFlag.getMask
  [ 1] StreamOpFlag.combineOpFlags
  [ 2] AbstractPipeline.<init>
  [ 3] ReferencePipeline.<init>
  [ 4] ReferencePipeline$StatelessOp.<init>
  [ 5] ReferencePipeline$3.<init>
  [ 6] ReferencePipeline.map
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 9840559 total (0.38%), 9843 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 9473517 total (0.36%), 9478 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 9469606 total (0.36%), 9492 samples
  [ 0] TickPerCell.lambda$tick$0
  [ 1] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 2] ChannelsGrid.lambda$forEachChannel$0
  [ 3] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 4] Dimensions.forEachRowCol
  [ 5] ChannelsGrid.forEachChannel
  [ 6] TickPerCell.tick
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 9158153 total (0.35%), 9166 samples
  [ 0] ReferencePipeline.map
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 8854079 total (0.34%), 8864 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 8239808 total (0.32%), 8247 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 8185060 total (0.32%), 8162 samples
  [ 0] ArrayList$ArrayListSpliterator.<init>
  [ 1] ArrayList.spliterator
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 8019850 total (0.31%), 8054 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 7581242 total (0.29%), 7580 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 6921484 total (0.27%), 6944 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 6593291 total (0.25%), 6613 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] [unknown_Java]

--- 6464460 total (0.25%), 6477 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 6459146 total (0.25%), 6445 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 6116814 total (0.24%), 6103 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 5928128 total (0.23%), 5953 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$68.0x0000000801033678.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 5774802 total (0.22%), 5801 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$68.0x0000000801033678.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 5633172 total (0.22%), 5638 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] [unknown_Java]

--- 4804958 total (0.19%), 4805 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] [unknown_Java]

--- 4798347 total (0.18%), 4800 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] [unknown_Java]

--- 4748755 total (0.18%), 4765 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$68.0x0000000801033678.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 4699167 total (0.18%), 4680 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 4672284 total (0.18%), 4688 samples
  [ 0] ReentrantLock$Sync.lock
  [ 1] ReentrantLock.lock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$68.0x0000000801033678.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 4564221 total (0.18%), 4588 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 4484613 total (0.17%), 4502 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 4122749 total (0.16%), 4135 samples
  [ 0] Channel.take
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 3961059 total (0.15%), 3952 samples
  [ 0] Object.<init>
  [ 1] PipelineHelper.<init>
  [ 2] AbstractPipeline.<init>
  [ 3] IntPipeline.<init>
  [ 4] IntPipeline$StatelessOp.<init>
  [ 5] ReferencePipeline$4.<init>
  [ 6] ReferencePipeline.mapToInt
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3913014 total (0.15%), 3936 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 3757279 total (0.14%), 3768 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3622352 total (0.14%), 3608 samples
  [ 0] Collection.stream
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 3561026 total (0.14%), 3571 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$68.0x0000000801033678.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 3338223 total (0.13%), 3324 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 3322962 total (0.13%), 3329 samples
  [ 0] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 3163825 total (0.12%), 3172 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] TickPerCell.lambda$tick$0
  [ 5] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 6] ChannelsGrid.lambda$forEachChannel$0
  [ 7] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] TickPerCell.tick
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 3071322 total (0.12%), 3074 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 3036043 total (0.12%), 3039 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 3003853 total (0.12%), 3006 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 2922238 total (0.11%), 2925 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2796852 total (0.11%), 2798 samples
  [ 0] StreamOpFlag.combineOpFlags
  [ 1] AbstractPipeline.<init>
  [ 2] ReferencePipeline.<init>
  [ 3] ReferencePipeline$StatelessOp.<init>
  [ 4] ReferencePipeline$3.<init>
  [ 5] ReferencePipeline.map
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 2683326 total (0.10%), 2685 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 2618527 total (0.10%), 2628 samples
  [ 0] ReentrantLock.lock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$68.0x0000000801033678.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 2540880 total (0.10%), 2536 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 2345813 total (0.09%), 2352 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 2274212 total (0.09%), 2286 samples
  [ 0] Channel.take
  [ 1] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [ 2] ChannelsGrid.lambda$forEachChannel$1
  [ 3] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 4] Dimensions.forEachRowCol
  [ 5] ChannelsGrid.forEachChannel
  [ 6] GameOfLife.calculateFrame
  [ 7] GameOfLife.lambda$calculateFrameBlocking$4
  [ 8] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 9] ThreadPoolExecutor.runWorker
  [10] ThreadPoolExecutor$Worker.run
  [11] Thread.run

--- 2250094 total (0.09%), 2248 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 2246393 total (0.09%), 2257 samples
  [ 0] ReentrantLock$Sync.isHeldExclusively
  [ 1] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$68.0x0000000801033678.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 2167248 total (0.08%), 2176 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$68.0x0000000801033678.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 2146432 total (0.08%), 2142 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] G1RemSetScanState::G1ClearCardTableTask::do_work(unsigned int)
  [ 2] G1BatchedTask::work(unsigned int)
  [ 3] WorkerThread::run()
  [ 4] Thread::call_run()
  [ 5] thread_native_entry(Thread*)
  [ 6] start_thread

--- 2136213 total (0.08%), 2132 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 2088983 total (0.08%), 2084 samples
  [ 0] ReferencePipeline.mapToInt
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 2083834 total (0.08%), 2093 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$68.0x0000000801033678.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 2081758 total (0.08%), 2091 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$68.0x0000000801033678.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 2034690 total (0.08%), 2043 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$68.0x0000000801033678.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 2034263 total (0.08%), 2043 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$68.0x0000000801033678.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 1939171 total (0.07%), 1948 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$68.0x0000000801033678.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 1895268 total (0.07%), 1897 samples
  [ 0] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 2] GameOfLife.lambda$calculateFrameBlocking$4
  [ 3] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 1880866 total (0.07%), 1885 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 1802486 total (0.07%), 1810 samples
  [ 0] AbstractQueuedSynchronizer.getState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$68.0x0000000801033678.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 1731435 total (0.07%), 1739 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 1704400 total (0.07%), 1712 samples
  [ 0] Boolean.booleanValue
  [ 1] Cell.lambda$calculateNextState$1
  [ 2] Cell$$Lambda$71.0x0000000801033aa8.applyAsInt
  [ 3] ReferencePipeline$4$1.accept
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 1682238 total (0.06%), 1690 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 1675845 total (0.06%), 1683 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 1661665 total (0.06%), 1669 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 1632524 total (0.06%), 1641 samples
  [ 0] Cell$$Lambda$54.0x0000000801036510.accept
  [ 1] ArrayList.forEach
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 1591132 total (0.06%), 1593 samples
  [ 0] MemAllocator::allocate() const
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 1585988 total (0.06%), 1593 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$68.0x0000000801033678.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 1547571 total (0.06%), 1548 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 1534736 total (0.06%), 1529 samples
  [ 0] G1CardSet::occupied() const
  [ 1] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 2] G1RemSetSamplingTask::execute()
  [ 3] G1ServiceThread::run_task(G1ServiceTask*)
  [ 4] G1ServiceThread::run_service()
  [ 5] ConcurrentGCThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 1516161 total (0.06%), 1493 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] ReduceOps$5ReducingSink.get
  [ 6] ReduceOps$5ReducingSink.get
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 1504726 total (0.06%), 1508 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 1498431 total (0.06%), 1501 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 1486757 total (0.06%), 1488 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 1481605 total (0.06%), 1456 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] AbstractQueuedSynchronizer$ConditionNode.block
  [14] ForkJoinPool.unmanagedBlock
  [15] ForkJoinPool.managedBlock
  [16] AbstractQueuedSynchronizer$ConditionObject.await
  [17] LockedSingleValue.take
  [18] Channel.take
  [19] TickPerCell.waitTick
  [20] Cell.notifyLiveness
  [21] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [22] Iterable.forEach
  [23] CellsGroup.run
  [24] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [25] ThreadPoolExecutor.runWorker
  [26] ThreadPoolExecutor$Worker.run
  [27] Thread.run

--- 1410197 total (0.05%), 1408 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 1292038 total (0.05%), 1297 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 1216489 total (0.05%), 1211 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 1200599 total (0.05%), 1197 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_change_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] AbstractQueuedSynchronizer.signalNext
  [15] AbstractQueuedSynchronizer.release
  [16] ReentrantLock.unlock
  [17] LockedSingleValue.put
  [18] Channel.put
  [19] TickPerCell.lambda$tick$0
  [20] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [21] ChannelsGrid.lambda$forEachChannel$0
  [22] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [23] Dimensions.forEachRowCol
  [24] ChannelsGrid.forEachChannel
  [25] TickPerCell.tick
  [26] GameOfLife.calculateFrame
  [27] GameOfLife.lambda$calculateFrameBlocking$4
  [28] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [29] ThreadPoolExecutor.runWorker
  [30] ThreadPoolExecutor$Worker.run
  [31] Thread.run

--- 1188954 total (0.05%), 1195 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$54.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 1150203 total (0.04%), 1153 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$68.0x0000000801033678.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 1141423 total (0.04%), 1145 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 1137619 total (0.04%), 1121 samples
  [ 0] update_curr_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] AbstractQueuedSynchronizer$ConditionNode.block
  [15] ForkJoinPool.unmanagedBlock
  [16] ForkJoinPool.managedBlock
  [17] AbstractQueuedSynchronizer$ConditionObject.await
  [18] LockedSingleValue.take
  [19] Channel.take
  [20] TickPerCell.waitTick
  [21] Cell.notifyLiveness
  [22] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [23] Iterable.forEach
  [24] CellsGroup.run
  [25] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 1115134 total (0.04%), 1117 samples
  [ 0] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 1112231 total (0.04%), 1118 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 1111997 total (0.04%), 1113 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 1105543 total (0.04%), 1100 samples
  [ 0] enqueue_entity_[k]
  [ 1] enqueue_task_fair_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] AbstractQueuedSynchronizer.signalNext
  [15] AbstractQueuedSynchronizer.release
  [16] ReentrantLock.unlock
  [17] LockedSingleValue.put
  [18] Channel.put
  [19] TickPerCell.lambda$tick$0
  [20] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [21] ChannelsGrid.lambda$forEachChannel$0
  [22] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [23] Dimensions.forEachRowCol
  [24] ChannelsGrid.forEachChannel
  [25] TickPerCell.tick
  [26] GameOfLife.calculateFrame
  [27] GameOfLife.lambda$calculateFrameBlocking$4
  [28] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [29] ThreadPoolExecutor.runWorker
  [30] ThreadPoolExecutor$Worker.run
  [31] Thread.run

--- 1058495 total (0.04%), 1065 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$54.0x0000000801036510.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 975894 total (0.04%), 977 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 933599 total (0.04%), 933 samples
  [ 0] ObjArrayAllocator::initialize(HeapWordImpl**) const
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 926634 total (0.04%), 931 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 915442 total (0.04%), 920 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$54.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 911795 total (0.04%), 901 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] AbstractQueuedSynchronizer$ConditionNode.block
  [ 4] ForkJoinPool.unmanagedBlock
  [ 5] ForkJoinPool.managedBlock
  [ 6] AbstractQueuedSynchronizer$ConditionObject.await
  [ 7] LockedSingleValue.take
  [ 8] Channel.take
  [ 9] TickPerCell.waitTick
  [10] Cell.notifyLiveness
  [11] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 907917 total (0.03%), 896 samples
  [ 0] update_cfs_group_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] AbstractQueuedSynchronizer$ConditionNode.block
  [15] ForkJoinPool.unmanagedBlock
  [16] ForkJoinPool.managedBlock
  [17] AbstractQueuedSynchronizer$ConditionObject.await
  [18] LockedSingleValue.take
  [19] Channel.take
  [20] TickPerCell.waitTick
  [21] Cell.notifyLiveness
  [22] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [23] Iterable.forEach
  [24] CellsGroup.run
  [25] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 889541 total (0.03%), 893 samples
  [ 0] Channel.put
  [ 1] Cell.lambda$notifyLiveness$0
  [ 2] Cell$$Lambda$54.0x0000000801036510.accept
  [ 3] ArrayList.forEach
  [ 4] Cell.notifyLiveness
  [ 5] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 6] Iterable.forEach
  [ 7] CellsGroup.run
  [ 8] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 9] ThreadPoolExecutor.runWorker
  [10] ThreadPoolExecutor$Worker.run
  [11] Thread.run

--- 878032 total (0.03%), 863 samples
  [ 0] update_load_avg_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] AbstractQueuedSynchronizer$ConditionNode.block
  [15] ForkJoinPool.unmanagedBlock
  [16] ForkJoinPool.managedBlock
  [17] AbstractQueuedSynchronizer$ConditionObject.await
  [18] LockedSingleValue.take
  [19] Channel.take
  [20] TickPerCell.waitTick
  [21] Cell.notifyLiveness
  [22] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [23] Iterable.forEach
  [24] CellsGroup.run
  [25] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 862029 total (0.03%), 862 samples
  [ 0] Thread.interrupted
  [ 1] AbstractQueuedSynchronizer$ConditionObject.await
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 853018 total (0.03%), 857 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$54.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 852461 total (0.03%), 848 samples
  [ 0] enqueue_task_fair_[k]
  [ 1] enqueue_task_[k]
  [ 2] ttwu_do_activate_[k]
  [ 3] try_to_wake_up_[k]
  [ 4] wake_up_q_[k]
  [ 5] futex_wake_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] ___pthread_cond_signal
  [11] Unsafe.unpark
  [12] LockSupport.unpark
  [13] AbstractQueuedSynchronizer.signalNext
  [14] AbstractQueuedSynchronizer.release
  [15] ReentrantLock.unlock
  [16] LockedSingleValue.put
  [17] Channel.put
  [18] TickPerCell.lambda$tick$0
  [19] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [20] ChannelsGrid.lambda$forEachChannel$0
  [21] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [22] Dimensions.forEachRowCol
  [23] ChannelsGrid.forEachChannel
  [24] TickPerCell.tick
  [25] GameOfLife.calculateFrame
  [26] GameOfLife.lambda$calculateFrameBlocking$4
  [27] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [28] ThreadPoolExecutor.runWorker
  [29] ThreadPoolExecutor$Worker.run
  [30] Thread.run

--- 835091 total (0.03%), 839 samples
  [ 0] AbstractQueuedSynchronizer.getState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$54.0x0000000801036510.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 828281 total (0.03%), 832 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.put
  [ 2] Channel.put
  [ 3] Cell.lambda$notifyLiveness$0
  [ 4] Cell$$Lambda$54.0x0000000801036510.accept
  [ 5] ArrayList.forEach
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 827848 total (0.03%), 828 samples
  [ 0] PipelineHelper.<init>
  [ 1] AbstractPipeline.<init>
  [ 2] ReferencePipeline.<init>
  [ 3] ReferencePipeline$Head.<init>
  [ 4] StreamSupport.stream
  [ 5] Collection.stream
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 804494 total (0.03%), 794 samples
  [ 0] __schedule_[k]
  [ 1] schedule_[k]
  [ 2] futex_wait_queue_[k]
  [ 3] futex_wait_[k]
  [ 4] do_futex_[k]
  [ 5] __x64_sys_futex_[k]
  [ 6] do_syscall_64_[k]
  [ 7] entry_SYSCALL_64_after_hwframe_[k]
  [ 8] __futex_abstimed_wait_common
  [ 9] Unsafe.park
  [10] LockSupport.park
  [11] AbstractQueuedSynchronizer$ConditionNode.block
  [12] ForkJoinPool.unmanagedBlock
  [13] ForkJoinPool.managedBlock
  [14] AbstractQueuedSynchronizer$ConditionObject.await
  [15] LockedSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 770948 total (0.03%), 773 samples
  [ 0] Thread.interrupted
  [ 1] AbstractQueuedSynchronizer$ConditionObject.await
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 728116 total (0.03%), 731 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] ChannelsGrid.lambda$forEachChannel$1
  [ 2] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 3] Dimensions.forEachRowCol
  [ 4] ChannelsGrid.forEachChannel
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 723367 total (0.03%), 724 samples
  [ 0] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 1] [unknown_Java]

--- 710630 total (0.03%), 701 samples
  [ 0] dequeue_task_fair_[k]
  [ 1] __schedule_[k]
  [ 2] schedule_[k]
  [ 3] futex_wait_queue_[k]
  [ 4] futex_wait_[k]
  [ 5] do_futex_[k]
  [ 6] __x64_sys_futex_[k]
  [ 7] do_syscall_64_[k]
  [ 8] entry_SYSCALL_64_after_hwframe_[k]
  [ 9] __futex_abstimed_wait_common
  [10] Unsafe.park
  [11] LockSupport.park
  [12] AbstractQueuedSynchronizer$ConditionNode.block
  [13] ForkJoinPool.unmanagedBlock
  [14] ForkJoinPool.managedBlock
  [15] AbstractQueuedSynchronizer$ConditionObject.await
  [16] LockedSingleValue.take
  [17] Channel.take
  [18] TickPerCell.waitTick
  [19] Cell.notifyLiveness
  [20] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [21] Iterable.forEach
  [22] CellsGroup.run
  [23] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [24] ThreadPoolExecutor.runWorker
  [25] ThreadPoolExecutor$Worker.run
  [26] Thread.run

--- 693924 total (0.03%), 689 samples
  [ 0] update_load_avg_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] AbstractQueuedSynchronizer.signalNext
  [16] AbstractQueuedSynchronizer.release
  [17] ReentrantLock.unlock
  [18] LockedSingleValue.put
  [19] Channel.put
  [20] TickPerCell.lambda$tick$0
  [21] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [22] ChannelsGrid.lambda$forEachChannel$0
  [23] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [24] Dimensions.forEachRowCol
  [25] ChannelsGrid.forEachChannel
  [26] TickPerCell.tick
  [27] GameOfLife.calculateFrame
  [28] GameOfLife.lambda$calculateFrameBlocking$4
  [29] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [30] ThreadPoolExecutor.runWorker
  [31] ThreadPoolExecutor$Worker.run
  [32] Thread.run

--- 690314 total (0.03%), 691 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 677867 total (0.03%), 680 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$68.0x0000000801033678.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 673565 total (0.03%), 673 samples
  [ 0] Klass::check_array_allocation_length(int, int, JavaThread*)
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 649469 total (0.03%), 652 samples
  [ 0] Cell$$Lambda$54.0x0000000801036510.accept
  [ 1] ArrayList.forEach
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 644537 total (0.02%), 626 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] __futex_abstimed_wait_common
  [ 4] Unsafe.park
  [ 5] LockSupport.park
  [ 6] AbstractQueuedSynchronizer$ConditionNode.block
  [ 7] ForkJoinPool.unmanagedBlock
  [ 8] ForkJoinPool.managedBlock
  [ 9] AbstractQueuedSynchronizer$ConditionObject.await
  [10] LockedSingleValue.take
  [11] Channel.take
  [12] TickPerCell.waitTick
  [13] Cell.notifyLiveness
  [14] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 642425 total (0.02%), 643 samples
  [ 0] FreeListAllocator::reset()
  [ 1] HeapRegionRemSet::clear_locked(bool)
  [ 2] HeapRegion::hr_clear(bool)
  [ 3] G1CollectedHeap::free_region(HeapRegion*, FreeRegionList*)
  [ 4] FreeCSetClosure::do_heap_region(HeapRegion*)
  [ 5] G1CollectedHeap::par_iterate_regions_array(HeapRegionClosure*, HeapRegionClaimer*, unsigned int const*, unsigned long, unsigned int) const
  [ 6] G1PostEvacuateCollectionSetCleanupTask2::FreeCollectionSetTask::do_work(unsigned int)
  [ 7] G1BatchedTask::work(unsigned int)
  [ 8] WorkerThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 626452 total (0.02%), 618 samples
  [ 0] __update_load_avg_cfs_rq_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] AbstractQueuedSynchronizer$ConditionNode.block
  [16] ForkJoinPool.unmanagedBlock
  [17] ForkJoinPool.managedBlock
  [18] AbstractQueuedSynchronizer$ConditionObject.await
  [19] LockedSingleValue.take
  [20] Channel.take
  [21] TickPerCell.waitTick
  [22] Cell.notifyLiveness
  [23] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [24] Iterable.forEach
  [25] CellsGroup.run
  [26] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 625095 total (0.02%), 624 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 615317 total (0.02%), 617 samples
  [ 0] Sink$ChainedReference.end
  [ 1] Sink$ChainedReference.end
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 611226 total (0.02%), 613 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] AbstractQueuedSynchronizer$ConditionNode.block
  [ 4] ForkJoinPool.unmanagedBlock
  [ 5] ForkJoinPool.managedBlock
  [ 6] AbstractQueuedSynchronizer$ConditionObject.await
  [ 7] LockedSingleValue.take
  [ 8] Channel.take
  [ 9] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [10] ChannelsGrid.lambda$forEachChannel$1
  [11] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [12] Dimensions.forEachRowCol
  [13] ChannelsGrid.forEachChannel
  [14] GameOfLife.calculateFrame
  [15] GameOfLife.lambda$calculateFrameBlocking$4
  [16] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 610136 total (0.02%), 606 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 602594 total (0.02%), 593 samples
  [ 0] Parker::park(bool, long)
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] AbstractQueuedSynchronizer$ConditionNode.block
  [ 5] ForkJoinPool.unmanagedBlock
  [ 6] ForkJoinPool.managedBlock
  [ 7] AbstractQueuedSynchronizer$ConditionObject.await
  [ 8] LockedSingleValue.take
  [ 9] Channel.take
  [10] TickPerCell.waitTick
  [11] Cell.notifyLiveness
  [12] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 596479 total (0.02%), 587 samples
  [ 0] __update_load_avg_se_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] AbstractQueuedSynchronizer$ConditionNode.block
  [16] ForkJoinPool.unmanagedBlock
  [17] ForkJoinPool.managedBlock
  [18] AbstractQueuedSynchronizer$ConditionObject.await
  [19] LockedSingleValue.take
  [20] Channel.take
  [21] TickPerCell.waitTick
  [22] Cell.notifyLiveness
  [23] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [24] Iterable.forEach
  [25] CellsGroup.run
  [26] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 568368 total (0.02%), 563 samples
  [ 0] futex_wake_[k]
  [ 1] do_futex_[k]
  [ 2] __x64_sys_futex_[k]
  [ 3] do_syscall_64_[k]
  [ 4] entry_SYSCALL_64_after_hwframe_[k]
  [ 5] ___pthread_cond_signal
  [ 6] Unsafe.unpark
  [ 7] LockSupport.unpark
  [ 8] AbstractQueuedSynchronizer.signalNext
  [ 9] AbstractQueuedSynchronizer.release
  [10] ReentrantLock.unlock
  [11] LockedSingleValue.put
  [12] Channel.put
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 564811 total (0.02%), 567 samples
  [ 0] G1Analytics::predict_scan_card_num(unsigned long, bool) const
  [ 1] G1Policy::predict_region_non_copy_time_ms(HeapRegion*, bool) const
  [ 2] G1CollectionSet::update_young_region_prediction(HeapRegion*, unsigned long)
  [ 3] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 4] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 5] G1RemSetSamplingTask::execute()
  [ 6] G1ServiceThread::run_task(G1ServiceTask*)
  [ 7] G1ServiceThread::run_service()
  [ 8] ConcurrentGCThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 560059 total (0.02%), 557 samples
  [ 0] MemAllocator::Allocation::notify_allocation_jvmti_sampler()
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 559773 total (0.02%), 555 samples
  [ 0] Unsafe.park
  [ 1] LockSupport.park
  [ 2] AbstractQueuedSynchronizer$ConditionNode.block
  [ 3] ForkJoinPool.unmanagedBlock
  [ 4] ForkJoinPool.managedBlock
  [ 5] AbstractQueuedSynchronizer$ConditionObject.await
  [ 6] LockedSingleValue.take
  [ 7] Channel.take
  [ 8] TickPerCell.waitTick
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 556519 total (0.02%), 558 samples
  [ 0] DirectMethodHandle.allocateInstance
  [ 1] DirectMethodHandle$Holder.newInvokeSpecial
  [ 2] Invokers$Holder.linkToTargetMethod
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 532450 total (0.02%), 531 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 525628 total (0.02%), 526 samples
  [ 0] MemAllocator::Allocation::check_out_of_memory()
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 521149 total (0.02%), 511 samples
  [ 0] update_blocked_averages_[k]
  [ 1] newidle_balance_[k]
  [ 2] pick_next_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] AbstractQueuedSynchronizer$ConditionNode.block
  [15] ForkJoinPool.unmanagedBlock
  [16] ForkJoinPool.managedBlock
  [17] AbstractQueuedSynchronizer$ConditionObject.await
  [18] LockedSingleValue.take
  [19] Channel.take
  [20] TickPerCell.waitTick
  [21] Cell.notifyLiveness
  [22] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [23] Iterable.forEach
  [24] CellsGroup.run
  [25] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 505260 total (0.02%), 506 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 500903 total (0.02%), 501 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 489525 total (0.02%), 491 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 477632 total (0.02%), 479 samples
  [ 0] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 2] GameOfLife.lambda$calculateFrameBlocking$4
  [ 3] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 476679 total (0.02%), 473 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 470707 total (0.02%), 471 samples
  [ 0] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 465157 total (0.02%), 442 samples
  [ 0] syscall_return_via_sysret_[k]
  [ 1] __futex_abstimed_wait_common
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] AbstractQueuedSynchronizer$ConditionNode.block
  [ 5] ForkJoinPool.unmanagedBlock
  [ 6] ForkJoinPool.managedBlock
  [ 7] AbstractQueuedSynchronizer$ConditionObject.await
  [ 8] LockedSingleValue.take
  [ 9] Channel.take
  [10] TickPerCell.waitTick
  [11] Cell.notifyLiveness
  [12] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 464309 total (0.02%), 464 samples
  [ 0] __update_load_avg_se_[k]
  [ 1] update_load_avg_[k]
  [ 2] enqueue_entity_[k]
  [ 3] enqueue_task_fair_[k]
  [ 4] enqueue_task_[k]
  [ 5] ttwu_do_activate_[k]
  [ 6] try_to_wake_up_[k]
  [ 7] wake_up_q_[k]
  [ 8] futex_wake_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] ___pthread_cond_signal
  [14] Unsafe.unpark
  [15] LockSupport.unpark
  [16] AbstractQueuedSynchronizer.signalNext
  [17] AbstractQueuedSynchronizer.release
  [18] ReentrantLock.unlock
  [19] LockedSingleValue.put
  [20] Channel.put
  [21] TickPerCell.lambda$tick$0
  [22] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [23] ChannelsGrid.lambda$forEachChannel$0
  [24] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [25] Dimensions.forEachRowCol
  [26] ChannelsGrid.forEachChannel
  [27] TickPerCell.tick
  [28] GameOfLife.calculateFrame
  [29] GameOfLife.lambda$calculateFrameBlocking$4
  [30] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [31] ThreadPoolExecutor.runWorker
  [32] ThreadPoolExecutor$Worker.run
  [33] Thread.run

--- 457056 total (0.02%), 458 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 451744 total (0.02%), 449 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] ___pthread_cond_signal
  [ 4] Unsafe.unpark
  [ 5] LockSupport.unpark
  [ 6] AbstractQueuedSynchronizer.signalNext
  [ 7] AbstractQueuedSynchronizer.release
  [ 8] ReentrantLock.unlock
  [ 9] LockedSingleValue.put
  [10] Channel.put
  [11] TickPerCell.lambda$tick$0
  [12] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [13] ChannelsGrid.lambda$forEachChannel$0
  [14] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [15] Dimensions.forEachRowCol
  [16] ChannelsGrid.forEachChannel
  [17] TickPerCell.tick
  [18] GameOfLife.calculateFrame
  [19] GameOfLife.lambda$calculateFrameBlocking$4
  [20] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [21] ThreadPoolExecutor.runWorker
  [22] ThreadPoolExecutor$Worker.run
  [23] Thread.run

--- 449381 total (0.02%), 443 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] AbstractQueuedSynchronizer$ConditionNode.block
  [14] ForkJoinPool.unmanagedBlock
  [15] ForkJoinPool.managedBlock
  [16] AbstractQueuedSynchronizer$ConditionObject.await
  [17] LockedSingleValue.take
  [18] Channel.take
  [19] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [20] ChannelsGrid.lambda$forEachChannel$1
  [21] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [22] Dimensions.forEachRowCol
  [23] ChannelsGrid.forEachChannel
  [24] GameOfLife.calculateFrame
  [25] GameOfLife.lambda$calculateFrameBlocking$4
  [26] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 440912 total (0.02%), 442 samples
  [ 0] Thread.interrupted
  [ 1] AbstractQueuedSynchronizer$ConditionObject.await
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$68.0x0000000801033678.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 439930 total (0.02%), 439 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 435555 total (0.02%), 434 samples
  [ 0] reweight_entity_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] AbstractQueuedSynchronizer.signalNext
  [16] AbstractQueuedSynchronizer.release
  [17] ReentrantLock.unlock
  [18] LockedSingleValue.put
  [19] Channel.put
  [20] TickPerCell.lambda$tick$0
  [21] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [22] ChannelsGrid.lambda$forEachChannel$0
  [23] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [24] Dimensions.forEachRowCol
  [25] ChannelsGrid.forEachChannel
  [26] TickPerCell.tick
  [27] GameOfLife.calculateFrame
  [28] GameOfLife.lambda$calculateFrameBlocking$4
  [29] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [30] ThreadPoolExecutor.runWorker
  [31] ThreadPoolExecutor$Worker.run
  [32] Thread.run

--- 432774 total (0.02%), 428 samples
  [ 0] reweight_entity_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] AbstractQueuedSynchronizer$ConditionNode.block
  [15] ForkJoinPool.unmanagedBlock
  [16] ForkJoinPool.managedBlock
  [17] AbstractQueuedSynchronizer$ConditionObject.await
  [18] LockedSingleValue.take
  [19] Channel.take
  [20] TickPerCell.waitTick
  [21] Cell.notifyLiveness
  [22] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [23] Iterable.forEach
  [24] CellsGroup.run
  [25] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 428006 total (0.02%), 429 samples
  [ 0] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 1] Dimensions.forEachRowCol
  [ 2] ChannelsGrid.forEachChannel
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 426355 total (0.02%), 428 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 426339 total (0.02%), 425 samples
  [ 0] IntPipeline.reduce
  [ 1] IntPipeline.sum
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 418474 total (0.02%), 419 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 418374 total (0.02%), 413 samples
  [ 0] update_sd_lb_stats.constprop.0_[k]
  [ 1] find_busiest_group_[k]
  [ 2] load_balance_[k]
  [ 3] newidle_balance_[k]
  [ 4] pick_next_task_fair_[k]
  [ 5] __schedule_[k]
  [ 6] schedule_[k]
  [ 7] futex_wait_queue_[k]
  [ 8] futex_wait_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] __futex_abstimed_wait_common
  [14] Unsafe.park
  [15] LockSupport.park
  [16] AbstractQueuedSynchronizer$ConditionNode.block
  [17] ForkJoinPool.unmanagedBlock
  [18] ForkJoinPool.managedBlock
  [19] AbstractQueuedSynchronizer$ConditionObject.await
  [20] LockedSingleValue.take
  [21] Channel.take
  [22] TickPerCell.waitTick
  [23] Cell.notifyLiveness
  [24] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [25] Iterable.forEach
  [26] CellsGroup.run
  [27] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [28] ThreadPoolExecutor.runWorker
  [29] ThreadPoolExecutor$Worker.run
  [30] Thread.run

--- 417400 total (0.02%), 419 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)3, 286822ul>::oop_access_barrier(oopDesc*, long)
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] AbstractQueuedSynchronizer$ConditionNode.block
  [ 5] ForkJoinPool.unmanagedBlock
  [ 6] ForkJoinPool.managedBlock
  [ 7] AbstractQueuedSynchronizer$ConditionObject.await
  [ 8] LockedSingleValue.take
  [ 9] Channel.take
  [10] TickPerCell.waitTick
  [11] Cell.notifyLiveness
  [12] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 417041 total (0.02%), 419 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$54.0x0000000801036510.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 406620 total (0.02%), 408 samples
  [ 0] ArrayList.forEach
  [ 1] [unknown_Java]

--- 396897 total (0.02%), 398 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 396798 total (0.02%), 392 samples
  [ 0] cpuacct_charge_[k]
  [ 1] update_curr_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] AbstractQueuedSynchronizer$ConditionNode.block
  [16] ForkJoinPool.unmanagedBlock
  [17] ForkJoinPool.managedBlock
  [18] AbstractQueuedSynchronizer$ConditionObject.await
  [19] LockedSingleValue.take
  [20] Channel.take
  [21] TickPerCell.waitTick
  [22] Cell.notifyLiveness
  [23] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [24] Iterable.forEach
  [25] CellsGroup.run
  [26] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 392764 total (0.02%), 391 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 391955 total (0.02%), 392 samples
  [ 0] MemAllocator::Allocation::notify_allocation_jfr_sampler()
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 391769 total (0.02%), 392 samples
  [ 0] ArrayList$SubList$1.checkForComodification
  [ 1] ArrayList$SubList$1.next
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 391676 total (0.02%), 388 samples
  [ 0] resched_curr_[k]
  [ 1] check_preempt_curr_[k]
  [ 2] ttwu_do_wakeup_[k]
  [ 3] try_to_wake_up_[k]
  [ 4] wake_up_q_[k]
  [ 5] futex_wake_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] ___pthread_cond_signal
  [11] Unsafe.unpark
  [12] LockSupport.unpark
  [13] AbstractQueuedSynchronizer.signalNext
  [14] AbstractQueuedSynchronizer.release
  [15] ReentrantLock.unlock
  [16] LockedSingleValue.put
  [17] Channel.put
  [18] TickPerCell.lambda$tick$0
  [19] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [20] ChannelsGrid.lambda$forEachChannel$0
  [21] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [22] Dimensions.forEachRowCol
  [23] ChannelsGrid.forEachChannel
  [24] TickPerCell.tick
  [25] GameOfLife.calculateFrame
  [26] GameOfLife.lambda$calculateFrameBlocking$4
  [27] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [28] ThreadPoolExecutor.runWorker
  [29] ThreadPoolExecutor$Worker.run
  [30] Thread.run

--- 390195 total (0.02%), 388 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.enableWait
  [ 1] AbstractQueuedSynchronizer$ConditionObject.await
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 386493 total (0.01%), 385 samples
  [ 0] update_cfs_group_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] AbstractQueuedSynchronizer.signalNext
  [16] AbstractQueuedSynchronizer.release
  [17] ReentrantLock.unlock
  [18] LockedSingleValue.put
  [19] Channel.put
  [20] TickPerCell.lambda$tick$0
  [21] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [22] ChannelsGrid.lambda$forEachChannel$0
  [23] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [24] Dimensions.forEachRowCol
  [25] ChannelsGrid.forEachChannel
  [26] TickPerCell.tick
  [27] GameOfLife.calculateFrame
  [28] GameOfLife.lambda$calculateFrameBlocking$4
  [29] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [30] ThreadPoolExecutor.runWorker
  [31] ThreadPoolExecutor$Worker.run
  [32] Thread.run

--- 381660 total (0.01%), 375 samples
  [ 0] __calc_delta_[k]
  [ 1] update_curr_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] AbstractQueuedSynchronizer$ConditionNode.block
  [16] ForkJoinPool.unmanagedBlock
  [17] ForkJoinPool.managedBlock
  [18] AbstractQueuedSynchronizer$ConditionObject.await
  [19] LockedSingleValue.take
  [20] Channel.take
  [21] TickPerCell.waitTick
  [22] Cell.notifyLiveness
  [23] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [24] Iterable.forEach
  [25] CellsGroup.run
  [26] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 374632 total (0.01%), 374 samples
  [ 0] __update_load_avg_cfs_rq_[k]
  [ 1] update_load_avg_[k]
  [ 2] enqueue_entity_[k]
  [ 3] enqueue_task_fair_[k]
  [ 4] enqueue_task_[k]
  [ 5] ttwu_do_activate_[k]
  [ 6] try_to_wake_up_[k]
  [ 7] wake_up_q_[k]
  [ 8] futex_wake_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] ___pthread_cond_signal
  [14] Unsafe.unpark
  [15] LockSupport.unpark
  [16] AbstractQueuedSynchronizer.signalNext
  [17] AbstractQueuedSynchronizer.release
  [18] ReentrantLock.unlock
  [19] LockedSingleValue.put
  [20] Channel.put
  [21] TickPerCell.lambda$tick$0
  [22] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [23] ChannelsGrid.lambda$forEachChannel$0
  [24] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [25] Dimensions.forEachRowCol
  [26] ChannelsGrid.forEachChannel
  [27] TickPerCell.tick
  [28] GameOfLife.calculateFrame
  [29] GameOfLife.lambda$calculateFrameBlocking$4
  [30] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [31] ThreadPoolExecutor.runWorker
  [32] ThreadPoolExecutor$Worker.run
  [33] Thread.run

--- 372107 total (0.01%), 372 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] AbstractQueuedSynchronizer$ConditionObject.enableWait
  [ 2] AbstractQueuedSynchronizer$ConditionObject.await
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$58.0x0000000801036fb0.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 370665 total (0.01%), 366 samples
  [ 0] dequeue_entity_[k]
  [ 1] dequeue_task_fair_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] AbstractQueuedSynchronizer$ConditionNode.block
  [14] ForkJoinPool.unmanagedBlock
  [15] ForkJoinPool.managedBlock
  [16] AbstractQueuedSynchronizer$ConditionObject.await
  [17] LockedSingleValue.take
  [18] Channel.take
  [19] TickPerCell.waitTick
  [20] Cell.notifyLiveness
  [21] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [22] Iterable.forEach
  [23] CellsGroup.run
  [24] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [25] ThreadPoolExecutor.runWorker
  [26] ThreadPoolExecutor$Worker.run
  [27] Thread.run

--- 368252 total (0.01%), 370 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.put
  [ 2] Channel.put
  [ 3] Cell.lambda$notifyLiveness$0
  [ 4] Cell$$Lambda$54.0x0000000801036510.accept
  [ 5] ArrayList.forEach
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 365400 total (0.01%), 366 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] AbstractQueuedSynchronizer$ConditionNode.block
  [ 4] ForkJoinPool.unmanagedBlock
  [ 5] ForkJoinPool.managedBlock
  [ 6] AbstractQueuedSynchronizer$ConditionObject.await
  [ 7] LockedSingleValue.take
  [ 8] Channel.take
  [ 9] Cell$$Lambda$68.0x0000000801033678.apply
  [10] ReferencePipeline$3$1.accept
  [11] ArrayList$ArrayListSpliterator.forEachRemaining
  [12] AbstractPipeline.copyInto
  [13] AbstractPipeline.wrapAndCopyInto
  [14] ReduceOps$ReduceOp.evaluateSequential
  [15] AbstractPipeline.evaluate
  [16] IntPipeline.reduce
  [17] IntPipeline.sum
  [18] Cell.calculateNextState
  [19] CellsGroup$$Lambda$60.0x0000000801037400.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 363417 total (0.01%), 364 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 360363 total (0.01%), 361 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 353134 total (0.01%), 351 samples
  [ 0] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 1] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 2] G1RemSetSamplingTask::execute()
  [ 3] G1ServiceThread::run_task(G1ServiceTask*)
  [ 4] G1ServiceThread::run_service()
  [ 5] ConcurrentGCThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 351735 total (0.01%), 349 samples
  [ 0] ObjArrayKlass::allocate(int, JavaThread*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 350856 total (0.01%), 353 samples
  [ 0] AbsSeq::dsd() const
  [ 1] G1Policy::predict_region_non_copy_time_ms(HeapRegion*, bool) const
  [ 2] G1CollectionSet::update_young_region_prediction(HeapRegion*, unsigned long)
  [ 3] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 4] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 5] G1RemSetSamplingTask::execute()
  [ 6] G1ServiceThread::run_task(G1ServiceTask*)
  [ 7] G1ServiceThread::run_service()
  [ 8] ConcurrentGCThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 349201 total (0.01%), 351 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] ChannelsGrid.lambda$forEachChannel$1
  [ 2] ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
  [ 3] Dimensions.forEachRowCol
  [ 4] ChannelsGrid.forEachChannel
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 346012 total (0.01%), 347 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 343150 total (0.01%), 341 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] __futex_abstimed_wait_common
  [ 4] Unsafe.park
  [ 5] LockSupport.park
  [ 6] AbstractQueuedSynchronizer$ConditionNode.block
  [ 7] ForkJoinPool.unmanagedBlock
  [ 8] ForkJoinPool.managedBlock
  [ 9] AbstractQueuedSynchronizer$ConditionObject.await
  [10] LockedSingleValue.take
  [11] Channel.take
  [12] GameOfLife.calculateFrameBlocking
  [13] GameOfLifeBenchmark.benchmark
  [14] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
  [15] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_Throughput
  [16] DirectMethodHandle$Holder.invokeSpecial
  [17] LambdaForm$MH.0x000000080102e000.invoke
  [18] LambdaForm$MH.0x000000080102e400.invokeExact_MT
  [19] DirectMethodHandleAccessor.invokeImpl
  [20] DirectMethodHandleAccessor.invoke
  [21] Method.invoke
  [22] BenchmarkHandler$BenchmarkTask.call
  [23] BenchmarkHandler$BenchmarkTask.call
  [24] FutureTask.run
  [25] Executors$RunnableAdapter.call
  [26] FutureTask.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 339671 total (0.01%), 334 samples
  [ 0] futex_q_lock_[k]
  [ 1] futex_wait_setup_[k]
  [ 2] futex_wait_[k]
  [ 3] do_futex_[k]
  [ 4] __x64_sys_futex_[k]
  [ 5] do_syscall_64_[k]
  [ 6] entry_SYSCALL_64_after_hwframe_[k]
  [ 7] __futex_abstimed_wait_common
  [ 8] Unsafe.park
  [ 9] LockSupport.park
  [10] AbstractQueuedSynchronizer$ConditionNode.block
  [11] ForkJoinPool.unmanagedBlock
  [12] ForkJoinPool.managedBlock
  [13] AbstractQueuedSynchronizer$ConditionObject.await
  [14] LockedSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$46.0x0000000801035478.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

       total  percent  samples  top
  ----------  -------  -------  ---
   953524426   36.74%   952556  ReduceOps$5ReducingSink.get
   176638451    6.81%   176287  AbstractPipeline.<init>
   172347771    6.64%   173085  AbstractQueuedSynchronizer.compareAndSetState
   131964680    5.08%   132333  Cell$$Lambda$68.0x0000000801033678.apply
   120206902    4.63%   120388  Sink$ChainedReference.<init>
    93061346    3.59%    93432  AbstractQueuedSynchronizer.release
    85924518    3.31%    85929  ReferencePipeline$4.opWrapSink
    83770341    3.23%    83785  vtable stub
    83355660    3.21%    83194  StreamOpFlag.fromCharacteristics
    78286223    3.02%    78546  Cell.lambda$notifyLiveness$0
    62480376    2.41%    62532  ReferencePipeline$3.opWrapSink
    58179705    2.24%    58382  LockedSingleValue.take
    57990986    2.23%    58177  Channel.take
    43687714    1.68%    43793  ReferencePipeline$3$1.accept
    41114597    1.58%    41155  Sink$ChainedReference.begin
    31707407    1.22%    31722  ArrayList$ArrayListSpliterator.<init>
    24799868    0.96%    24697  StreamSupport.stream
    10652564    0.41%    10654  AbstractPipeline.wrapSink
    10209997    0.39%    10203  StreamOpFlag.getMask
     9469606    0.36%     9492  TickPerCell.lambda$tick$0
     9158153    0.35%     9166  ReferencePipeline.map
     7930765    0.31%     7962  ReentrantLock$NonfairSync.initialTryLock
     7808102    0.30%     7840  AbstractQueuedSynchronizer.signalNext
     7401285    0.29%     7434  AbstractOwnableSynchronizer.getExclusiveOwnerThread
     6737410    0.26%     6766  AbstractQueuedSynchronizer$ConditionObject.signal
     5886043    0.23%     5867  Cell.calculateNextState
     5552182    0.21%     5508  __memset_avx2_unaligned_erms
     5302902    0.20%     5324  ReferencePipeline$4$1.accept
     5293615    0.20%     5311  ReentrantLock$Sync.lock
     4438764    0.17%     4429  Object.<init>
     4376304    0.17%     4335  psi_group_change_[k]
     3793782    0.15%     3808  ChannelsGrid.getChannel
     3675261    0.14%     3692  AbstractQueuedSynchronizer.setState
     3622352    0.14%     3608  Collection.stream
     3554372    0.14%     3560  CellsGroup$$Lambda$46.0x0000000801035478.accept
     3549691    0.14%     3567  LockedSingleValue.put
     3276577    0.13%     3279  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
     3099301    0.12%     3094  Dimensions.forEachRowCol
     2979097    0.11%     2991  ReentrantLock.lock
     2969161    0.11%     2981  AbstractQueuedSynchronizer.getState
     2957329    0.11%     2959  StreamOpFlag.combineOpFlags
     2858115    0.11%     2873  ReentrantLock$Sync.isHeldExclusively
     2766820    0.11%     2734  __update_load_avg_cfs_rq_[k]
     2692949    0.10%     2661  update_load_avg_[k]
     2645844    0.10%     2603  update_blocked_averages_[k]
     2600185    0.10%     2607  ArrayList.forEach
     2540889    0.10%     2499  syscall_exit_to_user_mode_[k]
     2457277    0.09%     2458  Cell.notifyLiveness
     2455329    0.09%     2465  ReentrantLock$Sync.tryRelease
     2388431    0.09%     2362  update_curr_[k]
     2324407    0.09%     2315  Unsafe_Park
     2304730    0.09%     2294  Iterable.forEach
     2302453    0.09%     2282  update_cfs_group_[k]
     2281993    0.09%     2293  Cell$$Lambda$54.0x0000000801036510.accept
     2243001    0.09%     2241  ArrayList$SubList$1.next
     2209118    0.09%     2212  Thread.interrupted
     2088983    0.08%     2084  ReferencePipeline.mapToInt
     1946422    0.07%     1948  MemAllocator::allocate() const
     1945608    0.07%     1947  ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
     1893651    0.07%     1876  __update_load_avg_se_[k]
     1785659    0.07%     1778  G1CardSet::occupied() const
     1746271    0.07%     1754  Boolean.booleanValue
     1705612    0.07%     1713  ReduceOps$5ReducingSink.accept
     1696232    0.07%     1706  AbstractQueuedSynchronizer.acquire
     1682483    0.06%     1672  enqueue_entity_[k]
     1592766    0.06%     1596  TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
     1506598    0.06%     1481  __entry_text_start_[k]
     1498431    0.06%     1501  ArrayList$ArrayListSpliterator.forEachRemaining
     1394038    0.05%     1383  reweight_entity_[k]
     1329951    0.05%     1316  __schedule_[k]
     1320570    0.05%     1312  enqueue_task_fair_[k]
     1292259    0.05%     1291  ObjArrayAllocator::initialize(HeapWordImpl**) const
     1263593    0.05%     1267  ChannelsGrid$$Lambda$61.0x0000000801037a48.accept
     1236224    0.05%     1213  futex_wake_[k]
     1194074    0.05%     1195  CellsGroup$$Lambda$60.0x0000000801037400.accept
     1171691    0.05%     1156  Parker::park(bool, long)
     1140060    0.04%     1136  Unsafe.park
     1111379    0.04%     1100  dequeue_task_fair_[k]
     1070350    0.04%     1072  Channel.put
     1056860    0.04%     1048  ___pthread_cond_wait
      954587    0.04%      955  AbstractQueuedSynchronizer$ConditionObject.enableWait
      927186    0.04%      931  AbstractQueuedSynchronizer.enqueue
      923182    0.04%      921  Klass::check_array_allocation_length(int, int, JavaThread*)
      912229    0.04%      915  Sink$ChainedReference.end
      910284    0.04%      872  syscall_return_via_sysret_[k]
      885885    0.03%      872  check_preemption_disabled_[k]
      864304    0.03%      855  update_rq_clock_[k]
      852699    0.03%      849  MemAllocator::Allocation::notify_allocation_jvmti_sampler()
      843346    0.03%      844  AbstractQueuedSynchronizer$ConditionObject.doSignal
      838810    0.03%      839  PipelineHelper.<init>
      831729    0.03%      823  _raw_spin_lock_[k]
      806996    0.03%      808  __tls_get_addr
      786482    0.03%      802  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)3, 286822ul>::oop_access_barrier(oopDesc*, long)
      779665    0.03%      783  Unsafe.getAndBitwiseAndInt
      750313    0.03%      752  G1ParScanThreadState::trim_queue_to_threshold(unsigned int)
      747091    0.03%      747  __GI___pthread_mutex_lock
      744392    0.03%      735  __calc_delta_[k]
      734956    0.03%      729  ArrayList$SubList$1.checkForComodification
      724131    0.03%      724  G1Analytics::predict_scan_card_num(unsigned long, bool) const
      704591    0.03%      698  dequeue_entity_[k]
      680222    0.03%      681  select_task_rq_fair_[k]
      671202    0.03%      672  MemAllocator::Allocation::check_out_of_memory()
      667758    0.03%      671  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<548964ul, G1BarrierSet>, (AccessInternal::BarrierType)2, 548964ul>::oop_access_barrier(void*)
      666574    0.03%      660  cpuacct_charge_[k]
      642732    0.02%      616  restore_fpregs_from_fpstate_[k]
      642425    0.02%      643  FreeListAllocator::reset()
      629029    0.02%      623  native_sched_clock_[k]
      626673    0.02%      572  __get_user_8_[k]
      613593    0.02%      568  fpregs_restore_userregs_[k]
      611044    0.02%      588  __condvar_dec_grefs
      608618    0.02%      601  update_sd_lb_stats.constprop.0_[k]
      597352    0.02%      592  try_to_wake_up_[k]
      595731    0.02%      598  AbsSeq::dsd() const
      587951    0.02%      589  DirectMethodHandle.allocateInstance
      586777    0.02%      590  pthread_mutex_trylock@@GLIBC_2.34
      586105    0.02%      533  exit_to_user_mode_prepare_[k]
      566704    0.02%      560  futex_q_lock_[k]
      561651    0.02%      558  psi_task_change_[k]
      554990    0.02%      558  AbstractQueuedSynchronizer$ConditionNode.isReleasable
      545871    0.02%      541  G1Policy::preventive_collection_required(unsigned int)
      539908    0.02%      540  ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
      513716    0.02%      510  __pthread_mutex_unlock_usercnt
      512311    0.02%      503  _raw_spin_lock_irqsave_[k]
      509124    0.02%      507  JavaThread::threadObj() const
      505662    0.02%      503  Unsafe_Unpark
      505071    0.02%      508  AbstractQueuedSynchronizer$ConditionObject.canReacquire
      498945    0.02%      493  futex_wake_mark_[k]
      496721    0.02%      492  resched_curr_[k]
      494989    0.02%      488  newidle_balance_[k]
      493858    0.02%      494  MemAllocator::Allocation::notify_allocation_jfr_sampler()
      484095    0.02%      488  ThreadsListHandle::ThreadsListHandle(Thread*)
      482391    0.02%      474  rb_next_[k]
      473474    0.02%      466  rcu_sched_clock_irq_[k]
      473362    0.02%      470  HeapRegionManager::allocate_free_region(HeapRegionType, unsigned int)
      471833    0.02%      470  LockSupport.unpark
      470614    0.02%      471  G1Analytics::predict_card_merge_time_ms(unsigned long, bool) const
      466913    0.02%      466  IntPipeline.reduce
      464794    0.02%      465  G1FromCardCache::clear(unsigned int)
      454210    0.02%      457  ___pthread_cond_signal
      448269    0.02%      447  ThreadsListHandle::cv_internal_thread_to_JavaThread(_jobject*, JavaThread**, oopDesc**)
      448011    0.02%      445  update_irq_load_avg_[k]
      442801    0.02%      436  __futex_abstimed_wait_common
      437860    0.02%      438  __GI___pthread_getspecific
      434763    0.02%      445  java_lang_Thread::set_thread_status(oopDesc*, JavaThreadStatus)
      412399    0.02%      408  G1CollectedHeap::attempt_allocation_slow(unsigned long)
      412165    0.02%      370  __rseq_handle_notify_resume_[k]
      398673    0.02%      400  AbstractQueuedSynchronizer.casTail
      397904    0.02%      393  iterate_groups_[k]
      391081    0.02%      381  futex_wait_[k]
      386654    0.01%      380  __hrtimer_run_queues_[k]
      382940    0.01%      379  preempt_count_add_[k]
      380144    0.01%      376  update_min_vruntime_[k]
      379420    0.01%      375  __cgroup_account_cputime_[k]
      373946    0.01%      377  native_write_msr_[k]
      371073    0.01%      369  available_idle_cpu_[k]
      364790    0.01%      362  ObjArrayKlass::allocate(int, JavaThread*)
      359743    0.01%      355  G1Allocator::unsafe_max_tlab_alloc()
      355107    0.01%      353  G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
      351400    0.01%      353  ReentrantLock.unlock
      346581    0.01%      347  Integer.intValue
      339814    0.01%      335  AbsSeq::davg() const
      339228    0.01%      337  G1CollectedHeap::allocate_new_tlab(unsigned long, unsigned long, unsigned long*)
      334453    0.01%      328  preempt_count_sub_[k]
      327458    0.01%      323  psi_task_switch_[k]
      327416    0.01%      323  __perf_event_task_sched_out_[k]
      317915    0.01%      319  java_lang_Thread::get_thread_status(oopDesc*)
      315708    0.01%      313  G1CardSet::clear()
      314995    0.01%      316  ArrayList.elementAt
      312994    0.01%      312  ClassLoaderData::holder() const
      312665    0.01%      309  timerqueue_add_[k]
      303446    0.01%      299  native_read_msr_[k]
      287473    0.01%      289  G1RemSetScanState::G1ClearCardTableTask::do_work(unsigned int)
      286191    0.01%      289  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<544868ul, G1BarrierSet>, (AccessInternal::BarrierType)2, 544868ul>::oop_access_barrier(void*)
      279979    0.01%      279  MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
      274116    0.01%      273  OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
      270495    0.01%      269  G1CollectedHeap::fill_with_dummy_object(HeapWordImpl**, HeapWordImpl**, bool)
      269923    0.01%      265  ThreadLocalStorage::is_initialized()
      267737    0.01%      264  __softirqentry_text_start_[k]
      257279    0.01%      253  asm_sysvec_apic_timer_interrupt_[k]
      256172    0.01%      255  G1SegmentedArray::num_segments() const
      254306    0.01%      246  __pthread_mutex_cond_lock
      252805    0.01%      255  AbstractQueuedSynchronizer$ConditionObject.await
      250199    0.01%      248  _find_next_bit_[k]
      249260    0.01%      250  void OopOopIterateBackwardsDispatch<G1ScanEvacuatedObjClosure>::Table::oop_oop_iterate_backwards<InstanceKlass, narrowOop>(G1ScanEvacuatedObjClosure*, oopDesc*, Klass*)
      247721    0.01%      247  __vdso_clock_gettime
      247541    0.01%      247  OptoRuntime::new_instance_C(Klass*, JavaThread*)
      245308    0.01%      242  JavaFrameAnchor::make_walkable()
      244519    0.01%      243  check_preempt_curr_[k]
      242974    0.01%      243  G1Policy::predict_region_non_copy_time_ms(HeapRegion*, bool) const
      240653    0.01%      242  G1Analytics::predict_card_scan_time_ms(unsigned long, bool) const
      238993    0.01%      237  HSpaceCounters::update_used(unsigned long)
      238813    0.01%      238  ObjAllocator::initialize(HeapWordImpl**) const
      237433    0.01%      234  timekeeping_advance_[k]
      234532    0.01%      231  AbstractOwnableSynchronizer.setExclusiveOwnerThread
      234074    0.01%      200  __irqentry_text_end_[k]
      231591    0.01%      230  check_spread.isra.0_[k]
      230470    0.01%      229  __update_idle_core_[k]
      229394    0.01%      226  rb_erase_[k]
      223436    0.01%      221  psi_flags_change_[k]
      222494    0.01%      206  __GI___pthread_disable_asynccancel
