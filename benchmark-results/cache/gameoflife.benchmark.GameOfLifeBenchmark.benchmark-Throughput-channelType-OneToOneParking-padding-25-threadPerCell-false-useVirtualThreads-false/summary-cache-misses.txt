--- Execution profile ---
Total samples       : 4530220
unknown_Java        : 33140 (0.73%)
not_walkable_Java   : 196 (0.00%)
deoptimization      : 6 (0.00%)
skipped             : 8 (0.00%)

--- 1692173687 total (37.35%), 1691395 samples
  [ 0] ReduceOps$5ReducingSink.get
  [ 1] ReduceOps$5ReducingSink.get
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 254212181 total (5.61%), 255094 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$59.0x0000000801036b68.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 152368189 total (3.36%), 152565 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$53.0x0000000801036510.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 143015231 total (3.16%), 143284 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 131445357 total (2.90%), 131680 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 124185468 total (2.74%), 124459 samples
  [ 0] Cell$$Lambda$59.0x0000000801036b68.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 117573248 total (2.60%), 117393 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 113163701 total (2.50%), 113196 samples
  [ 0] vtable stub
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 109156889 total (2.41%), 109161 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 99567617 total (2.20%), 99754 samples
  [ 0] OneToOneParkingSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$53.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 82110434 total (1.81%), 82155 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 74518032 total (1.64%), 74603 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 73518802 total (1.62%), 73606 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 72134644 total (1.59%), 72030 samples
  [ 0] Objects.requireNonNull
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 65081036 total (1.44%), 65231 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 64937821 total (1.43%), 64698 samples
  [ 0] ArrayList$ArrayListSpliterator.<init>
  [ 1] ArrayList.spliterator
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 62626195 total (1.38%), 62483 samples
  [ 0] Objects.requireNonNull
  [ 1] ReferencePipeline.map
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 54095339 total (1.19%), 54255 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$59.0x0000000801036b68.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 44905249 total (0.99%), 44819 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 37620140 total (0.83%), 37601 samples
  [ 0] Collection.stream
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 29748607 total (0.66%), 29804 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 27389905 total (0.60%), 27487 samples
  [ 0] OneToOneParkingSingleValue.put
  [ 1] Channel.put
  [ 2] TickPerCell.lambda$tick$0
  [ 3] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 4] ChannelsGrid.lambda$forEachChannel$0
  [ 5] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] TickPerCell.tick
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 26751403 total (0.59%), 26801 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 21344414 total (0.47%), 21303 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 21096541 total (0.47%), 21157 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 20982489 total (0.46%), 21050 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$59.0x0000000801036b68.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 20276814 total (0.45%), 20360 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$58.0x0000000801036d90.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 19807118 total (0.44%), 19866 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] GameOfLife.calculateFrame
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 19595537 total (0.43%), 19650 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 17411093 total (0.38%), 17476 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 17256430 total (0.38%), 17271 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 15570685 total (0.34%), 15575 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 15288599 total (0.34%), 15300 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 14485361 total (0.32%), 14503 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 14321259 total (0.32%), 14328 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 13497955 total (0.30%), 13486 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 12631722 total (0.28%), 12627 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 12018737 total (0.27%), 12019 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 11432342 total (0.25%), 11441 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 11395376 total (0.25%), 11433 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$59.0x0000000801036b68.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 10446270 total (0.23%), 10465 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 10306182 total (0.23%), 10268 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 10176169 total (0.22%), 10172 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 9293681 total (0.21%), 9312 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 9283656 total (0.20%), 9300 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] [unknown_Java]

--- 9189662 total (0.20%), 9218 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 9140272 total (0.20%), 9172 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 8888990 total (0.20%), 8922 samples
  [ 0] Cell$$Lambda$61.0x00000008010371e8.applyAsInt
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 8830230 total (0.19%), 8845 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 8818776 total (0.19%), 8804 samples
  [ 0] IntPipeline$StatelessOp.<init>
  [ 1] ReferencePipeline$4.<init>
  [ 2] ReferencePipeline.mapToInt
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 8309760 total (0.18%), 8344 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 8184001 total (0.18%), 8214 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 8143937 total (0.18%), 8174 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 7986249 total (0.18%), 7985 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 7894319 total (0.17%), 7924 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 7782710 total (0.17%), 7781 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] [unknown_Java]

--- 7347049 total (0.16%), 7330 samples
  [ 0] PipelineHelper.<init>
  [ 1] AbstractPipeline.<init>
  [ 2] ReferencePipeline.<init>
  [ 3] ReferencePipeline$Head.<init>
  [ 4] StreamSupport.stream
  [ 5] Collection.stream
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 7111023 total (0.16%), 7118 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] [unknown_Java]

--- 6893646 total (0.15%), 6909 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 6884159 total (0.15%), 6872 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 6539407 total (0.14%), 6526 samples
  [ 0] ReferencePipeline.map
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 6422053 total (0.14%), 6432 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 6275215 total (0.14%), 6292 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$59.0x0000000801036b68.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 6209977 total (0.14%), 6210 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 5948625 total (0.13%), 5969 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$59.0x0000000801036b68.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 5095639 total (0.11%), 5091 samples
  [ 0] ReferencePipeline.mapToInt
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 5051390 total (0.11%), 5061 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] [unknown_Java]

--- 5000630 total (0.11%), 4912 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] OneToOneParkingSingleValue.take
  [14] Channel.take
  [15] TickPerCell.waitTick
  [16] Cell.notifyLiveness
  [17] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [18] Iterable.forEach
  [19] CellsGroup.run
  [20] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [21] ThreadPoolExecutor.runWorker
  [22] ThreadPoolExecutor$Worker.run
  [23] Thread.run

--- 4999833 total (0.11%), 5006 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 4882187 total (0.11%), 4863 samples
  [ 0] ReferencePipeline.map
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 4783593 total (0.11%), 4792 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 4763488 total (0.11%), 4768 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 4499785 total (0.10%), 4506 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 4440324 total (0.10%), 4448 samples
  [ 0] AbstractPipeline.copyInto
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 4402707 total (0.10%), 4411 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 4194386 total (0.09%), 4204 samples
  [ 0] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 4157250 total (0.09%), 4165 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 4076355 total (0.09%), 4074 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 3934601 total (0.09%), 3938 samples
  [ 0] Objects.requireNonNull
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 3912269 total (0.09%), 3920 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3885572 total (0.09%), 3878 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] G1RemSetScanState::G1ClearCardTableTask::do_work(unsigned int)
  [ 2] G1BatchedTask::work(unsigned int)
  [ 3] WorkerThread::run()
  [ 4] Thread::call_run()
  [ 5] thread_native_entry(Thread*)
  [ 6] start_thread

--- 3862404 total (0.09%), 3870 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3809741 total (0.08%), 3816 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3764216 total (0.08%), 3771 samples
  [ 0] OneToOneParkingSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$53.0x0000000801036510.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 3606768 total (0.08%), 3611 samples
  [ 0] ReferencePipeline$StatelessOp.<init>
  [ 1] ReferencePipeline$3.<init>
  [ 2] ReferencePipeline.map
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 3564700 total (0.08%), 3573 samples
  [ 0] Sink$ChainedReference.end
  [ 1] Sink$ChainedReference.end
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 3212543 total (0.07%), 3217 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 3183080 total (0.07%), 3184 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 2996384 total (0.07%), 3000 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2995221 total (0.07%), 2998 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 2909332 total (0.06%), 2915 samples
  [ 0] Cell$$Lambda$59.0x0000000801036b68.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 2529344 total (0.06%), 2534 samples
  [ 0] ReferencePipeline.mapToInt
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 2426055 total (0.05%), 2393 samples
  [ 0] update_cfs_group_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 2361170 total (0.05%), 2366 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$53.0x0000000801036510.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2307786 total (0.05%), 2274 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] ReduceOps$5ReducingSink.get
  [ 6] ReduceOps$5ReducingSink.get
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 2252349 total (0.05%), 2249 samples
  [ 0] Objects.requireNonNull
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2210559 total (0.05%), 2216 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 2188157 total (0.05%), 2191 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 2142518 total (0.05%), 2144 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 2139312 total (0.05%), 2145 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 2132792 total (0.05%), 2138 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 2093959 total (0.05%), 2099 samples
  [ 0] ArrayList$SubList$1.checkForComodification
  [ 1] ArrayList$SubList$1.next
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 2085411 total (0.05%), 2091 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 2082649 total (0.05%), 2087 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 2040272 total (0.05%), 2020 samples
  [ 0] enqueue_entity_[k]
  [ 1] enqueue_task_fair_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] OneToOneParkingSingleValue.put
  [15] Channel.put
  [16] TickPerCell.lambda$tick$0
  [17] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [18] ChannelsGrid.lambda$forEachChannel$0
  [19] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [20] Dimensions.forEachRowCol
  [21] ChannelsGrid.forEachChannel
  [22] TickPerCell.tick
  [23] GameOfLife.calculateFrame
  [24] GameOfLife.lambda$calculateFrameBlocking$4
  [25] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 2020218 total (0.04%), 2024 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 2009102 total (0.04%), 2013 samples
  [ 0] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 1] [unknown_Java]

--- 1933556 total (0.04%), 1936 samples
  [ 0] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 2] GameOfLife.calculateFrame
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 1891872 total (0.04%), 1895 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 1803977 total (0.04%), 1808 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 1801878 total (0.04%), 1807 samples
  [ 0] OneToOneParkingSingleValue.put
  [ 1] Channel.put
  [ 2] TickPerCell.lambda$tick$0
  [ 3] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 4] ChannelsGrid.lambda$forEachChannel$0
  [ 5] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] TickPerCell.tick
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 1780575 total (0.04%), 1765 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_change_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] OneToOneParkingSingleValue.put
  [15] Channel.put
  [16] TickPerCell.lambda$tick$0
  [17] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [18] ChannelsGrid.lambda$forEachChannel$0
  [19] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [20] Dimensions.forEachRowCol
  [21] ChannelsGrid.forEachChannel
  [22] TickPerCell.tick
  [23] GameOfLife.calculateFrame
  [24] GameOfLife.lambda$calculateFrameBlocking$4
  [25] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [26] ThreadPoolExecutor.runWorker
  [27] ThreadPoolExecutor$Worker.run
  [28] Thread.run

--- 1749810 total (0.04%), 1759 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$58.0x0000000801036d90.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 1664419 total (0.04%), 1639 samples
  [ 0] update_curr_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 1640395 total (0.04%), 1617 samples
  [ 0] __update_load_avg_cfs_rq_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] OneToOneParkingSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 1630525 total (0.04%), 1606 samples
  [ 0] update_load_avg_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 1587692 total (0.04%), 1590 samples
  [ 0] Object.<init>
  [ 1] PipelineHelper.<init>
  [ 2] AbstractPipeline.<init>
  [ 3] ReferencePipeline.<init>
  [ 4] ReferencePipeline$Head.<init>
  [ 5] StreamSupport.stream
  [ 6] Collection.stream
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 1496502 total (0.03%), 1471 samples
  [ 0] __schedule_[k]
  [ 1] schedule_[k]
  [ 2] futex_wait_queue_[k]
  [ 3] futex_wait_[k]
  [ 4] do_futex_[k]
  [ 5] __x64_sys_futex_[k]
  [ 6] do_syscall_64_[k]
  [ 7] entry_SYSCALL_64_after_hwframe_[k]
  [ 8] __futex_abstimed_wait_common
  [ 9] Unsafe.park
  [10] LockSupport.park
  [11] OneToOneParkingSingleValue.take
  [12] Channel.take
  [13] TickPerCell.waitTick
  [14] Cell.notifyLiveness
  [15] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [19] ThreadPoolExecutor.runWorker
  [20] ThreadPoolExecutor$Worker.run
  [21] Thread.run

--- 1489628 total (0.03%), 1494 samples
  [ 0] MemAllocator::allocate() const
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] GameOfLife.calculateFrame
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 1469728 total (0.03%), 1465 samples
  [ 0] G1CardSet::occupied() const
  [ 1] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 2] G1RemSetSamplingTask::execute()
  [ 3] G1ServiceThread::run_task(G1ServiceTask*)
  [ 4] G1ServiceThread::run_service()
  [ 5] ConcurrentGCThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 1447236 total (0.03%), 1451 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 1376451 total (0.03%), 1376 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 1353264 total (0.03%), 1344 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] OneToOneParkingSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 1343641 total (0.03%), 1339 samples
  [ 0] Unsafe.park
  [ 1] LockSupport.park
  [ 2] OneToOneParkingSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 1311912 total (0.03%), 1293 samples
  [ 0] __update_load_avg_se_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] OneToOneParkingSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 1303504 total (0.03%), 1291 samples
  [ 0] enqueue_task_fair_[k]
  [ 1] enqueue_task_[k]
  [ 2] ttwu_do_activate_[k]
  [ 3] try_to_wake_up_[k]
  [ 4] wake_up_q_[k]
  [ 5] futex_wake_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] ___pthread_cond_signal
  [11] Unsafe.unpark
  [12] LockSupport.unpark
  [13] OneToOneParkingSingleValue.put
  [14] Channel.put
  [15] TickPerCell.lambda$tick$0
  [16] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [17] ChannelsGrid.lambda$forEachChannel$0
  [18] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [19] Dimensions.forEachRowCol
  [20] ChannelsGrid.forEachChannel
  [21] TickPerCell.tick
  [22] GameOfLife.calculateFrame
  [23] GameOfLife.lambda$calculateFrameBlocking$4
  [24] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [25] ThreadPoolExecutor.runWorker
  [26] ThreadPoolExecutor$Worker.run
  [27] Thread.run

--- 1293681 total (0.03%), 1296 samples
  [ 0] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 2] GameOfLife.calculateFrame
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 1281461 total (0.03%), 1282 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 1205498 total (0.03%), 1190 samples
  [ 0] reweight_entity_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 1147232 total (0.03%), 1150 samples
  [ 0] FreeListAllocator::reset()
  [ 1] HeapRegionRemSet::clear_locked(bool)
  [ 2] HeapRegion::hr_clear(bool)
  [ 3] G1CollectedHeap::free_region(HeapRegion*, FreeRegionList*)
  [ 4] FreeCSetClosure::do_heap_region(HeapRegion*)
  [ 5] G1CollectedHeap::par_iterate_regions_array(HeapRegionClosure*, HeapRegionClaimer*, unsigned int const*, unsigned long, unsigned int) const
  [ 6] G1PostEvacuateCollectionSetCleanupTask2::FreeCollectionSetTask::do_work(unsigned int)
  [ 7] G1BatchedTask::work(unsigned int)
  [ 8] WorkerThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 1026102 total (0.02%), 1020 samples
  [ 0] Parker::park(bool, long)
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] OneToOneParkingSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 1025720 total (0.02%), 1030 samples
  [ 0] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 1022879 total (0.02%), 1009 samples
  [ 0] dequeue_task_fair_[k]
  [ 1] __schedule_[k]
  [ 2] schedule_[k]
  [ 3] futex_wait_queue_[k]
  [ 4] futex_wait_[k]
  [ 5] do_futex_[k]
  [ 6] __x64_sys_futex_[k]
  [ 7] do_syscall_64_[k]
  [ 8] entry_SYSCALL_64_after_hwframe_[k]
  [ 9] __futex_abstimed_wait_common
  [10] Unsafe.park
  [11] LockSupport.park
  [12] OneToOneParkingSingleValue.take
  [13] Channel.take
  [14] TickPerCell.waitTick
  [15] Cell.notifyLiveness
  [16] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 999777 total (0.02%), 991 samples
  [ 0] update_load_avg_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] OneToOneParkingSingleValue.put
  [16] Channel.put
  [17] TickPerCell.lambda$tick$0
  [18] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [19] ChannelsGrid.lambda$forEachChannel$0
  [20] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [21] Dimensions.forEachRowCol
  [22] ChannelsGrid.forEachChannel
  [23] TickPerCell.tick
  [24] GameOfLife.calculateFrame
  [25] GameOfLife.lambda$calculateFrameBlocking$4
  [26] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 986675 total (0.02%), 989 samples
  [ 0] Sink$ChainedReference.end
  [ 1] Sink$ChainedReference.end
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 963881 total (0.02%), 954 samples
  [ 0] reweight_entity_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] OneToOneParkingSingleValue.put
  [16] Channel.put
  [17] TickPerCell.lambda$tick$0
  [18] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [19] ChannelsGrid.lambda$forEachChannel$0
  [20] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [21] Dimensions.forEachRowCol
  [22] ChannelsGrid.forEachChannel
  [23] TickPerCell.tick
  [24] GameOfLife.calculateFrame
  [25] GameOfLife.lambda$calculateFrameBlocking$4
  [26] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 952837 total (0.02%), 927 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] __futex_abstimed_wait_common
  [ 4] Unsafe.park
  [ 5] LockSupport.park
  [ 6] OneToOneParkingSingleValue.take
  [ 7] Channel.take
  [ 8] TickPerCell.waitTick
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [14] ThreadPoolExecutor.runWorker
  [15] ThreadPoolExecutor$Worker.run
  [16] Thread.run

--- 941421 total (0.02%), 945 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 932639 total (0.02%), 936 samples
  [ 0] Channel.put
  [ 1] Cell.lambda$notifyLiveness$0
  [ 2] Cell$$Lambda$53.0x0000000801036510.accept
  [ 3] ArrayList.forEach
  [ 4] Cell.notifyLiveness
  [ 5] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 6] Iterable.forEach
  [ 7] CellsGroup.run
  [ 8] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 9] ThreadPoolExecutor.runWorker
  [10] ThreadPoolExecutor$Worker.run
  [11] Thread.run

--- 928019 total (0.02%), 929 samples
  [ 0] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 916688 total (0.02%), 919 samples
  [ 0] ObjArrayAllocator::initialize(HeapWordImpl**) const
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 905302 total (0.02%), 908 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 880692 total (0.02%), 884 samples
  [ 0] Channel.put
  [ 1] TickPerCell.lambda$tick$0
  [ 2] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [ 3] ChannelsGrid.lambda$forEachChannel$0
  [ 4] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] TickPerCell.tick
  [ 8] GameOfLife.calculateFrame
  [ 9] GameOfLife.lambda$calculateFrameBlocking$4
  [10] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 877651 total (0.02%), 881 samples
  [ 0] ThreadsListHandle::ThreadsListHandle(Thread*)
  [ 1] Unsafe_Unpark
  [ 2] Unsafe.unpark
  [ 3] LockSupport.unpark
  [ 4] OneToOneParkingSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 855664 total (0.02%), 845 samples
  [ 0] cpuacct_charge_[k]
  [ 1] update_curr_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] OneToOneParkingSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 847286 total (0.02%), 848 samples
  [ 0] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 839520 total (0.02%), 832 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] ___pthread_cond_signal
  [ 4] Unsafe.unpark
  [ 5] LockSupport.unpark
  [ 6] OneToOneParkingSingleValue.put
  [ 7] Channel.put
  [ 8] TickPerCell.lambda$tick$0
  [ 9] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [10] ChannelsGrid.lambda$forEachChannel$0
  [11] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [12] Dimensions.forEachRowCol
  [13] ChannelsGrid.forEachChannel
  [14] TickPerCell.tick
  [15] GameOfLife.calculateFrame
  [16] GameOfLife.lambda$calculateFrameBlocking$4
  [17] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 820143 total (0.02%), 827 samples
  [ 0] Unsafe_Park
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] OneToOneParkingSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$58.0x0000000801036d90.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 816078 total (0.02%), 819 samples
  [ 0] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 2] ThreadPoolExecutor.runWorker
  [ 3] ThreadPoolExecutor$Worker.run
  [ 4] Thread.run

--- 801575 total (0.02%), 804 samples
  [ 0] Sink$ChainedReference.end
  [ 1] [unknown_Java]

--- 792489 total (0.02%), 782 samples
  [ 0] dequeue_entity_[k]
  [ 1] dequeue_task_fair_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] OneToOneParkingSingleValue.take
  [14] Channel.take
  [15] TickPerCell.waitTick
  [16] Cell.notifyLiveness
  [17] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [18] Iterable.forEach
  [19] CellsGroup.run
  [20] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [21] ThreadPoolExecutor.runWorker
  [22] ThreadPoolExecutor$Worker.run
  [23] Thread.run

--- 787223 total (0.02%), 787 samples
  [ 0] ___pthread_cond_wait
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] OneToOneParkingSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 784064 total (0.02%), 786 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 779749 total (0.02%), 781 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 776415 total (0.02%), 779 samples
  [ 0] Sink$ChainedReference.end
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 772343 total (0.02%), 774 samples
  [ 0] Unsafe_Unpark
  [ 1] Unsafe.unpark
  [ 2] LockSupport.unpark
  [ 3] OneToOneParkingSingleValue.put
  [ 4] Channel.put
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 769831 total (0.02%), 771 samples
  [ 0] __GI___pthread_mutex_lock
  [ 1] Unsafe_Unpark
  [ 2] Unsafe.unpark
  [ 3] LockSupport.unpark
  [ 4] OneToOneParkingSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 765354 total (0.02%), 754 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] ReferencePipeline$3.opWrapSink
  [ 6] AbstractPipeline.wrapSink
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 755274 total (0.02%), 757 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$59.0x0000000801036b68.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 746406 total (0.02%), 744 samples
  [ 0] MemAllocator::Allocation::notify_allocation_jvmti_sampler()
  [ 1] MemAllocator::allocate() const
  [ 2] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 4] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 743106 total (0.02%), 738 samples
  [ 0] __update_load_avg_se_[k]
  [ 1] update_load_avg_[k]
  [ 2] enqueue_entity_[k]
  [ 3] enqueue_task_fair_[k]
  [ 4] enqueue_task_[k]
  [ 5] ttwu_do_activate_[k]
  [ 6] try_to_wake_up_[k]
  [ 7] wake_up_q_[k]
  [ 8] futex_wake_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] ___pthread_cond_signal
  [14] Unsafe.unpark
  [15] LockSupport.unpark
  [16] OneToOneParkingSingleValue.put
  [17] Channel.put
  [18] TickPerCell.lambda$tick$0
  [19] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [20] ChannelsGrid.lambda$forEachChannel$0
  [21] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [22] Dimensions.forEachRowCol
  [23] ChannelsGrid.forEachChannel
  [24] TickPerCell.tick
  [25] GameOfLife.calculateFrame
  [26] GameOfLife.lambda$calculateFrameBlocking$4
  [27] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [28] ThreadPoolExecutor.runWorker
  [29] ThreadPoolExecutor$Worker.run
  [30] Thread.run

--- 739958 total (0.02%), 729 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] AbstractPipeline.<init>
  [ 6] ReferencePipeline.<init>
  [ 7] ReferencePipeline$Head.<init>
  [ 8] StreamSupport.stream
  [ 9] Collection.stream
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 736629 total (0.02%), 725 samples
  [ 0] update_irq_load_avg_[k]
  [ 1] update_rq_clock_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] OneToOneParkingSingleValue.take
  [14] Channel.take
  [15] TickPerCell.waitTick
  [16] Cell.notifyLiveness
  [17] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [18] Iterable.forEach
  [19] CellsGroup.run
  [20] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [21] ThreadPoolExecutor.runWorker
  [22] ThreadPoolExecutor$Worker.run
  [23] Thread.run

--- 731571 total (0.02%), 726 samples
  [ 0] __update_load_avg_cfs_rq_[k]
  [ 1] update_load_avg_[k]
  [ 2] enqueue_entity_[k]
  [ 3] enqueue_task_fair_[k]
  [ 4] enqueue_task_[k]
  [ 5] ttwu_do_activate_[k]
  [ 6] try_to_wake_up_[k]
  [ 7] wake_up_q_[k]
  [ 8] futex_wake_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] ___pthread_cond_signal
  [14] Unsafe.unpark
  [15] LockSupport.unpark
  [16] OneToOneParkingSingleValue.put
  [17] Channel.put
  [18] TickPerCell.lambda$tick$0
  [19] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [20] ChannelsGrid.lambda$forEachChannel$0
  [21] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [22] Dimensions.forEachRowCol
  [23] ChannelsGrid.forEachChannel
  [24] TickPerCell.tick
  [25] GameOfLife.calculateFrame
  [26] GameOfLife.lambda$calculateFrameBlocking$4
  [27] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [28] ThreadPoolExecutor.runWorker
  [29] ThreadPoolExecutor$Worker.run
  [30] Thread.run

--- 724073 total (0.02%), 726 samples
  [ 0] MemAllocator::allocate() const
  [ 1] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 2] ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 3] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 719176 total (0.02%), 714 samples
  [ 0] __condvar_dec_grefs
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] OneToOneParkingSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$58.0x0000000801036d90.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 707404 total (0.02%), 696 samples
  [ 0] update_rq_clock_[k]
  [ 1] __schedule_[k]
  [ 2] schedule_[k]
  [ 3] futex_wait_queue_[k]
  [ 4] futex_wait_[k]
  [ 5] do_futex_[k]
  [ 6] __x64_sys_futex_[k]
  [ 7] do_syscall_64_[k]
  [ 8] entry_SYSCALL_64_after_hwframe_[k]
  [ 9] __futex_abstimed_wait_common
  [10] Unsafe.park
  [11] LockSupport.park
  [12] OneToOneParkingSingleValue.take
  [13] Channel.take
  [14] TickPerCell.waitTick
  [15] Cell.notifyLiveness
  [16] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [20] ThreadPoolExecutor.runWorker
  [21] ThreadPoolExecutor$Worker.run
  [22] Thread.run

--- 705243 total (0.02%), 696 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] OneToOneParkingSingleValue.take
  [14] Channel.take
  [15] GameOfLife$$Lambda$58.0x0000000801036d90.test
  [16] ChannelsGrid.lambda$forEachChannel$1
  [17] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [18] Dimensions.forEachRowCol
  [19] ChannelsGrid.forEachChannel
  [20] GameOfLife.calculateFrame
  [21] GameOfLife.lambda$calculateFrameBlocking$4
  [22] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 694445 total (0.02%), 683 samples
  [ 0] iterate_groups_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] OneToOneParkingSingleValue.take
  [14] Channel.take
  [15] TickPerCell.waitTick
  [16] Cell.notifyLiveness
  [17] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [18] Iterable.forEach
  [19] CellsGroup.run
  [20] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [21] ThreadPoolExecutor.runWorker
  [22] ThreadPoolExecutor$Worker.run
  [23] Thread.run

--- 694176 total (0.02%), 698 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] ChannelsGrid.lambda$forEachChannel$1
  [ 2] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [ 3] Dimensions.forEachRowCol
  [ 4] ChannelsGrid.forEachChannel
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 8] ThreadPoolExecutor.runWorker
  [ 9] ThreadPoolExecutor$Worker.run
  [10] Thread.run

--- 687048 total (0.02%), 688 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] GameOfLife.calculateFrame
  [ 3] GameOfLife.lambda$calculateFrameBlocking$4
  [ 4] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 682875 total (0.02%), 686 samples
  [ 0] ChannelsGrid.lambda$forEachChannel$1
  [ 1] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [ 2] Dimensions.forEachRowCol
  [ 3] ChannelsGrid.forEachChannel
  [ 4] GameOfLife.calculateFrame
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 674133 total (0.01%), 666 samples
  [ 0] __entry_text_start_[k]
  [ 1] __lll_lock_wake
  [ 2] Unsafe_Park
  [ 3] Unsafe.park
  [ 4] LockSupport.park
  [ 5] OneToOneParkingSingleValue.take
  [ 6] Channel.take
  [ 7] GameOfLife$$Lambda$58.0x0000000801036d90.test
  [ 8] ChannelsGrid.lambda$forEachChannel$1
  [ 9] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] GameOfLife.calculateFrame
  [13] GameOfLife.lambda$calculateFrameBlocking$4
  [14] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 671060 total (0.01%), 674 samples
  [ 0] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [ 1] Dimensions.forEachRowCol
  [ 2] ChannelsGrid.forEachChannel
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [ 6] ThreadPoolExecutor.runWorker
  [ 7] ThreadPoolExecutor$Worker.run
  [ 8] Thread.run

--- 669844 total (0.01%), 653 samples
  [ 0] GameOfLife.calculateFrameBlocking
  [ 1] GameOfLifeBenchmark.benchmark
  [ 2] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
  [ 3] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_Throughput
  [ 4] DirectMethodHandle$Holder.invokeSpecial
  [ 5] LambdaForm$MH.0x000000080102e000.invoke
  [ 6] LambdaForm$MH.0x000000080102e400.invokeExact_MT
  [ 7] DirectMethodHandleAccessor.invokeImpl
  [ 8] DirectMethodHandleAccessor.invoke
  [ 9] Method.invoke
  [10] BenchmarkHandler$BenchmarkTask.call
  [11] BenchmarkHandler$BenchmarkTask.call
  [12] FutureTask.run
  [13] Executors$RunnableAdapter.call
  [14] FutureTask.run
  [15] ThreadPoolExecutor.runWorker
  [16] ThreadPoolExecutor$Worker.run
  [17] Thread.run

--- 668069 total (0.01%), 669 samples
  [ 0] OneToOneParkingSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$58.0x0000000801036d90.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [10] ThreadPoolExecutor.runWorker
  [11] ThreadPoolExecutor$Worker.run
  [12] Thread.run

--- 653905 total (0.01%), 648 samples
  [ 0] update_cfs_group_[k]
  [ 1] enqueue_entity_[k]
  [ 2] enqueue_task_fair_[k]
  [ 3] enqueue_task_[k]
  [ 4] ttwu_do_activate_[k]
  [ 5] try_to_wake_up_[k]
  [ 6] wake_up_q_[k]
  [ 7] futex_wake_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] ___pthread_cond_signal
  [13] Unsafe.unpark
  [14] LockSupport.unpark
  [15] OneToOneParkingSingleValue.put
  [16] Channel.put
  [17] TickPerCell.lambda$tick$0
  [18] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [19] ChannelsGrid.lambda$forEachChannel$0
  [20] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [21] Dimensions.forEachRowCol
  [22] ChannelsGrid.forEachChannel
  [23] TickPerCell.tick
  [24] GameOfLife.calculateFrame
  [25] GameOfLife.lambda$calculateFrameBlocking$4
  [26] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [27] ThreadPoolExecutor.runWorker
  [28] ThreadPoolExecutor$Worker.run
  [29] Thread.run

--- 645443 total (0.01%), 637 samples
  [ 0] syscall_exit_to_user_mode_[k]
  [ 1] do_syscall_64_[k]
  [ 2] entry_SYSCALL_64_after_hwframe_[k]
  [ 3] __futex_abstimed_wait_common
  [ 4] Unsafe.park
  [ 5] LockSupport.park
  [ 6] OneToOneParkingSingleValue.take
  [ 7] Channel.take
  [ 8] GameOfLife$$Lambda$58.0x0000000801036d90.test
  [ 9] ChannelsGrid.lambda$forEachChannel$1
  [10] ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
  [11] Dimensions.forEachRowCol
  [12] ChannelsGrid.forEachChannel
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 640756 total (0.01%), 630 samples
  [ 0] update_sd_lb_stats.constprop.0_[k]
  [ 1] find_busiest_group_[k]
  [ 2] load_balance_[k]
  [ 3] newidle_balance_[k]
  [ 4] pick_next_task_fair_[k]
  [ 5] __schedule_[k]
  [ 6] schedule_[k]
  [ 7] futex_wait_queue_[k]
  [ 8] futex_wait_[k]
  [ 9] do_futex_[k]
  [10] __x64_sys_futex_[k]
  [11] do_syscall_64_[k]
  [12] entry_SYSCALL_64_after_hwframe_[k]
  [13] __futex_abstimed_wait_common
  [14] Unsafe.park
  [15] LockSupport.park
  [16] OneToOneParkingSingleValue.take
  [17] Channel.take
  [18] TickPerCell.waitTick
  [19] Cell.notifyLiveness
  [20] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [21] Iterable.forEach
  [22] CellsGroup.run
  [23] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [24] ThreadPoolExecutor.runWorker
  [25] ThreadPoolExecutor$Worker.run
  [26] Thread.run

--- 621169 total (0.01%), 622 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 615117 total (0.01%), 604 samples
  [ 0] futex_q_lock_[k]
  [ 1] futex_wait_setup_[k]
  [ 2] futex_wait_[k]
  [ 3] do_futex_[k]
  [ 4] __x64_sys_futex_[k]
  [ 5] do_syscall_64_[k]
  [ 6] entry_SYSCALL_64_after_hwframe_[k]
  [ 7] __futex_abstimed_wait_common
  [ 8] Unsafe.park
  [ 9] LockSupport.park
  [10] OneToOneParkingSingleValue.take
  [11] Channel.take
  [12] TickPerCell.waitTick
  [13] Cell.notifyLiveness
  [14] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 607957 total (0.01%), 598 samples
  [ 0] __calc_delta_[k]
  [ 1] update_curr_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] OneToOneParkingSingleValue.take
  [16] Channel.take
  [17] TickPerCell.waitTick
  [18] Cell.notifyLiveness
  [19] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [20] Iterable.forEach
  [21] CellsGroup.run
  [22] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [23] ThreadPoolExecutor.runWorker
  [24] ThreadPoolExecutor$Worker.run
  [25] Thread.run

--- 596070 total (0.01%), 598 samples
  [ 0] TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
  [ 1] OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
  [ 2] ThreadPoolExecutor.runWorker
  [ 3] ThreadPoolExecutor$Worker.run
  [ 4] Thread.run

--- 587100 total (0.01%), 577 samples
  [ 0] __entry_text_start_[k]
  [ 1] ___pthread_cond_signal
  [ 2] Unsafe.unpark
  [ 3] LockSupport.unpark
  [ 4] OneToOneParkingSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 583457 total (0.01%), 585 samples
  [ 0] Channel.take
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 7] ThreadPoolExecutor.runWorker
  [ 8] ThreadPoolExecutor$Worker.run
  [ 9] Thread.run

--- 582459 total (0.01%), 581 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 578533 total (0.01%), 570 samples
  [ 0] __memset_avx2_unaligned_erms
  [ 1] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 2] MemAllocator::allocate() const
  [ 3] InstanceKlass::allocate_instance(JavaThread*)
  [ 4] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 5] ReferencePipeline$4.opWrapSink
  [ 6] AbstractPipeline.wrapSink
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [17] ThreadPoolExecutor.runWorker
  [18] ThreadPoolExecutor$Worker.run
  [19] Thread.run

--- 577993 total (0.01%), 577 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 4] ThreadPoolExecutor.runWorker
  [ 5] ThreadPoolExecutor$Worker.run
  [ 6] Thread.run

--- 560751 total (0.01%), 548 samples
  [ 0] update_blocked_averages_[k]
  [ 1] newidle_balance_[k]
  [ 2] pick_next_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 553462 total (0.01%), 551 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 3] ThreadPoolExecutor.runWorker
  [ 4] ThreadPoolExecutor$Worker.run
  [ 5] Thread.run

--- 549094 total (0.01%), 540 samples
  [ 0] __entry_text_start_[k]
  [ 1] Unsafe.park
  [ 2] LockSupport.park
  [ 3] OneToOneParkingSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [11] ThreadPoolExecutor.runWorker
  [12] ThreadPoolExecutor$Worker.run
  [13] Thread.run

--- 547702 total (0.01%), 543 samples
  [ 0] G1CardSet::clear()
  [ 1] HeapRegionRemSet::clear_locked(bool)
  [ 2] HeapRegion::hr_clear(bool)
  [ 3] G1CollectedHeap::free_region(HeapRegion*, FreeRegionList*)
  [ 4] FreeCSetClosure::do_heap_region(HeapRegion*)
  [ 5] G1CollectedHeap::par_iterate_regions_array(HeapRegionClosure*, HeapRegionClaimer*, unsigned int const*, unsigned long, unsigned int) const
  [ 6] G1PostEvacuateCollectionSetCleanupTask2::FreeCollectionSetTask::do_work(unsigned int)
  [ 7] G1BatchedTask::work(unsigned int)
  [ 8] WorkerThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

--- 544549 total (0.01%), 539 samples
  [ 0] psi_task_change_[k]
  [ 1] enqueue_task_[k]
  [ 2] ttwu_do_activate_[k]
  [ 3] try_to_wake_up_[k]
  [ 4] wake_up_q_[k]
  [ 5] futex_wake_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] ___pthread_cond_signal
  [11] Unsafe.unpark
  [12] LockSupport.unpark
  [13] OneToOneParkingSingleValue.put
  [14] Channel.put
  [15] TickPerCell.lambda$tick$0
  [16] TickPerCell$$Lambda$51.0x00000008010360d8.accept
  [17] ChannelsGrid.lambda$forEachChannel$0
  [18] ChannelsGrid$$Lambda$52.0x00000008010362f0.accept
  [19] Dimensions.forEachRowCol
  [20] ChannelsGrid.forEachChannel
  [21] TickPerCell.tick
  [22] GameOfLife.calculateFrame
  [23] GameOfLife.lambda$calculateFrameBlocking$4
  [24] GameOfLife$$Lambda$50.0x0000000801035ec8.run
  [25] ThreadPoolExecutor.runWorker
  [26] ThreadPoolExecutor$Worker.run
  [27] Thread.run

--- 544121 total (0.01%), 535 samples
  [ 0] native_sched_clock_[k]
  [ 1] sched_clock_cpu_[k]
  [ 2] update_rq_clock_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] OneToOneParkingSingleValue.take
  [15] Channel.take
  [16] TickPerCell.waitTick
  [17] Cell.notifyLiveness
  [18] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [19] Iterable.forEach
  [20] CellsGroup.run
  [21] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [22] ThreadPoolExecutor.runWorker
  [23] ThreadPoolExecutor$Worker.run
  [24] Thread.run

--- 540191 total (0.01%), 541 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [ 5] ThreadPoolExecutor.runWorker
  [ 6] ThreadPoolExecutor$Worker.run
  [ 7] Thread.run

--- 537787 total (0.01%), 533 samples
  [ 0] G1Policy::preventive_collection_required(unsigned int)
  [ 1] G1CollectedHeap::attempt_allocation_slow(unsigned long)
  [ 2] G1CollectedHeap::allocate_new_tlab(unsigned long, unsigned long, unsigned long*)
  [ 3] MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
  [ 4] MemAllocator::allocate() const
  [ 5] InstanceKlass::allocate_instance(JavaThread*)
  [ 6] OptoRuntime::new_instance_C(Klass*, JavaThread*)
  [ 7] ReduceOps$5ReducingSink.get
  [ 8] ReduceOps$5ReducingSink.get
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$55.0x0000000801036950.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [18] ThreadPoolExecutor.runWorker
  [19] ThreadPoolExecutor$Worker.run
  [20] Thread.run

--- 529199 total (0.01%), 510 samples
  [ 0] fpregs_restore_userregs_[k]
  [ 1] exit_to_user_mode_prepare_[k]
  [ 2] syscall_exit_to_user_mode_[k]
  [ 3] do_syscall_64_[k]
  [ 4] entry_SYSCALL_64_after_hwframe_[k]
  [ 5] __futex_abstimed_wait_common
  [ 6] Unsafe.park
  [ 7] LockSupport.park
  [ 8] OneToOneParkingSingleValue.take
  [ 9] Channel.take
  [10] TickPerCell.waitTick
  [11] Cell.notifyLiveness
  [12] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [16] ThreadPoolExecutor.runWorker
  [17] ThreadPoolExecutor$Worker.run
  [18] Thread.run

--- 516218 total (0.01%), 514 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)3, 286822ul>::oop_access_barrier(oopDesc*, long)
  [ 1] Unsafe_Park
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] OneToOneParkingSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 515676 total (0.01%), 502 samples
  [ 0] __entry_text_start_[k]
  [ 1] __futex_abstimed_wait_common
  [ 2] Unsafe.park
  [ 3] LockSupport.park
  [ 4] OneToOneParkingSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$47.0x0000000801035690.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] ThreadPerCoreGameOfLife$$Lambda$45.0x0000000801035268.run
  [12] ThreadPoolExecutor.runWorker
  [13] ThreadPoolExecutor$Worker.run
  [14] Thread.run

--- 514851 total (0.01%), 517 samples
  [ 0] G1Analytics::predict_scan_card_num(unsigned long, bool) const
  [ 1] G1Policy::predict_region_non_copy_time_ms(HeapRegion*, bool) const
  [ 2] G1CollectionSet::update_young_region_prediction(HeapRegion*, unsigned long)
  [ 3] G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
  [ 4] G1CollectionSet::iterate(HeapRegionClosure*) const
  [ 5] G1RemSetSamplingTask::execute()
  [ 6] G1ServiceThread::run_task(G1ServiceTask*)
  [ 7] G1ServiceThread::run_service()
  [ 8] ConcurrentGCThread::run()
  [ 9] Thread::call_run()
  [10] thread_native_entry(Thread*)
  [11] start_thread

       total  percent  samples  top
  ----------  -------  -------  ---
  1692194589   37.35%  1691416  ReduceOps$5ReducingSink.get
   387598533    8.56%   388876  OneToOneParkingSingleValue.take
   354366333    7.82%   354427  StreamSupport.stream
   298184984    6.58%   298693  Sink$ChainedReference.<init>
   154730392    3.42%   154932  Cell.lambda$notifyLiveness$0
   140951755    3.11%   140704  Objects.requireNonNull
   135443734    2.99%   135462  ReferencePipeline$4.opWrapSink
   133187932    2.94%   133481  OneToOneParkingSingleValue.put
   133099710    2.94%   133009  AbstractPipeline.<init>
   127094800    2.81%   127374  Cell$$Lambda$59.0x0000000801036b68.apply
   113164698    2.50%   113197  vtable stub
    96564938    2.13%    96674  ReferencePipeline$3.opWrapSink
    94032636    2.08%    94222  ArrayList$ArrayListSpliterator.forEachRemaining
    64937821    1.43%    64698  ArrayList$ArrayListSpliterator.<init>
    56826569    1.25%    56931  Sink$ChainedReference.begin
    42084979    0.93%    42207  Dimensions.forEachRowCol
    39202115    0.87%    39326  ReferencePipeline$3$1.accept
    37620140    0.83%    37601  Collection.stream
    26780650    0.59%    26874  ReferencePipeline$4$1.accept
    20592514    0.45%    20552  Cell.calculateNextState
    17018570    0.38%    17025  AbstractPipeline.wrapSink
    16616470    0.37%    16680  ReduceOps$5ReducingSink.accept
    13032036    0.29%    13041  Iterable.forEach
    11421594    0.25%    11389  ReferencePipeline.map
     9848453    0.22%     9860  ArrayList$SubList$1.next
     9814546    0.22%     9685  psi_group_change_[k]
     9335201    0.21%     9260  __memset_avx2_unaligned_erms
     8888990    0.20%     8922  Cell$$Lambda$61.0x00000008010371e8.applyAsInt
     8818776    0.19%     8804  IntPipeline$StatelessOp.<init>
     7624983    0.17%     7625  ReferencePipeline.mapToInt
     7595238    0.17%     7621  Channel.take
     7347049    0.16%     7330  PipelineHelper.<init>
     6203488    0.14%     6217  CellsGroup$$Lambda$55.0x0000000801036950.accept
     6154496    0.14%     6170  Sink$ChainedReference.end
     5685241    0.13%     5605  __update_load_avg_cfs_rq_[k]
     5506708    0.12%     5441  update_cfs_group_[k]
     5339759    0.12%     5346  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
     4752879    0.10%     4692  update_load_avg_[k]
     4440324    0.10%     4448  AbstractPipeline.copyInto
     4259463    0.09%     4196  syscall_exit_to_user_mode_[k]
     3914675    0.09%     3852  __entry_text_start_[k]
     3894614    0.09%     3843  update_curr_[k]
     3813762    0.08%     3768  __update_load_avg_se_[k]
     3780625    0.08%     3712  update_blocked_averages_[k]
     3606768    0.08%     3611  ReferencePipeline$StatelessOp.<init>
     3394947    0.07%     3357  reweight_entity_[k]
     3366876    0.07%     3372  StreamOpFlag.fromCharacteristics
     3261686    0.07%     3254  Unsafe_Park
     3150021    0.07%     3154  TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
     3117595    0.07%     3087  enqueue_entity_[k]
     2856002    0.06%     2861  ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
     2821206    0.06%     2827  MemAllocator::allocate() const
     2558156    0.06%     2558  Unsafe.park
     2506512    0.06%     2506  ArrayList$SubList$1.checkForComodification
     2367516    0.05%     2365  ___pthread_cond_wait
     2300418    0.05%     2306  CellsGroup$$Lambda$47.0x0000000801035690.accept
     2267204    0.05%     2271  Object.<init>
     2204869    0.05%     2173  __schedule_[k]
     2189155    0.05%     2197  Channel.put
     2179091    0.05%     2157  enqueue_task_fair_[k]
     2171440    0.05%     2173  Cell.notifyLiveness
     2074201    0.05%     2063  G1CardSet::occupied() const
     1860548    0.04%     1848  Parker::park(bool, long)
     1828561    0.04%     1825  ThreadPoolExecutor.runWorker
     1741822    0.04%     1711  check_preemption_disabled_[k]
     1740581    0.04%     1715  _raw_spin_lock_[k]
     1730993    0.04%     1735  ObjArrayAllocator::initialize(HeapWordImpl**) const
     1644566    0.04%     1612  rcu_sched_clock_irq_[k]
     1639091    0.04%     1642  __tls_get_addr
     1514315    0.03%     1495  __calc_delta_[k]
     1498350    0.03%     1480  update_rq_clock_[k]
     1494804    0.03%     1494  __GI___pthread_mutex_lock
     1483002    0.03%     1465  dequeue_task_fair_[k]
     1475206    0.03%     1480  Unsafe_Unpark
     1474699    0.03%     1460  __condvar_dec_grefs
     1433920    0.03%     1439  ThreadsListHandle::ThreadsListHandle(Thread*)
     1357081    0.03%     1338  native_sched_clock_[k]
     1355849    0.03%     1309  syscall_return_via_sysret_[k]
     1352940    0.03%     1348  MemAllocator::Allocation::notify_allocation_jvmti_sampler()
     1237043    0.03%     1223  cpuacct_charge_[k]
     1230249    0.03%     1217  dequeue_entity_[k]
     1223155    0.03%     1169  fpregs_restore_userregs_[k]
     1177621    0.03%     1151  futex_wake_[k]
     1147232    0.03%     1150  FreeListAllocator::reset()
     1142696    0.03%     1152  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)3, 286822ul>::oop_access_barrier(oopDesc*, long)
     1119041    0.02%     1105  update_irq_load_avg_[k]
     1068301    0.02%     1059  select_task_rq_fair_[k]
     1067279    0.02%     1056  psi_task_change_[k]
     1061963    0.02%     1060  __pthread_mutex_unlock_usercnt
     1029959    0.02%     1015  futex_q_lock_[k]
     1017829    0.02%     1002  iterate_groups_[k]
     1014477    0.02%      994  rb_next_[k]
      994028    0.02%      990  G1Analytics::predict_scan_card_num(unsigned long, bool) const
      980071    0.02%      984  ___pthread_cond_signal
      951031    0.02%      956  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<548964ul, G1BarrierSet>, (AccessInternal::BarrierType)2, 548964ul>::oop_access_barrier(void*)
      925950    0.02%      913  GameOfLife.calculateFrameBlocking
      889366    0.02%      882  G1Policy::preventive_collection_required(unsigned int)
      865828    0.02%      806  __get_user_8_[k]
      861217    0.02%      832  restore_fpregs_from_fpstate_[k]
      849832    0.02%      837  update_sd_lb_stats.constprop.0_[k]
      828434    0.02%      816  update_min_vruntime_[k]
      816660    0.02%      816  G1FromCardCache::clear(unsigned int)
      811094    0.02%      819  pthread_mutex_trylock@@GLIBC_2.34
      778141    0.02%      771  JavaThread::threadObj() const
      758785    0.02%      748  preempt_count_add_[k]
      757511    0.02%      766  java_lang_Thread::set_thread_status(oopDesc*, JavaThreadStatus)
      751068    0.02%      755  ChannelsGrid.getChannel
      744531    0.02%      728  __hrtimer_run_queues_[k]
      735464    0.02%      737  AbsSeq::dsd() const
      724499    0.02%      725  ThreadsListHandle::cv_internal_thread_to_JavaThread(_jobject*, JavaThread**, oopDesc**)
      703715    0.02%      705  Klass::check_array_allocation_length(int, int, JavaThread*)
      703584    0.02%      694  try_to_wake_up_[k]
      697890    0.02%      701  ChannelsGrid$$Lambda$60.0x0000000801036fc8.accept
      689963    0.02%      693  ChannelsGrid.lambda$forEachChannel$1
      687624    0.02%      694  AbstractOwnableSynchronizer.setExclusiveOwnerThread
      675008    0.01%      618  exit_to_user_mode_prepare_[k]
      672077    0.01%      668  HeapRegionManager::allocate_free_region(HeapRegionType, unsigned int)
      670204    0.01%      666  GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
      659217    0.01%      656  ObjAllocator::initialize(HeapWordImpl**) const
      652473    0.01%      643  psi_task_switch_[k]
      638932    0.01%      632  G1Allocator::unsafe_max_tlab_alloc()
      626680    0.01%      625  ClassLoaderData::holder() const
      619410    0.01%      617  MemAllocator::Allocation::check_out_of_memory()
      617841    0.01%      605  _raw_spin_lock_irqsave_[k]
      617689    0.01%      610  futex_wake_mark_[k]
      610894    0.01%      601  timerqueue_add_[k]
      603679    0.01%      595  __get_user_nocheck_4_[k]
      600530    0.01%      600  ObjArrayKlass::allocate(int, JavaThread*)
      595106    0.01%      585  native_read_msr_[k]
      593689    0.01%      594  __vdso_clock_gettime
      593478    0.01%      587  G1CollectedHeap::attempt_allocation_slow(unsigned long)
      583724    0.01%      582  G1Analytics::predict_card_merge_time_ms(unsigned long, bool) const
      581731    0.01%      574  __perf_event_task_sched_out_[k]
      575269    0.01%      569  AbsSeq::davg() const
      564450    0.01%      561  G1CollectedHeap::allocate_new_tlab(unsigned long, unsigned long, unsigned long*)
      563871    0.01%      566  DirectMethodHandle$Holder.newInvokeSpecial
      557689    0.01%      551  resched_curr_[k]
      548720    0.01%      544  G1CardSet::clear()
      542265    0.01%      542  __GI___pthread_getspecific
      540947    0.01%      540  OptoRuntime::new_instance_C(Klass*, JavaThread*)
      540564    0.01%      532  __softirqentry_text_start_[k]
      529893    0.01%      518  preempt_count_sub_[k]
      529404    0.01%      535  native_write_msr_[k]
      528972    0.01%      526  available_idle_cpu_[k]
      525471    0.01%      524  CardTableBarrierSet::on_slowpath_allocation_exit(JavaThread*, oopDesc*)
      516423    0.01%      516  JfrObjectAllocationSample::send_event(Klass const*, unsigned long, bool, Thread*)
      515848    0.01%      509  task_tick_fair_[k]
      512346    0.01%      515  G1RemSetScanState::G1ClearCardTableTask::do_work(unsigned int)
      508197    0.01%      494  futex_wait_[k]
      502379    0.01%      503  ArrayList.elementAt
      498014    0.01%      492  __cgroup_account_cputime_[k]
      497239    0.01%      498  MemAllocator::Allocation::notify_allocation_jfr_sampler()
      496721    0.01%      498  I2C/C2I adapters
      493029    0.01%      487  newidle_balance_[k]
      478181    0.01%      476  G1YoungRemSetSamplingClosure::do_heap_region(HeapRegion*)
      472158    0.01%      469  G1RegionsOnNodes::add(HeapRegion*)
      471248    0.01%      462  asm_sysvec_apic_timer_interrupt_[k]
      465358    0.01%      465  Unsafe.unpark
      463750    0.01%      462  G1SegmentedArray::num_segments() const
      457144    0.01%      454  JavaFrameAnchor::make_walkable()
      455195    0.01%      452  GameOfLife.endOfFrame
      448170    0.01%      448  Cell$$Lambda$53.0x0000000801036510.accept
      440115    0.01%      443  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<544868ul, G1BarrierSet>, (AccessInternal::BarrierType)2, 544868ul>::oop_access_barrier(void*)
      430990    0.01%      429  AbstractQueuedSynchronizer.acquire
      428884    0.01%      393  __rseq_handle_notify_resume_[k]
      426650    0.01%      426  MemAllocator::allocate_inside_tlab_slow(MemAllocator::Allocation&) const
      422854    0.01%      415  timekeeping_advance_[k]
      413763    0.01%      412  HeapRegionTracer::send_region_type_change(unsigned int, G1HeapRegionTraceType::Type, G1HeapRegionTraceType::Type, unsigned long, unsigned long)
      408560    0.01%      408  AbstractQueuedSynchronizer$ConditionObject.enableWait
      401437    0.01%      403  ReferencePipeline.<init>
      397156    0.01%      394  __futex_abstimed_wait_common
      387203    0.01%      388  SharedRuntime::on_slowpath_allocation_exit(JavaThread*)
      384707    0.01%      379  check_preempt_curr_[k]
      384010    0.01%      381  G1MonitoringSupport::update_eden_size()
      378632    0.01%      377  G1CollectedHeap::fill_with_dummy_object(HeapWordImpl**, HeapWordImpl**, bool)
      370637    0.01%      366  ThreadLocalStorage::is_initialized()
      370489    0.01%      369  G1RebuildFreeListTask::work(unsigned int)
      368462    0.01%      364  __rcu_read_lock_[k]
      367395    0.01%      365  HSpaceCounters::update_used(unsigned long)
      367394    0.01%      362  entry_SYSCALL_64_safe_stack_[k]
      357243    0.01%      350  update_process_times_[k]
      350684    0.01%      350  G1CardTable::is_in_young(oopDesc*) const
      349903    0.01%      347  _find_next_bit_[k]
      344721    0.01%      342  place_entity_[k]
      343905    0.01%      336  pvclock_gtod_notify?[kvm]_[k]
      339609    0.01%      335  rebalance_domains_[k]
      338436    0.01%      339  ReentrantLock$NonfairSync.initialTryLock
      337822    0.01%      329  __pthread_mutex_cond_lock
      334618    0.01%      332  G1Analytics::predict_young_other_time_ms(unsigned long) const
      325895    0.01%      320  account_user_time_[k]
      321708    0.01%      326  LinkedBlockingQueue.enqueue
      319395    0.01%      323  Unsafe_AllocateInstance
      316664    0.01%      312  LinkedBlockingQueue.take
      316387    0.01%      310  __cgroup_account_cputime_field_[k]
      315219    0.01%      306  futex_hash_[k]
      315072    0.01%      316  ArrayList.forEach
      312904    0.01%      309  __rcu_read_unlock_[k]
      312216    0.01%      309  G1CollectionSet::add_eden_region(HeapRegion*)
      310264    0.01%      307  check_spread.isra.0_[k]
      307296    0.01%      306  LockSupport.park
