--- Execution profile ---
Total samples       : 10081519
not_walkable_not_Java: 4 (0.00%)
unknown_Java        : 39566 (0.39%)
not_walkable_Java   : 12662 (0.13%)
deoptimization      : 26 (0.00%)

--- 1509922574 total (14.98%), 1510251 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 1033915076 total (10.26%), 1035611 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 1023262184 total (10.15%), 1026088 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$63.0x0000000801036978.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 969309566 total (9.61%), 966993 samples
  [ 0] ReduceOps$5ReducingSink.get
  [ 1] ReduceOps$5ReducingSink.get
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 503838800 total (5.00%), 504022 samples
  [ 0] Cell$$Lambda$63.0x0000000801036978.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 431020826 total (4.28%), 431554 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] Cell.lambda$notifyLiveness$0
  [ 5] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 6] ArrayList.forEach
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 317665811 total (3.15%), 318298 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$63.0x0000000801036978.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 260250579 total (2.58%), 260594 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$63.0x0000000801036978.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 193600887 total (1.92%), 193783 samples
  [ 0] Channel.take
  [ 1] Cell$$Lambda$63.0x0000000801036978.apply
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] Continuation.enterSpecial
  [15] Continuation.run
  [16] VirtualThread.runContinuation
  [17] VirtualThread$$Lambda$51.0x000000080103df08.run
  [18] ForkJoinTask$RunnableExecuteAction.exec
  [19] ForkJoinTask.doExec
  [20] ForkJoinPool$WorkQueue.topLevelExec
  [21] ForkJoinPool.scan
  [22] ForkJoinPool.runWorker
  [23] ForkJoinWorkerThread.run

--- 181878655 total (1.80%), 182077 samples
  [ 0] TickPerCell.lambda$tick$0
  [ 1] TickPerCell$$Lambda$57.0x0000000801035cb0.accept
  [ 2] ChannelsGrid.lambda$forEachChannel$0
  [ 3] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [ 4] Dimensions.forEachRowCol
  [ 5] ChannelsGrid.forEachChannel
  [ 6] TickPerCell.tick
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$53.0x0000000801035458.run
  [10] VirtualThread.run
  [11] VirtualThread$VThreadContinuation.lambda$new$0
  [12] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [13] Continuation.enter0
  [14] Continuation.enter
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 140419916 total (1.39%), 140753 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$57.0x0000000801035cb0.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$53.0x0000000801035458.run
  [16] VirtualThread.run
  [17] VirtualThread$VThreadContinuation.lambda$new$0
  [18] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [19] Continuation.enter0
  [20] Continuation.enter
  [21] Continuation.enterSpecial
  [22] Continuation.run
  [23] VirtualThread.runContinuation
  [24] VirtualThread$$Lambda$51.0x000000080103df08.run
  [25] ForkJoinTask$RunnableExecuteAction.exec
  [26] ForkJoinTask.doExec
  [27] ForkJoinPool$WorkQueue.topLevelExec
  [28] ForkJoinPool.scan
  [29] ForkJoinPool.runWorker
  [30] ForkJoinWorkerThread.run

--- 95172931 total (0.94%), 95059 samples
  [ 0] Channel.take
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 91268058 total (0.91%), 91251 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 89882065 total (0.89%), 90002 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035458.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 85379481 total (0.85%), 85373 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] TickPerCell.lambda$tick$0
  [ 5] TickPerCell$$Lambda$57.0x0000000801035cb0.accept
  [ 6] ChannelsGrid.lambda$forEachChannel$0
  [ 7] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] TickPerCell.tick
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035458.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 83500703 total (0.83%), 83455 samples
  [ 0] Cell.calculateNextState
  [ 1] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 68694964 total (0.68%), 68601 samples
  [ 0] ReferencePipeline$3.<init>
  [ 1] ReferencePipeline.map
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 68220962 total (0.68%), 68016 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 67764049 total (0.67%), 67569 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 67208528 total (0.67%), 67203 samples
  [ 0] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 65324536 total (0.65%), 65264 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 64717262 total (0.64%), 64819 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$63.0x0000000801036978.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 64361437 total (0.64%), 64361 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035458.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 61380255 total (0.61%), 61235 samples
  [ 0] vtable stub
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 61354513 total (0.61%), 61521 samples
  [ 0] ReentrantLock$Sync.lock
  [ 1] ReentrantLock.lock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$63.0x0000000801036978.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 56041956 total (0.56%), 55925 samples
  [ 0] StreamOpFlag.fromCharacteristics
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 51732880 total (0.51%), 51747 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 5] Iterable.forEach
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 46372580 total (0.46%), 46306 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 43294909 total (0.43%), 43154 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 35747292 total (0.35%), 35772 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$53.0x0000000801035458.run
  [13] VirtualThread.run
  [14] VirtualThread$VThreadContinuation.lambda$new$0
  [15] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [16] Continuation.enter0
  [17] Continuation.enter
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 35724661 total (0.35%), 35781 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [11] Iterable.forEach
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 32386449 total (0.32%), 32389 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 31594527 total (0.31%), 31659 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 30906743 total (0.31%), 30924 samples
  [ 0] Channel.take
  [ 1] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 2] ChannelsGrid.lambda$forEachChannel$1
  [ 3] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 4] Dimensions.forEachRowCol
  [ 5] ChannelsGrid.forEachChannel
  [ 6] GameOfLife.calculateFrame
  [ 7] GameOfLife.lambda$calculateFrameBlocking$4
  [ 8] GameOfLife$$Lambda$53.0x0000000801035458.run
  [ 9] VirtualThread.run
  [10] VirtualThread$VThreadContinuation.lambda$new$0
  [11] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [12] Continuation.enter0
  [13] Continuation.enter
  [14] Continuation.enterSpecial
  [15] Continuation.run
  [16] VirtualThread.runContinuation
  [17] VirtualThread$$Lambda$51.0x000000080103df08.run
  [18] ForkJoinTask$RunnableExecuteAction.exec
  [19] ForkJoinTask.doExec
  [20] ForkJoinPool$WorkQueue.topLevelExec
  [21] ForkJoinPool.scan
  [22] ForkJoinPool.runWorker
  [23] ForkJoinWorkerThread.run

--- 29626856 total (0.29%), 29700 samples
  [ 0] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 1] ArrayList.forEach
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 29183079 total (0.29%), 29276 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$63.0x0000000801036978.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 28942330 total (0.29%), 28973 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 28389028 total (0.28%), 28336 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 28257258 total (0.28%), 28308 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 28193886 total (0.28%), 28283 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$63.0x0000000801036978.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 27878682 total (0.28%), 27856 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 27840406 total (0.28%), 27925 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$63.0x0000000801036978.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 27172441 total (0.27%), 27179 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$53.0x0000000801035458.run
  [ 6] VirtualThread.run
  [ 7] VirtualThread$VThreadContinuation.lambda$new$0
  [ 8] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 9] Continuation.enter0
  [10] Continuation.enter
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 26855241 total (0.27%), 26754 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 25192192 total (0.25%), 25134 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 24195243 total (0.24%), 24240 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 23737320 total (0.24%), 23720 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 23353537 total (0.23%), 23327 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 22559739 total (0.22%), 22605 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 21759333 total (0.22%), 21817 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 20890375 total (0.21%), 20871 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 19524977 total (0.19%), 19536 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$53.0x0000000801035458.run
  [10] VirtualThread.run
  [11] VirtualThread$VThreadContinuation.lambda$new$0
  [12] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [13] Continuation.enter0
  [14] Continuation.enter
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 19152971 total (0.19%), 19105 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 18766042 total (0.19%), 18731 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 17635092 total (0.17%), 17692 samples
  [ 0] ReentrantLock$Sync.isHeldExclusively
  [ 1] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$63.0x0000000801036978.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 17472487 total (0.17%), 17491 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 16655577 total (0.17%), 16656 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 16620354 total (0.16%), 16669 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$63.0x0000000801036978.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 16377808 total (0.16%), 16425 samples
  [ 0] ReentrantLock.lock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$63.0x0000000801036978.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 16375267 total (0.16%), 16424 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$63.0x0000000801036978.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 15941620 total (0.16%), 15992 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$63.0x0000000801036978.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 15865050 total (0.16%), 15839 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$53.0x0000000801035458.run
  [ 6] VirtualThread.run
  [ 7] VirtualThread$VThreadContinuation.lambda$new$0
  [ 8] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 9] Continuation.enter0
  [10] Continuation.enter
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 15558052 total (0.15%), 15609 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$63.0x0000000801036978.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 15524306 total (0.15%), 15576 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$63.0x0000000801036978.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 15423804 total (0.15%), 15444 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.put
  [ 3] Channel.put
  [ 4] Cell.lambda$notifyLiveness$0
  [ 5] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 6] ArrayList.forEach
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 9] Iterable.forEach
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 15061855 total (0.15%), 15100 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 14528958 total (0.14%), 14550 samples
  [ 0] Channel.put
  [ 1] Cell.lambda$notifyLiveness$0
  [ 2] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 3] ArrayList.forEach
  [ 4] Cell.notifyLiveness
  [ 5] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 6] Iterable.forEach
  [ 7] CellsGroup.run
  [ 8] Continuation.enterSpecial
  [ 9] Continuation.run
  [10] VirtualThread.runContinuation
  [11] VirtualThread$$Lambda$51.0x000000080103df08.run
  [12] ForkJoinTask$RunnableExecuteAction.exec
  [13] ForkJoinTask.doExec
  [14] ForkJoinPool$WorkQueue.topLevelExec
  [15] ForkJoinPool.scan
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 14095051 total (0.14%), 14127 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.put
  [ 2] Channel.put
  [ 3] Cell.lambda$notifyLiveness$0
  [ 4] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 5] ArrayList.forEach
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 13533760 total (0.13%), 13556 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 13414316 total (0.13%), 13389 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 13121398 total (0.13%), 13100 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 13094325 total (0.13%), 12761 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_switch_[k]
  [ 2] __schedule_[k]
  [ 3] schedule_[k]
  [ 4] futex_wait_queue_[k]
  [ 5] futex_wait_[k]
  [ 6] do_futex_[k]
  [ 7] __x64_sys_futex_[k]
  [ 8] do_syscall_64_[k]
  [ 9] entry_SYSCALL_64_after_hwframe_[k]
  [10] __futex_abstimed_wait_common
  [11] Unsafe.park
  [12] LockSupport.park
  [13] ForkJoinPool.awaitWork
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 12673762 total (0.13%), 12669 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 12503516 total (0.12%), 12482 samples
  [ 0] StreamSupport.stream
  [ 1] Collection.stream
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 12477822 total (0.12%), 12494 samples
  [ 0] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 1] ArrayList.forEach
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 11628491 total (0.12%), 11659 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 11440445 total (0.11%), 11465 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 10917525 total (0.11%), 10931 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 10842617 total (0.11%), 10841 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] Sink$ChainedReference.begin
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 10705693 total (0.11%), 10713 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.put
  [ 2] Channel.put
  [ 3] Cell.lambda$notifyLiveness$0
  [ 4] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 5] ArrayList.forEach
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 10640175 total (0.11%), 10658 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$63.0x0000000801036978.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 10539502 total (0.10%), 10562 samples
  [ 0] ReferencePipeline$4$1.accept
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 10512977 total (0.10%), 10535 samples
  [ 0] Cell$$Lambda$66.0x0000000801036fd0.applyAsInt
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] Continuation.enterSpecial
  [15] Continuation.run
  [16] VirtualThread.runContinuation
  [17] VirtualThread$$Lambda$51.0x000000080103df08.run
  [18] ForkJoinTask$RunnableExecuteAction.exec
  [19] ForkJoinTask.doExec
  [20] ForkJoinPool$WorkQueue.topLevelExec
  [21] ForkJoinPool.scan
  [22] ForkJoinPool.runWorker
  [23] ForkJoinWorkerThread.run

--- 10419903 total (0.10%), 10398 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 10163363 total (0.10%), 10173 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$63.0x0000000801036978.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 10148077 total (0.10%), 10138 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 10098034 total (0.10%), 10070 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$4$1.<init>
  [ 2] ReferencePipeline$4.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 10042095 total (0.10%), 10041 samples
  [ 0] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [ 1] Dimensions.forEachRowCol
  [ 2] ChannelsGrid.forEachChannel
  [ 3] TickPerCell.tick
  [ 4] GameOfLife.calculateFrame
  [ 5] GameOfLife.lambda$calculateFrameBlocking$4
  [ 6] GameOfLife$$Lambda$53.0x0000000801035458.run
  [ 7] VirtualThread.run
  [ 8] VirtualThread$VThreadContinuation.lambda$new$0
  [ 9] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [10] Continuation.enter0
  [11] Continuation.enter
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 9945638 total (0.10%), 9967 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] Continuation.enterSpecial
  [15] Continuation.run
  [16] VirtualThread.runContinuation
  [17] VirtualThread$$Lambda$51.0x000000080103df08.run
  [18] ForkJoinTask$RunnableExecuteAction.exec
  [19] ForkJoinTask.doExec
  [20] ForkJoinPool$WorkQueue.topLevelExec
  [21] ForkJoinPool.scan
  [22] ForkJoinPool.runWorker
  [23] ForkJoinWorkerThread.run

--- 9916239 total (0.10%), 9901 samples
  [ 0] Sink$ChainedReference.<init>
  [ 1] ReferencePipeline$3$1.<init>
  [ 2] ReferencePipeline$3.opWrapSink
  [ 3] AbstractPipeline.wrapSink
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 9799644 total (0.10%), 9808 samples
  [ 0] ReentrantLock$Sync.lock
  [ 1] ReentrantLock.lock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035458.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 9690975 total (0.10%), 9710 samples
  [ 0] ReduceOps$5ReducingSink.accept
  [ 1] ReferencePipeline$4$1.accept
  [ 2] ReferencePipeline$3$1.accept
  [ 3] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 4] AbstractPipeline.copyInto
  [ 5] AbstractPipeline.wrapAndCopyInto
  [ 6] ReduceOps$ReduceOp.evaluateSequential
  [ 7] AbstractPipeline.evaluate
  [ 8] IntPipeline.reduce
  [ 9] IntPipeline.sum
  [10] Cell.calculateNextState
  [11] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [12] Iterable.forEach
  [13] CellsGroup.run
  [14] Continuation.enterSpecial
  [15] Continuation.run
  [16] VirtualThread.runContinuation
  [17] VirtualThread$$Lambda$51.0x000000080103df08.run
  [18] ForkJoinTask$RunnableExecuteAction.exec
  [19] ForkJoinTask.doExec
  [20] ForkJoinPool$WorkQueue.topLevelExec
  [21] ForkJoinPool.scan
  [22] ForkJoinPool.runWorker
  [23] ForkJoinWorkerThread.run

--- 9563559 total (0.09%), 9550 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 9332207 total (0.09%), 9357 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$63.0x0000000801036978.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$AdaptedRunnableAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 9243357 total (0.09%), 9244 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 9169321 total (0.09%), 9185 samples
  [ 0] ReferencePipeline$3$1.accept
  [ 1] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 8995440 total (0.09%), 8981 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 8856739 total (0.09%), 8867 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$53.0x0000000801035458.run
  [13] VirtualThread.run
  [14] VirtualThread$VThreadContinuation.lambda$new$0
  [15] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [16] Continuation.enter0
  [17] Continuation.enter
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 8782016 total (0.09%), 8782 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 8741729 total (0.09%), 8722 samples
  [ 0] ReduceOps$5ReducingSink.get
  [ 1] ReduceOps$5ReducingSink.get
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$AdaptedRunnableAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 8552981 total (0.08%), 8550 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 8541403 total (0.08%), 8544 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$53.0x0000000801035458.run
  [10] VirtualThread.run
  [11] VirtualThread$VThreadContinuation.lambda$new$0
  [12] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [13] Continuation.enter0
  [14] Continuation.enter
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 8461197 total (0.08%), 8462 samples
  [ 0] Cell.notifyLiveness
  [ 1] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 2] Iterable.forEach
  [ 3] CellsGroup.run
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 7597770 total (0.08%), 7599 samples
  [ 0] ReentrantLock$Sync.lock
  [ 1] ReentrantLock.lock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] TickPerCell.waitTick
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 7444146 total (0.07%), 7422 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 7336781 total (0.07%), 7339 samples
  [ 0] Cell.lambda$notifyLiveness$0
  [ 1] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 2] ArrayList.forEach
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$AdaptedRunnableAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 7150673 total (0.07%), 7165 samples
  [ 0] ReentrantLock.lock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$63.0x0000000801036978.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 7122805 total (0.07%), 7104 samples
  [ 0] AbstractPipeline.<init>
  [ 1] IntPipeline.<init>
  [ 2] IntPipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$4.<init>
  [ 4] ReferencePipeline.mapToInt
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 7088571 total (0.07%), 7082 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$Head.<init>
  [ 3] StreamSupport.stream
  [ 4] Collection.stream
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 6902763 total (0.07%), 6925 samples
  [ 0] ChannelsGrid.getChannel
  [ 1] ChannelsGrid.lambda$forEachChannel$1
  [ 2] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 3] Dimensions.forEachRowCol
  [ 4] ChannelsGrid.forEachChannel
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$53.0x0000000801035458.run
  [ 8] VirtualThread.run
  [ 9] VirtualThread$VThreadContinuation.lambda$new$0
  [10] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [11] Continuation.enter0
  [12] Continuation.enter
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 6902383 total (0.07%), 6904 samples
  [ 0] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 1] Dimensions.forEachRowCol
  [ 2] ChannelsGrid.forEachChannel
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$53.0x0000000801035458.run
  [ 6] VirtualThread.run
  [ 7] VirtualThread$VThreadContinuation.lambda$new$0
  [ 8] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 9] Continuation.enter0
  [10] Continuation.enter
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 6630679 total (0.07%), 6580 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] TickPerCell.waitTick
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 6526648 total (0.06%), 6524 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] [unknown_Java]

--- 6451048 total (0.06%), 6434 samples
  [ 0] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
  [ 1] GameOfLifeBenchmark_benchmark_jmhTest.benchmark_Throughput
  [ 2] DirectMethodHandle$Holder.invokeSpecial
  [ 3] LambdaForm$MH.0x000000080102e000.invoke
  [ 4] LambdaForm$MH.0x000000080102e400.invokeExact_MT
  [ 5] DirectMethodHandleAccessor.invokeImpl
  [ 6] DirectMethodHandleAccessor.invoke
  [ 7] Method.invoke
  [ 8] BenchmarkHandler$BenchmarkTask.call
  [ 9] BenchmarkHandler$BenchmarkTask.call
  [10] FutureTask.run
  [11] Executors$RunnableAdapter.call
  [12] FutureTask.run
  [13] ThreadPoolExecutor.runWorker
  [14] ThreadPoolExecutor$Worker.run
  [15] Thread.run

--- 6361504 total (0.06%), 6358 samples
  [ 0] ReferencePipeline$3.<init>
  [ 1] ReferencePipeline.map
  [ 2] Cell.calculateNextState
  [ 3] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 6204151 total (0.06%), 6217 samples
  [ 0] ReentrantLock.unlock
  [ 1] LockedSingleValue.put
  [ 2] Channel.put
  [ 3] Cell.lambda$notifyLiveness$0
  [ 4] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 5] ArrayList.forEach
  [ 6] Cell.notifyLiveness
  [ 7] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 6145379 total (0.06%), 6075 samples
  [ 0] vframeStream::vframeStream(JavaThread*, bool, bool, bool)
  [ 1] SharedRuntime::find_callee_method(JavaThread*)
  [ 2] SharedRuntime::reresolve_call_site(JavaThread*)
  [ 3] SharedRuntime::handle_wrong_method(JavaThread*)
  [ 4] Continuation.enterSpecial
  [ 5] Continuation.run
  [ 6] VirtualThread.runContinuation
  [ 7] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 8] ForkJoinTask$RunnableExecuteAction.exec
  [ 9] ForkJoinTask.doExec
  [10] ForkJoinPool$WorkQueue.topLevelExec
  [11] ForkJoinPool.scan
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 5977613 total (0.06%), 5963 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 5829007 total (0.06%), 5831 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 5578876 total (0.06%), 5565 samples
  [ 0] DirectMethodHandle.allocateInstance
  [ 1] DirectMethodHandle$Holder.newInvokeSpecial
  [ 2] Invokers$Holder.linkToTargetMethod
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 5169936 total (0.05%), 5161 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] [unknown_Java]

--- 5082637 total (0.05%), 5068 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] [unknown_Java]

--- 5044641 total (0.05%), 4943 samples
  [ 0] update_cfs_group_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] ForkJoinPool.awaitWork
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 5014052 total (0.05%), 5023 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$AdaptedRunnableAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 4947615 total (0.05%), 4954 samples
  [ 0] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 4836629 total (0.05%), 4844 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035458.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$AdaptedRunnableAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 4656126 total (0.05%), 4646 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 4655059 total (0.05%), 4657 samples
  [ 0] Cell$$Lambda$63.0x0000000801036978.apply
  [ 1] ReferencePipeline$3$1.accept
  [ 2] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 3] AbstractPipeline.copyInto
  [ 4] AbstractPipeline.wrapAndCopyInto
  [ 5] ReduceOps$ReduceOp.evaluateSequential
  [ 6] AbstractPipeline.evaluate
  [ 7] IntPipeline.reduce
  [ 8] IntPipeline.sum
  [ 9] Cell.calculateNextState
  [10] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$AdaptedRunnableAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 4621272 total (0.05%), 4635 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$63.0x0000000801036978.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [17] Iterable.forEach
  [18] CellsGroup.run
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 4620940 total (0.05%), 4618 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] [unknown_Java]

--- 4596624 total (0.05%), 4591 samples
  [ 0] Sink$ChainedReference.begin
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 4571712 total (0.05%), 4566 samples
  [ 0] AbstractPipeline.wrapSink
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 4379438 total (0.04%), 4375 samples
  [ 0] Iterable.forEach
  [ 1] CellsGroup.run
  [ 2] Continuation.enterSpecial
  [ 3] Continuation.run
  [ 4] VirtualThread.runContinuation
  [ 5] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 6] ForkJoinTask$RunnableExecuteAction.exec
  [ 7] ForkJoinTask.doExec
  [ 8] ForkJoinPool$WorkQueue.topLevelExec
  [ 9] ForkJoinPool.scan
  [10] ForkJoinPool.runWorker
  [11] ForkJoinWorkerThread.run

--- 4233676 total (0.04%), 4217 samples
  [ 0] Collection.stream
  [ 1] Cell.calculateNextState
  [ 2] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 4211102 total (0.04%), 4121 samples
  [ 0] InstanceKlass::find_method_index(Array<Method*> const*, Symbol const*, Symbol const*, Klass::OverpassLookupMode, Klass::StaticLookupMode, Klass::PrivateLookupMode) [clone .constprop.0]
  [ 1] InstanceKlass::uncached_lookup_method(Symbol const*, Symbol const*, Klass::OverpassLookupMode, Klass::PrivateLookupMode) const
  [ 2] LinkResolver::lookup_method_in_klasses(LinkInfo const&, bool, bool)
  [ 3] LinkResolver::resolve_method(LinkInfo const&, Bytecodes::Code, JavaThread*)
  [ 4] LinkResolver::resolve_continuation_enter(CallInfo&, JavaThread*)
  [ 5] SharedRuntime::find_callee_info_helper(vframeStream&, Bytecodes::Code&, CallInfo&, JavaThread*)
  [ 6] SharedRuntime::find_callee_method(JavaThread*)
  [ 7] SharedRuntime::reresolve_call_site(JavaThread*)
  [ 8] SharedRuntime::handle_wrong_method(JavaThread*)
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 4129293 total (0.04%), 4049 samples
  [ 0] update_curr_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] ForkJoinPool.awaitWork
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 4121607 total (0.04%), 4117 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.enableWait
  [ 1] AbstractQueuedSynchronizer$ConditionObject.await
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035458.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 4062752 total (0.04%), 4063 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$57.0x0000000801035cb0.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$53.0x0000000801035458.run
  [16] VirtualThread.run
  [17] VirtualThread$VThreadContinuation.lambda$new$0
  [18] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [19] Continuation.enter0
  [20] Continuation.enter
  [21] Continuation.enterSpecial
  [22] Continuation.run
  [23] VirtualThread.runContinuation
  [24] VirtualThread$$Lambda$51.0x000000080103df08.run
  [25] ForkJoinTask$RunnableExecuteAction.exec
  [26] ForkJoinTask.doExec
  [27] ForkJoinPool$WorkQueue.topLevelExec
  [28] ForkJoinPool.scan
  [29] ForkJoinPool.runWorker
  [30] ForkJoinWorkerThread.run

--- 4052039 total (0.04%), 4047 samples
  [ 0] AbstractPipeline.copyInto
  [ 1] AbstractPipeline.wrapAndCopyInto
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 8] Iterable.forEach
  [ 9] CellsGroup.run
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 4015475 total (0.04%), 4013 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 4015123 total (0.04%), 4009 samples
  [ 0] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 1] [unknown_Java]

--- 4008742 total (0.04%), 4013 samples
  [ 0] AbstractOwnableSynchronizer.setExclusiveOwnerThread
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] TickPerCell.lambda$tick$0
  [ 7] TickPerCell$$Lambda$57.0x0000000801035cb0.accept
  [ 8] ChannelsGrid.lambda$forEachChannel$0
  [ 9] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [10] Dimensions.forEachRowCol
  [11] ChannelsGrid.forEachChannel
  [12] TickPerCell.tick
  [13] GameOfLife.calculateFrame
  [14] GameOfLife.lambda$calculateFrameBlocking$4
  [15] GameOfLife$$Lambda$53.0x0000000801035458.run
  [16] VirtualThread.run
  [17] VirtualThread$VThreadContinuation.lambda$new$0
  [18] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [19] Continuation.enter0
  [20] Continuation.enter
  [21] Continuation.enterSpecial
  [22] Continuation.run
  [23] VirtualThread.runContinuation
  [24] VirtualThread$$Lambda$51.0x000000080103df08.run
  [25] ForkJoinTask$RunnableExecuteAction.exec
  [26] ForkJoinTask.doExec
  [27] ForkJoinPool$WorkQueue.topLevelExec
  [28] ForkJoinPool.scan
  [29] ForkJoinPool.runWorker
  [30] ForkJoinWorkerThread.run

--- 3999435 total (0.04%), 3913 samples
  [ 0] __schedule_[k]
  [ 1] schedule_[k]
  [ 2] futex_wait_queue_[k]
  [ 3] futex_wait_[k]
  [ 4] do_futex_[k]
  [ 5] __x64_sys_futex_[k]
  [ 6] do_syscall_64_[k]
  [ 7] entry_SYSCALL_64_after_hwframe_[k]
  [ 8] __futex_abstimed_wait_common
  [ 9] Unsafe.park
  [10] LockSupport.park
  [11] ForkJoinPool.awaitWork
  [12] ForkJoinPool.runWorker
  [13] ForkJoinWorkerThread.run

--- 3932596 total (0.04%), 3934 samples
  [ 0] ArrayList.forEach
  [ 1] [unknown_Java]

--- 3824340 total (0.04%), 3832 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] Cell.lambda$notifyLiveness$0
  [ 3] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 4] ArrayList.forEach
  [ 5] Cell.notifyLiveness
  [ 6] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 3774093 total (0.04%), 3696 samples
  [ 0] __update_load_avg_cfs_rq_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] ForkJoinPool.awaitWork
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 3730627 total (0.04%), 3727 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 3705818 total (0.04%), 3628 samples
  [ 0] update_load_avg_[k]
  [ 1] dequeue_entity_[k]
  [ 2] dequeue_task_fair_[k]
  [ 3] __schedule_[k]
  [ 4] schedule_[k]
  [ 5] futex_wait_queue_[k]
  [ 6] futex_wait_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] __futex_abstimed_wait_common
  [12] Unsafe.park
  [13] LockSupport.park
  [14] ForkJoinPool.awaitWork
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 3664212 total (0.04%), 3671 samples
  [ 0] ReentrantLock$Sync.tryRelease
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 6] ChannelsGrid.lambda$forEachChannel$1
  [ 7] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 8] Dimensions.forEachRowCol
  [ 9] ChannelsGrid.forEachChannel
  [10] GameOfLife.calculateFrame
  [11] GameOfLife.lambda$calculateFrameBlocking$4
  [12] GameOfLife$$Lambda$53.0x0000000801035458.run
  [13] VirtualThread.run
  [14] VirtualThread$VThreadContinuation.lambda$new$0
  [15] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [16] Continuation.enter0
  [17] Continuation.enter
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 3663352 total (0.04%), 3674 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$63.0x0000000801036978.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 3646936 total (0.04%), 3648 samples
  [ 0] AbstractOwnableSynchronizer.getExclusiveOwnerThread
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035458.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 3642224 total (0.04%), 3654 samples
  [ 0] AbstractQueuedSynchronizer.signalNext
  [ 1] AbstractQueuedSynchronizer.release
  [ 2] ReentrantLock.unlock
  [ 3] LockedSingleValue.take
  [ 4] Channel.take
  [ 5] Cell$$Lambda$63.0x0000000801036978.apply
  [ 6] ReferencePipeline$3$1.accept
  [ 7] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 8] AbstractPipeline.copyInto
  [ 9] AbstractPipeline.wrapAndCopyInto
  [10] ReduceOps$ReduceOp.evaluateSequential
  [11] AbstractPipeline.evaluate
  [12] IntPipeline.reduce
  [13] IntPipeline.sum
  [14] Cell.calculateNextState
  [15] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [16] Iterable.forEach
  [17] CellsGroup.run
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 3605836 total (0.04%), 3598 samples
  [ 0] AbstractPipeline.<init>
  [ 1] ReferencePipeline.<init>
  [ 2] ReferencePipeline$StatelessOp.<init>
  [ 3] ReferencePipeline$3.<init>
  [ 4] ReferencePipeline.map
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 3581046 total (0.04%), 3585 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 7] ChannelsGrid.lambda$forEachChannel$1
  [ 8] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] GameOfLife.calculateFrame
  [12] GameOfLife.lambda$calculateFrameBlocking$4
  [13] GameOfLife$$Lambda$53.0x0000000801035458.run
  [14] VirtualThread.run
  [15] VirtualThread$VThreadContinuation.lambda$new$0
  [16] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [17] Continuation.enter0
  [18] Continuation.enter
  [19] Continuation.enterSpecial
  [20] Continuation.run
  [21] VirtualThread.runContinuation
  [22] VirtualThread$$Lambda$51.0x000000080103df08.run
  [23] ForkJoinTask$RunnableExecuteAction.exec
  [24] ForkJoinTask.doExec
  [25] ForkJoinPool$WorkQueue.topLevelExec
  [26] ForkJoinPool.scan
  [27] ForkJoinPool.runWorker
  [28] ForkJoinWorkerThread.run

--- 3567523 total (0.04%), 3561 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 3556043 total (0.04%), 3562 samples
  [ 0] ReentrantLock$Sync.isHeldExclusively
  [ 1] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035458.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 3518318 total (0.03%), 3526 samples
  [ 0] Sink$ChainedReference.end
  [ 1] Sink$ChainedReference.end
  [ 2] AbstractPipeline.copyInto
  [ 3] AbstractPipeline.wrapAndCopyInto
  [ 4] ReduceOps$ReduceOp.evaluateSequential
  [ 5] AbstractPipeline.evaluate
  [ 6] IntPipeline.reduce
  [ 7] IntPipeline.sum
  [ 8] Cell.calculateNextState
  [ 9] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [10] Iterable.forEach
  [11] CellsGroup.run
  [12] Continuation.enterSpecial
  [13] Continuation.run
  [14] VirtualThread.runContinuation
  [15] VirtualThread$$Lambda$51.0x000000080103df08.run
  [16] ForkJoinTask$RunnableExecuteAction.exec
  [17] ForkJoinTask.doExec
  [18] ForkJoinPool$WorkQueue.topLevelExec
  [19] ForkJoinPool.scan
  [20] ForkJoinPool.runWorker
  [21] ForkJoinWorkerThread.run

--- 3462603 total (0.03%), 3472 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] Cell$$Lambda$63.0x0000000801036978.apply
  [ 7] ReferencePipeline$3$1.accept
  [ 8] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 9] AbstractPipeline.copyInto
  [10] AbstractPipeline.wrapAndCopyInto
  [11] ReduceOps$ReduceOp.evaluateSequential
  [12] AbstractPipeline.evaluate
  [13] IntPipeline.reduce
  [14] IntPipeline.sum
  [15] Cell.calculateNextState
  [16] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [17] Iterable.forEach
  [18] Continuation.enterSpecial
  [19] Continuation.run
  [20] VirtualThread.runContinuation
  [21] VirtualThread$$Lambda$51.0x000000080103df08.run
  [22] ForkJoinTask$RunnableExecuteAction.exec
  [23] ForkJoinTask.doExec
  [24] ForkJoinPool$WorkQueue.topLevelExec
  [25] ForkJoinPool.scan
  [26] ForkJoinPool.runWorker
  [27] ForkJoinWorkerThread.run

--- 3419804 total (0.03%), 3407 samples
  [ 0] AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 286822ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
  [ 1] int freeze<Config<(oop_kind)0, G1BarrierSet> >(JavaThread*, long*)
  [ 2] Continuation.enterSpecial
  [ 3] Continuation.run
  [ 4] VirtualThread.runContinuation
  [ 5] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 6] ForkJoinTask$RunnableExecuteAction.exec
  [ 7] ForkJoinTask.doExec
  [ 8] ForkJoinPool$WorkQueue.topLevelExec
  [ 9] ForkJoinPool.scan
  [10] ForkJoinPool.runWorker
  [11] ForkJoinWorkerThread.run

--- 3406657 total (0.03%), 3407 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$53.0x0000000801035458.run
  [ 6] VirtualThread.run
  [ 7] VirtualThread$VThreadContinuation.lambda$new$0
  [ 8] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 9] Continuation.enter0
  [10] Continuation.enter
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 3351534 total (0.03%), 3348 samples
  [ 0] Channel.take
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 4] Iterable.forEach
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 3299127 total (0.03%), 3310 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] Cell$$Lambda$63.0x0000000801036978.apply
  [ 3] ReferencePipeline$3$1.accept
  [ 4] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 5] AbstractPipeline.copyInto
  [ 6] AbstractPipeline.wrapAndCopyInto
  [ 7] ReduceOps$ReduceOp.evaluateSequential
  [ 8] AbstractPipeline.evaluate
  [ 9] IntPipeline.reduce
  [10] IntPipeline.sum
  [11] Cell.calculateNextState
  [12] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [13] Iterable.forEach
  [14] CellsGroup.run
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 3296183 total (0.03%), 3296 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035458.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$AdaptedRunnableAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 3272902 total (0.03%), 3266 samples
  [ 0] ReferencePipeline$4.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 3260914 total (0.03%), 3254 samples
  [ 0] ReduceOps$5ReducingSink.get
  [ 1] ReduceOps$5ReducingSink.get
  [ 2] ReduceOps$ReduceOp.evaluateSequential
  [ 3] AbstractPipeline.evaluate
  [ 4] IntPipeline.reduce
  [ 5] IntPipeline.sum
  [ 6] Cell.calculateNextState
  [ 7] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 8] Iterable.forEach
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 3234190 total (0.03%), 3231 samples
  [ 0] ReferencePipeline$3.opWrapSink
  [ 1] AbstractPipeline.wrapSink
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 3215335 total (0.03%), 3217 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 3191970 total (0.03%), 3200 samples
  [ 0] ReentrantLock$NonfairSync.initialTryLock
  [ 1] ReentrantLock$Sync.lock
  [ 2] ReentrantLock.lock
  [ 3] LockedSingleValue.put
  [ 4] Channel.put
  [ 5] TickPerCell.lambda$tick$0
  [ 6] TickPerCell$$Lambda$57.0x0000000801035cb0.accept
  [ 7] ChannelsGrid.lambda$forEachChannel$0
  [ 8] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [ 9] Dimensions.forEachRowCol
  [10] ChannelsGrid.forEachChannel
  [11] TickPerCell.tick
  [12] GameOfLife.calculateFrame
  [13] GameOfLife.lambda$calculateFrameBlocking$4
  [14] GameOfLife$$Lambda$53.0x0000000801035458.run
  [15] VirtualThread.run
  [16] VirtualThread$VThreadContinuation.lambda$new$0
  [17] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [18] Continuation.enter0
  [19] Continuation.enter
  [20] Continuation.enterSpecial
  [21] Continuation.run
  [22] VirtualThread.runContinuation
  [23] VirtualThread$$Lambda$51.0x000000080103df08.run
  [24] ForkJoinTask$RunnableExecuteAction.exec
  [25] ForkJoinTask.doExec
  [26] ForkJoinPool$WorkQueue.topLevelExec
  [27] ForkJoinPool.scan
  [28] ForkJoinPool.runWorker
  [29] ForkJoinWorkerThread.run

--- 3169950 total (0.03%), 3170 samples
  [ 0] ArrayList.forEach
  [ 1] Cell.notifyLiveness
  [ 2] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 3] Iterable.forEach
  [ 4] CellsGroup.run
  [ 5] Continuation.enterSpecial
  [ 6] Continuation.run
  [ 7] VirtualThread.runContinuation
  [ 8] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 9] ForkJoinTask$RunnableExecuteAction.exec
  [10] ForkJoinTask.doExec
  [11] ForkJoinPool$WorkQueue.topLevelExec
  [12] ForkJoinPool.scan
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 3169931 total (0.03%), 3169 samples
  [ 0] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [ 1] [unknown_Java]

--- 3095608 total (0.03%), 3087 samples
  [ 0] Objects.requireNonNull
  [ 1] StreamSupport.stream
  [ 2] Collection.stream
  [ 3] Cell.calculateNextState
  [ 4] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 3084588 total (0.03%), 3084 samples
  [ 0] AbstractQueuedSynchronizer.compareAndSetState
  [ 1] ReentrantLock$NonfairSync.initialTryLock
  [ 2] ReentrantLock$Sync.lock
  [ 3] ReentrantLock.lock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 9] Iterable.forEach
  [10] Continuation.enterSpecial
  [11] Continuation.run
  [12] VirtualThread.runContinuation
  [13] VirtualThread$$Lambda$51.0x000000080103df08.run
  [14] ForkJoinTask$RunnableExecuteAction.exec
  [15] ForkJoinTask.doExec
  [16] ForkJoinPool$WorkQueue.topLevelExec
  [17] ForkJoinPool.scan
  [18] ForkJoinPool.runWorker
  [19] ForkJoinWorkerThread.run

--- 3079081 total (0.03%), 3015 samples
  [ 0] __update_load_avg_se_[k]
  [ 1] update_load_avg_[k]
  [ 2] dequeue_entity_[k]
  [ 3] dequeue_task_fair_[k]
  [ 4] __schedule_[k]
  [ 5] schedule_[k]
  [ 6] futex_wait_queue_[k]
  [ 7] futex_wait_[k]
  [ 8] do_futex_[k]
  [ 9] __x64_sys_futex_[k]
  [10] do_syscall_64_[k]
  [11] entry_SYSCALL_64_after_hwframe_[k]
  [12] __futex_abstimed_wait_common
  [13] Unsafe.park
  [14] LockSupport.park
  [15] ForkJoinPool.awaitWork
  [16] ForkJoinPool.runWorker
  [17] ForkJoinWorkerThread.run

--- 3072061 total (0.03%), 3081 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$63.0x0000000801036978.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 3054979 total (0.03%), 3060 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$53.0x0000000801035458.run
  [10] VirtualThread.run
  [11] VirtualThread$VThreadContinuation.lambda$new$0
  [12] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [13] Continuation.enter0
  [14] Continuation.enter
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 3054281 total (0.03%), 3050 samples
  [ 0] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 3045529 total (0.03%), 3050 samples
  [ 0] ReentrantLock.lock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 4] ChannelsGrid.lambda$forEachChannel$1
  [ 5] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] GameOfLife.calculateFrame
  [ 9] GameOfLife.lambda$calculateFrameBlocking$4
  [10] GameOfLife$$Lambda$53.0x0000000801035458.run
  [11] VirtualThread.run
  [12] VirtualThread$VThreadContinuation.lambda$new$0
  [13] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [14] Continuation.enter0
  [15] Continuation.enter
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 3040925 total (0.03%), 3044 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 4] ChannelsGrid.lambda$forEachChannel$1
  [ 5] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] GameOfLife.calculateFrame
  [ 9] GameOfLife.lambda$calculateFrameBlocking$4
  [10] GameOfLife$$Lambda$53.0x0000000801035458.run
  [11] VirtualThread.run
  [12] VirtualThread$VThreadContinuation.lambda$new$0
  [13] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [14] Continuation.enter0
  [15] Continuation.enter
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 2998346 total (0.03%), 2997 samples
  [ 0] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 1] AbstractPipeline.copyInto
  [ 2] AbstractPipeline.wrapAndCopyInto
  [ 3] ReduceOps$ReduceOp.evaluateSequential
  [ 4] AbstractPipeline.evaluate
  [ 5] IntPipeline.reduce
  [ 6] IntPipeline.sum
  [ 7] Cell.calculateNextState
  [ 8] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 2983711 total (0.03%), 2979 samples
  [ 0] ArrayList$SubList$1.next
  [ 1] Iterable.forEach
  [ 2] CellsGroup.run
  [ 3] Continuation.enterSpecial
  [ 4] Continuation.run
  [ 5] VirtualThread.runContinuation
  [ 6] VirtualThread$$Lambda$51.0x000000080103df08.run
  [ 7] ForkJoinTask$RunnableExecuteAction.exec
  [ 8] ForkJoinTask.doExec
  [ 9] ForkJoinPool$WorkQueue.topLevelExec
  [10] ForkJoinPool.scan
  [11] ForkJoinPool.runWorker
  [12] ForkJoinWorkerThread.run

--- 2931306 total (0.03%), 2941 samples
  [ 0] ReentrantLock.unlock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] Cell$$Lambda$63.0x0000000801036978.apply
  [ 4] ReferencePipeline$3$1.accept
  [ 5] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 6] AbstractPipeline.copyInto
  [ 7] AbstractPipeline.wrapAndCopyInto
  [ 8] ReduceOps$ReduceOp.evaluateSequential
  [ 9] AbstractPipeline.evaluate
  [10] IntPipeline.reduce
  [11] IntPipeline.sum
  [12] Cell.calculateNextState
  [13] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [14] Iterable.forEach
  [15] CellsGroup.run
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 2910489 total (0.03%), 2916 samples
  [ 0] AbstractQueuedSynchronizer.enqueue
  [ 1] AbstractQueuedSynchronizer$ConditionObject.doSignal
  [ 2] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 3] LockedSingleValue.put
  [ 4] Channel.put
  [ 5] Cell.calculateNextState
  [ 6] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [ 7] Iterable.forEach
  [ 8] CellsGroup.run
  [ 9] Continuation.enterSpecial
  [10] Continuation.run
  [11] VirtualThread.runContinuation
  [12] VirtualThread$$Lambda$51.0x000000080103df08.run
  [13] ForkJoinTask$RunnableExecuteAction.exec
  [14] ForkJoinTask.doExec
  [15] ForkJoinPool$WorkQueue.topLevelExec
  [16] ForkJoinPool.scan
  [17] ForkJoinPool.runWorker
  [18] ForkJoinWorkerThread.run

--- 2898186 total (0.03%), 2902 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 3] ChannelsGrid.lambda$forEachChannel$1
  [ 4] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 5] Dimensions.forEachRowCol
  [ 6] ChannelsGrid.forEachChannel
  [ 7] GameOfLife.calculateFrame
  [ 8] GameOfLife.lambda$calculateFrameBlocking$4
  [ 9] GameOfLife$$Lambda$53.0x0000000801035458.run
  [10] VirtualThread.run
  [11] VirtualThread$VThreadContinuation.lambda$new$0
  [12] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [13] Continuation.enter0
  [14] Continuation.enter
  [15] Continuation.enterSpecial
  [16] Continuation.run
  [17] VirtualThread.runContinuation
  [18] VirtualThread$$Lambda$51.0x000000080103df08.run
  [19] ForkJoinTask$RunnableExecuteAction.exec
  [20] ForkJoinTask.doExec
  [21] ForkJoinPool$WorkQueue.topLevelExec
  [22] ForkJoinPool.scan
  [23] ForkJoinPool.runWorker
  [24] ForkJoinWorkerThread.run

--- 2876079 total (0.03%), 2874 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.enableWait
  [ 1] AbstractQueuedSynchronizer$ConditionObject.await
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 5] ChannelsGrid.lambda$forEachChannel$1
  [ 6] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 7] Dimensions.forEachRowCol
  [ 8] ChannelsGrid.forEachChannel
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035458.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 2847414 total (0.03%), 2846 samples
  [ 0] Dimensions.forEachRowCol
  [ 1] ChannelsGrid.forEachChannel
  [ 2] TickPerCell.tick
  [ 3] GameOfLife.calculateFrame
  [ 4] GameOfLife.lambda$calculateFrameBlocking$4
  [ 5] GameOfLife$$Lambda$53.0x0000000801035458.run
  [ 6] VirtualThread.run
  [ 7] VirtualThread$VThreadContinuation.lambda$new$0
  [ 8] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [ 9] Continuation.enter0
  [10] Continuation.enter
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 2812066 total (0.03%), 2792 samples
  [ 0] G1CollectedHeap::requires_barriers(stackChunkOopDesc*) const
  [ 1] Cont thaw
  [ 2] [not_walkable_Java]

--- 2787521 total (0.03%), 2793 samples
  [ 0] AbstractQueuedSynchronizer.release
  [ 1] ReentrantLock.unlock
  [ 2] LockedSingleValue.take
  [ 3] Channel.take
  [ 4] Cell$$Lambda$63.0x0000000801036978.apply
  [ 5] ReferencePipeline$3$1.accept
  [ 6] ArrayList$ArrayListSpliterator.forEachRemaining
  [ 7] AbstractPipeline.copyInto
  [ 8] AbstractPipeline.wrapAndCopyInto
  [ 9] ReduceOps$ReduceOp.evaluateSequential
  [10] AbstractPipeline.evaluate
  [11] IntPipeline.reduce
  [12] IntPipeline.sum
  [13] Cell.calculateNextState
  [14] CellsGroup$$Lambda$62.0x0000000801036760.accept
  [15] Iterable.forEach
  [16] CellsGroup.run
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$AdaptedRunnableAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 2786565 total (0.03%), 2784 samples
  [ 0] ForkJoinPool$WorkQueue.topLevelExec
  [ 1] ForkJoinPool.scan
  [ 2] ForkJoinPool.runWorker
  [ 3] ForkJoinWorkerThread.run

--- 2784125 total (0.03%), 2731 samples
  [ 0] dequeue_task_fair_[k]
  [ 1] __schedule_[k]
  [ 2] schedule_[k]
  [ 3] futex_wait_queue_[k]
  [ 4] futex_wait_[k]
  [ 5] do_futex_[k]
  [ 6] __x64_sys_futex_[k]
  [ 7] do_syscall_64_[k]
  [ 8] entry_SYSCALL_64_after_hwframe_[k]
  [ 9] __futex_abstimed_wait_common
  [10] Unsafe.park
  [11] LockSupport.park
  [12] ForkJoinPool.awaitWork
  [13] ForkJoinPool.runWorker
  [14] ForkJoinWorkerThread.run

--- 2776165 total (0.03%), 2782 samples
  [ 0] LockedSingleValue.put
  [ 1] Channel.put
  [ 2] TickPerCell.lambda$tick$0
  [ 3] TickPerCell$$Lambda$57.0x0000000801035cb0.accept
  [ 4] ChannelsGrid.lambda$forEachChannel$0
  [ 5] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] TickPerCell.tick
  [ 9] GameOfLife.calculateFrame
  [10] GameOfLife.lambda$calculateFrameBlocking$4
  [11] GameOfLife$$Lambda$53.0x0000000801035458.run
  [12] VirtualThread.run
  [13] VirtualThread$VThreadContinuation.lambda$new$0
  [14] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [15] Continuation.enter0
  [16] Continuation.enter
  [17] Continuation.enterSpecial
  [18] Continuation.run
  [19] VirtualThread.runContinuation
  [20] VirtualThread$$Lambda$51.0x000000080103df08.run
  [21] ForkJoinTask$RunnableExecuteAction.exec
  [22] ForkJoinTask.doExec
  [23] ForkJoinPool$WorkQueue.topLevelExec
  [24] ForkJoinPool.scan
  [25] ForkJoinPool.runWorker
  [26] ForkJoinWorkerThread.run

--- 2768683 total (0.03%), 2769 samples
  [ 0] ArrayList.elementAt
  [ 1] ArrayList.forEach
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 2741659 total (0.03%), 2745 samples
  [ 0] AbstractQueuedSynchronizer$ConditionObject.signal
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 4] ChannelsGrid.lambda$forEachChannel$1
  [ 5] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] GameOfLife.calculateFrame
  [ 9] GameOfLife.lambda$calculateFrameBlocking$4
  [10] GameOfLife$$Lambda$53.0x0000000801035458.run
  [11] VirtualThread.run
  [12] VirtualThread$VThreadContinuation.lambda$new$0
  [13] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [14] Continuation.enter0
  [15] Continuation.enter
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 2741548 total (0.03%), 2742 samples
  [ 0] AbstractQueuedSynchronizer.setState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.take
  [ 5] Channel.take
  [ 6] TickPerCell.waitTick
  [ 7] Cell.notifyLiveness
  [ 8] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 9] Iterable.forEach
  [10] CellsGroup.run
  [11] Continuation.enterSpecial
  [12] Continuation.run
  [13] VirtualThread.runContinuation
  [14] VirtualThread$$Lambda$51.0x000000080103df08.run
  [15] ForkJoinTask$RunnableExecuteAction.exec
  [16] ForkJoinTask.doExec
  [17] ForkJoinPool$WorkQueue.topLevelExec
  [18] ForkJoinPool.scan
  [19] ForkJoinPool.runWorker
  [20] ForkJoinWorkerThread.run

--- 2705093 total (0.03%), 2695 samples
  [ 0] psi_group_change_[k]
  [ 1] psi_task_change_[k]
  [ 2] enqueue_task_[k]
  [ 3] ttwu_do_activate_[k]
  [ 4] try_to_wake_up_[k]
  [ 5] wake_up_q_[k]
  [ 6] futex_wake_[k]
  [ 7] do_futex_[k]
  [ 8] __x64_sys_futex_[k]
  [ 9] do_syscall_64_[k]
  [10] entry_SYSCALL_64_after_hwframe_[k]
  [11] ___pthread_cond_signal
  [12] Unsafe.unpark
  [13] LockSupport.unpark
  [14] ForkJoinPool.signalWork
  [15] ForkJoinPool$WorkQueue.push
  [16] ForkJoinPool.poolSubmit
  [17] ForkJoinPool.execute
  [18] VirtualThread.submitRunContinuation
  [19] VirtualThread.submitRunContinuation
  [20] VirtualThread.unpark
  [21] System$2.unparkVirtualThread
  [22] VirtualThreads.unpark
  [23] LockSupport.unpark
  [24] AbstractQueuedSynchronizer.signalNext
  [25] AbstractQueuedSynchronizer.release
  [26] ReentrantLock.unlock
  [27] LockedSingleValue.put
  [28] Channel.put
  [29] TickPerCell.lambda$tick$0
  [30] TickPerCell$$Lambda$57.0x0000000801035cb0.accept
  [31] ChannelsGrid.lambda$forEachChannel$0
  [32] ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
  [33] Dimensions.forEachRowCol
  [34] ChannelsGrid.forEachChannel
  [35] TickPerCell.tick
  [36] GameOfLife.calculateFrame
  [37] GameOfLife.lambda$calculateFrameBlocking$4
  [38] GameOfLife$$Lambda$53.0x0000000801035458.run
  [39] VirtualThread.run
  [40] VirtualThread$VThreadContinuation.lambda$new$0
  [41] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [42] Continuation.enter0
  [43] Continuation.enter
  [44] Continuation.enterSpecial
  [45] Continuation.run
  [46] VirtualThread.runContinuation
  [47] VirtualThread$$Lambda$51.0x000000080103df08.run
  [48] ForkJoinTask$RunnableExecuteAction.exec
  [49] ForkJoinTask.doExec
  [50] ForkJoinPool$WorkQueue.topLevelExec
  [51] ForkJoinPool.scan
  [52] ForkJoinPool.runWorker
  [53] ForkJoinWorkerThread.run

--- 2681666 total (0.03%), 2688 samples
  [ 0] AbstractQueuedSynchronizer.getState
  [ 1] ReentrantLock$Sync.tryRelease
  [ 2] AbstractQueuedSynchronizer.release
  [ 3] ReentrantLock.unlock
  [ 4] LockedSingleValue.put
  [ 5] Channel.put
  [ 6] Cell.lambda$notifyLiveness$0
  [ 7] Cell$$Lambda$59.0x00000008010360e8.accept
  [ 8] ArrayList.forEach
  [ 9] Cell.notifyLiveness
  [10] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [11] Iterable.forEach
  [12] CellsGroup.run
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

--- 2636247 total (0.03%), 2636 samples
  [ 0] LockedSingleValue.take
  [ 1] Channel.take
  [ 2] TickPerCell.waitTick
  [ 3] Cell.notifyLiveness
  [ 4] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 5] Iterable.forEach
  [ 6] CellsGroup.run
  [ 7] Continuation.enterSpecial
  [ 8] Continuation.run
  [ 9] VirtualThread.runContinuation
  [10] VirtualThread$$Lambda$51.0x000000080103df08.run
  [11] ForkJoinTask$RunnableExecuteAction.exec
  [12] ForkJoinTask.doExec
  [13] ForkJoinPool$WorkQueue.topLevelExec
  [14] ForkJoinPool.scan
  [15] ForkJoinPool.runWorker
  [16] ForkJoinWorkerThread.run

--- 2620672 total (0.03%), 2624 samples
  [ 0] ReentrantLock.unlock
  [ 1] LockedSingleValue.take
  [ 2] Channel.take
  [ 3] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 4] ChannelsGrid.lambda$forEachChannel$1
  [ 5] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 6] Dimensions.forEachRowCol
  [ 7] ChannelsGrid.forEachChannel
  [ 8] GameOfLife.calculateFrame
  [ 9] GameOfLife.lambda$calculateFrameBlocking$4
  [10] GameOfLife$$Lambda$53.0x0000000801035458.run
  [11] VirtualThread.run
  [12] VirtualThread$VThreadContinuation.lambda$new$0
  [13] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [14] Continuation.enter0
  [15] Continuation.enter
  [16] Continuation.enterSpecial
  [17] Continuation.run
  [18] VirtualThread.runContinuation
  [19] VirtualThread$$Lambda$51.0x000000080103df08.run
  [20] ForkJoinTask$RunnableExecuteAction.exec
  [21] ForkJoinTask.doExec
  [22] ForkJoinPool$WorkQueue.topLevelExec
  [23] ForkJoinPool.scan
  [24] ForkJoinPool.runWorker
  [25] ForkJoinWorkerThread.run

--- 2593108 total (0.03%), 2596 samples
  [ 0] Channel.take
  [ 1] TickPerCell.waitTick
  [ 2] Cell.notifyLiveness
  [ 3] CellsGroup$$Lambda$54.0x0000000801035668.accept
  [ 4] Iterable.forEach
  [ 5] CellsGroup.run
  [ 6] Continuation.enterSpecial
  [ 7] Continuation.run
  [ 8] VirtualThread.runContinuation
  [ 9] VirtualThread$$Lambda$51.0x000000080103df08.run
  [10] ForkJoinTask$RunnableExecuteAction.exec
  [11] ForkJoinTask.doExec
  [12] ForkJoinPool$WorkQueue.topLevelExec
  [13] ForkJoinPool.scan
  [14] ForkJoinPool.runWorker
  [15] ForkJoinWorkerThread.run

--- 2590863 total (0.03%), 2592 samples
  [ 0] G1ParScanThreadState::trim_queue_to_threshold(unsigned int)
  [ 1] G1ParScanThreadState::steal_and_trim_queue(GenericTaskQueueSet<OverflowTaskQueue<ScannerTask, (MEMFLAGS)5, 131072u>, (MEMFLAGS)5>*)
  [ 2] G1ParEvacuateFollowersClosure::do_void()
  [ 3] G1EvacuateRegionsTask::evacuate_live_objects(G1ParScanThreadState*, unsigned int)
  [ 4] G1EvacuateRegionsBaseTask::work(unsigned int)
  [ 5] WorkerThread::run()
  [ 6] Thread::call_run()
  [ 7] thread_native_entry(Thread*)
  [ 8] start_thread

--- 2583491 total (0.03%), 2584 samples
  [ 0] GameOfLife$$Lambda$60.0x0000000801036308.test
  [ 1] ChannelsGrid.lambda$forEachChannel$1
  [ 2] ChannelsGrid$$Lambda$61.0x0000000801036540.accept
  [ 3] Dimensions.forEachRowCol
  [ 4] ChannelsGrid.forEachChannel
  [ 5] GameOfLife.calculateFrame
  [ 6] GameOfLife.lambda$calculateFrameBlocking$4
  [ 7] GameOfLife$$Lambda$53.0x0000000801035458.run
  [ 8] VirtualThread.run
  [ 9] VirtualThread$VThreadContinuation.lambda$new$0
  [10] VirtualThread$VThreadContinuation$$Lambda$50.0x000000080103dcf8.run
  [11] Continuation.enter0
  [12] Continuation.enter
  [13] Continuation.enterSpecial
  [14] Continuation.run
  [15] VirtualThread.runContinuation
  [16] VirtualThread$$Lambda$51.0x000000080103df08.run
  [17] ForkJoinTask$RunnableExecuteAction.exec
  [18] ForkJoinTask.doExec
  [19] ForkJoinPool$WorkQueue.topLevelExec
  [20] ForkJoinPool.scan
  [21] ForkJoinPool.runWorker
  [22] ForkJoinWorkerThread.run

       total  percent  samples  top
  ----------  -------  -------  ---
  2444496604   24.25%  2449563  AbstractQueuedSynchronizer.compareAndSetState
  1569675705   15.57%  1570020  Cell.lambda$notifyLiveness$0
   992121050    9.84%   993250  AbstractQueuedSynchronizer.release
   982007829    9.74%   979663  ReduceOps$5ReducingSink.get
   510558107    5.06%   510744  Cell$$Lambda$63.0x0000000801036978.apply
   488154196    4.84%   488857  LockedSingleValue.take
   332958215    3.30%   333050  Channel.take
   182289039    1.81%   182488  TickPerCell.lambda$tick$0
   144843086    1.44%   144496  AbstractPipeline.<init>
   127683479    1.27%   127367  StreamOpFlag.fromCharacteristics
   111810119    1.11%   111783  ArrayList$ArrayListSpliterator.forEachRemaining
    88808146    0.88%    88988  AbstractQueuedSynchronizer$ConditionObject.signal
    88027783    0.87%    87777  ReferencePipeline$4.opWrapSink
    87449892    0.87%    87619  LockedSingleValue.put
    86928970    0.86%    86881  Cell.calculateNextState
    80353585    0.80%    80533  ReentrantLock$Sync.lock
    76194033    0.76%    76197  CellsGroup$$Lambda$54.0x0000000801035668.accept
    76004359    0.75%    75906  ReferencePipeline$3.<init>
    72255568    0.72%    72366  ChannelsGrid.getChannel
    69452926    0.69%    69323  Sink$ChainedReference.<init>
    66540764    0.66%    66607  ReferencePipeline$3$1.accept
    65115151    0.65%    65022  ReferencePipeline$3.opWrapSink
    62180984    0.62%    62034  vtable stub
    61992094    0.61%    62148  AbstractQueuedSynchronizer.setState
    56757859    0.56%    56634  StreamSupport.stream
    53166710    0.53%    53147  Dimensions.forEachRowCol
    49442021    0.49%    49425  AbstractQueuedSynchronizer.signalNext
    49229978    0.49%    49203  Sink$ChainedReference.begin
    43720262    0.43%    43813  Cell$$Lambda$59.0x00000008010360e8.accept
    43177886    0.43%    43175  Cell.notifyLiveness
    39130865    0.39%    39160  ReentrantLock$NonfairSync.initialTryLock
    38627911    0.38%    38740  ReentrantLock$Sync.tryRelease
    38036408    0.38%    38094  AbstractOwnableSynchronizer.getExclusiveOwnerThread
    30129463    0.30%    30198  ReentrantLock.lock
    25048834    0.25%    25112  ReentrantLock$Sync.isHeldExclusively
    23199064    0.23%    23178  ArrayList.forEach
    22898835    0.23%    22948  ReferencePipeline$4$1.accept
    20839236    0.21%    20883  ReduceOps$5ReducingSink.accept
    20695018    0.21%    20692  ArrayList$SubList$1.next
    20602656    0.20%    20191  psi_group_change_[k]
    17785745    0.18%    17778  Iterable.forEach
    17507802    0.17%    17528  Channel.put
    17279733    0.17%    17286  ChannelsGrid$$Lambda$61.0x0000000801036540.accept
    15843622    0.16%    15840  ChannelsGrid$$Lambda$58.0x0000000801035ec8.accept
    14221770    0.14%    14249  ReentrantLock.unlock
    13029962    0.13%    12787  __update_load_avg_cfs_rq_[k]
    12867161    0.13%    12856  AbstractQueuedSynchronizer$ConditionObject.enableWait
    12073263    0.12%    11859  update_load_avg_[k]
    10673516    0.11%    10653  AbstractPipeline.wrapSink
    10643471    0.11%    10666  Cell$$Lambda$66.0x0000000801036fd0.applyAsInt
    10258249    0.10%    10080  update_cfs_group_[k]
     9414671    0.09%     9247  __update_load_avg_se_[k]
     8768425    0.09%     8750  GameOfLifeBenchmark_benchmark_jmhTest.benchmark_thrpt_jmhStub
     8679059    0.09%     8517  update_curr_[k]
     7370248    0.07%     7371  ArrayList$SubList$1.checkForComodification
     7336411    0.07%     7336  AbstractOwnableSynchronizer.setExclusiveOwnerThread
     7257360    0.07%     7251  ForkJoinPool.scan
     7100430    0.07%     7090  CellsGroup$$Lambda$62.0x0000000801036760.accept
     6468251    0.06%     6395  vframeStream::vframeStream(JavaThread*, bool, bool, bool)
     6205546    0.06%     6222  Sink$ChainedReference.end
     6075734    0.06%     6055  DirectMethodHandle.allocateInstance
     5972118    0.06%     5884  reweight_entity_[k]
     5671327    0.06%     5678  AbstractQueuedSynchronizer.enqueue
     5634938    0.06%     5616  Collection.stream
     5590940    0.06%     5600  AbstractQueuedSynchronizer.getState
     5231898    0.05%     5204  G1CollectedHeap::requires_barriers(stackChunkOopDesc*) const
     5118228    0.05%     5007  InstanceKlass::find_method_index(Array<Method*> const*, Symbol const*, Symbol const*, Klass::OverpassLookupMode, Klass::StaticLookupMode, Klass::PrivateLookupMode) [clone .constprop.0]
     5074425    0.05%     5047  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 286822ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
     5072268    0.05%     4956  update_blocked_averages_[k]
     4541810    0.05%     4541  AbstractQueuedSynchronizer.acquire
     4360400    0.04%     4271  __schedule_[k]
     4284049    0.04%     4241  enqueue_entity_[k]
     4110936    0.04%     4106  AbstractPipeline.copyInto
     4106545    0.04%     4082  __memset_avx2_unaligned_erms
     4102314    0.04%     4083  Unsafe_Park
     4096260    0.04%     4004  syscall_exit_to_user_mode_[k]
     3872043    0.04%     3865  ForkJoinPool.awaitWork
     3812336    0.04%     3813  Integer.intValue
     3699787    0.04%     3697  ForkJoinPool$WorkQueue.topLevelExec
     3543529    0.04%     3545  G1ParScanThreadState::trim_queue_to_threshold(unsigned int)
     3516758    0.03%     3515  VirtualThread.runContinuation
     3468665    0.03%     3475  ForkJoinPool$WorkQueue.push
     3328402    0.03%     3293  enqueue_task_fair_[k]
     3325374    0.03%     3256  __entry_text_start_[k]
     3244771    0.03%     3228  __memmove_sse2_unaligned_erms
     3237591    0.03%     3226  PipelineHelper.<init>
     3192765    0.03%     3204  Unsafe.getAndBitwiseAndInt
     3149679    0.03%     3155  AbstractQueuedSynchronizer$ConditionObject.doSignal
     3134680    0.03%     3126  Objects.requireNonNull
     3060283    0.03%     3052  ReferencePipeline.map
     3035899    0.03%     2984  _raw_spin_lock_[k]
     3035659    0.03%     2981  dequeue_task_fair_[k]
     2973250    0.03%     2972  VirtualThread.unmount
     2952299    0.03%     2947  Parker::park(bool, long)
     2941466    0.03%     2942  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<548964ul, G1BarrierSet>, (AccessInternal::BarrierType)2, 548964ul>::oop_access_barrier(void*)
     2881610    0.03%     2882  ArrayList.elementAt
     2845120    0.03%     2830  int freeze<Config<(oop_kind)0, G1BarrierSet> >(JavaThread*, long*)
     2816984    0.03%     2761  __calc_delta_[k]
     2744334    0.03%     2691  futex_q_lock_[k]
     2711514    0.03%     2712  GameOfLife$$Lambda$60.0x0000000801036308.test
     2561112    0.03%     2561  ForkJoinTask$AdaptedRunnableAction.exec
     2534388    0.03%     2534  ChannelsGrid.lambda$forEachChannel$1
     2470352    0.02%     2428  update_irq_load_avg_[k]
     2461451    0.02%     2412  check_preemption_disabled_[k]
     2442395    0.02%     2430  Unsafe.park
     2398098    0.02%     2354  dequeue_entity_[k]
     2376874    0.02%     2354  LinkResolver::resolve_continuation_enter(CallInfo&, JavaThread*)
     2340559    0.02%     2319  CellsGroup.run
     2336227    0.02%     2293  update_rq_clock_[k]
     2221781    0.02%     2189  oopDesc::address_field(int) const
     2213571    0.02%     2222  VirtualThread.unpark
     2207816    0.02%     2208  ___pthread_cond_wait
     2168404    0.02%     2122  update_min_vruntime_[k]
     2114777    0.02%     2076  cpuacct_charge_[k]
     2110731    0.02%     2120  ForkJoinPool.signalWork
     2061858    0.02%     2017  iterate_groups_[k]
     2020372    0.02%     1995  select_task_rq_fair_[k]
     1983829    0.02%     1987  MemAllocator::allocate() const
     1981254    0.02%     1985  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<286822ul, G1BarrierSet>, (AccessInternal::BarrierType)3, 286822ul>::oop_access_barrier(oopDesc*, long)
     1921215    0.02%     1910  G1CardSet::occupied() const
     1906921    0.02%     1891  LinkResolver::check_method_loader_constraints(LinkInfo const&, methodHandle const&, char const*, JavaThread*)
     1898240    0.02%     1867  futex_wake_[k]
     1867801    0.02%     1875  AccessInternal::PostRuntimeDispatch<G1BarrierSet::AccessBarrier<1335398ul, G1BarrierSet>, (AccessInternal::BarrierType)1, 1335398ul>::oop_access_barrier(oopDesc*, long, oopDesc*)
     1846073    0.02%     1851  TypeArrayKlass::multi_allocate(int, int*, JavaThread*)
     1825338    0.02%     1820  __tls_get_addr
     1806914    0.02%     1742  __get_user_8_[k]
     1770569    0.02%     1718  rb_next_[k]
     1765113    0.02%     1750  psi_task_change_[k]
     1749652    0.02%     1752  System$2.unparkVirtualThread
     1711584    0.02%     1682  native_sched_clock_[k]
     1651563    0.02%     1649  Continuation.yield0
     1616330    0.02%     1619  update_register_map1(ImmutableOopMap const*, frame const*, RegisterMap*)
     1604600    0.02%     1601  Continuation::prepare_thaw(JavaThread*, bool)
     1589634    0.02%     1557  psi_task_switch_[k]
     1544399    0.02%     1515  task_tick_fair_[k]
     1536565    0.02%     1535  AbstractQueuedSynchronizer$ConditionObject.canReacquire
     1519042    0.02%     1487  timerqueue_add_[k]
     1506315    0.01%     1509  ObjArrayKlass::multi_allocate(int, int*, JavaThread*)
     1489154    0.01%     1486  long* thaw<Config<(oop_kind)0, G1BarrierSet> >(JavaThread*, int)
     1474008    0.01%     1409  syscall_return_via_sysret_[k]
     1468179    0.01%     1459  __futex_abstimed_wait_common
     1453981    0.01%     1433  try_to_wake_up_[k]
     1430936    0.01%     1392  rcu_sched_clock_irq_[k]
     1416205    0.01%     1418  TickPerCell.waitTick
     1402806    0.01%     1400  __GI___pthread_mutex_lock
     1372985    0.01%     1379  I2C/C2I adapters
     1367701    0.01%     1328  __hrtimer_run_queues_[k]
     1357723    0.01%     1357  SafepointMechanism::update_poll_values(JavaThread*)
     1351690    0.01%     1352  ArrayList$SubList$1.hasNext
     1342897    0.01%     1332  Freeze<Config<(oop_kind)0, G1BarrierSet> >::allocate_chunk(unsigned long)
     1314026    0.01%     1302  futex_wake_mark_[k]
     1299198    0.01%     1293  Thaw<Config<(oop_kind)0, G1BarrierSet> >::thaw_fast(stackChunkOopDesc*)
     1290770    0.01%     1280  FreezeBase::freeze_fast_copy(stackChunkOopDesc*, int)
     1287412    0.01%     1263  newidle_balance_[k]
     1284119    0.01%     1280  __memcpy_sse2_unaligned
     1280565    0.01%     1260  preempt_count_add_[k]
     1230380    0.01%     1215  __condvar_dec_grefs
     1221619    0.01%     1198  __perf_event_task_sched_out_[k]
     1200226    0.01%     1160  restore_fpregs_from_fpstate_[k]
     1196118    0.01%     1195  pthread_mutex_trylock@@GLIBC_2.34
     1190281    0.01%     1191  ForkJoinPool.compareAndExchangeCtl
     1184129    0.01%     1189  AbstractQueuedSynchronizer$ConditionObject.await
     1135574    0.01%     1112  G1Allocator::unsafe_max_tlab_alloc()
     1126320    0.01%     1104  __cgroup_account_cputime_[k]
     1121403    0.01%     1111  __vdso_clock_gettime
     1119557    0.01%     1124  void OopOopIterateBackwardsDispatch<G1ScanEvacuatedObjClosure>::Table::oop_oop_iterate_backwards<InstanceKlass, narrowOop>(G1ScanEvacuatedObjClosure*, oopDesc*, Klass*)
     1117690    0.01%     1093  preempt_count_sub_[k]
     1095704    0.01%     1085  JavaThread::threadObj() const
     1078044    0.01%     1058  G1Policy::preventive_collection_required(unsigned int)
     1044366    0.01%     1022  asm_sysvec_apic_timer_interrupt_[k]
     1022508    0.01%     1023  ReferencePipeline$Head.<init>
     1022032    0.01%     1021  ForkJoinTask.doExec
     1016436    0.01%     1008  ForkJoinPool.hasTasks
     1012517    0.01%      994  wrong_method_stub
     1000991    0.01%     1003  native_write_msr_[k]
     1000431    0.01%      986  G1Analytics::predict_scan_card_num(unsigned long, bool) const
      987446    0.01%      994  ___pthread_cond_signal
      981512    0.01%      980  ObjArrayKlass::allocate(int, JavaThread*)
      973283    0.01%      949  futex_wait_[k]
      950681    0.01%      955  java_lang_Thread::set_thread_status(oopDesc*, JavaThreadStatus)
      928455    0.01%      931  ObjArrayAllocator::initialize(HeapWordImpl**) const
      921664    0.01%      920  __clock_gettime
      916008    0.01%      905  HeapRegionManager::allocate_free_region(HeapRegionType, unsigned int)
      909361    0.01%      897  ImmutableOopMapSet::find_map_at_offset(int) const
      906419    0.01%      906  OptoRuntime::multianewarray2_C(Klass*, int, int, JavaThread*)
      905066    0.01%      886  VirtualThread.afterTerminate
      884138    0.01%      891  OopMapStream::find_next() [clone .part.0]
      881739    0.01%      867  __get_user_nocheck_4_[k]
      874842    0.01%      875  MemAllocator::Allocation::notify_allocation_jvmti_sampler()
      867465    0.01%      846  update_process_times_[k]
      863857    0.01%      840  __cgroup_account_cputime_field_[k]
      861522    0.01%      864  VirtualThread.compareAndSetState
      858565    0.01%      847  G1CollectedHeap::attempt_allocation_slow(unsigned long)
      841436    0.01%      824  __accumulate_pelt_segments_[k]
      836277    0.01%      817  _raw_spin_lock_irqsave_[k]
      826448    0.01%      785  exit_to_user_mode_prepare_[k]
      824710    0.01%      814  G1CollectedHeap::allocate_new_tlab(unsigned long, unsigned long, unsigned long*)
      823440    0.01%      782  fpregs_restore_userregs_[k]
      818306    0.01%      821  SharedRuntime::handle_wrong_method(JavaThread*)
      817182    0.01%      818  System$2.setExtentLocalCache
